f | copy_page_64.S | g | 2.2K |  | Jan Beulich | JBeulich@suse.com | 1325849137 |  | x86-64: Slightly shorten copy_page()  %r13 got saved and restored without ever getting touched, so there's no need to do so.  Signed-off-by: Jan Beulich <jbeulich@suse.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Link: http://lkml.kernel.org/r/4F05D9F9020000780006AA0D@nat28.tlf.novell.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | thunk_64.S | g | 1.1K |  | Jan Beulich | JBeulich@novell.com | 1311231816 |  | x86: Fix write lock scalability 64-bit issue  With the write lock path simply subtracting RW_LOCK_BIAS there is, on large systems, the theoretical possibility of overflowing the 32-bit value that was used so far (namely if 128 or more CPUs manage to do the subtraction, but don't get to do the inverse addition in the failure path quickly enough).  A first measure is to modify RW_LOCK_BIAS itself - with the new value chosen, it is good for up to 2048 CPUs each allowed to nest over 2048 times on the read path without causing an issue. Quite possibly it would even be sufficient to adjust the bias a little further, assuming that allowing for significantly less nesting would suffice.  However, as the original value chosen allowed for even more nesting levels, to support more than 2048 CPUs (possible currently only for 64-bit kernels) the lock itself gets widened to 64 bits.  Signed-off-by: Jan Beulich <jbeulich@novell.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Peter Zijlstra <a.p.zijlstra@chello.nl> Link: http://lkml.kernel.org/r/4E258E0D020000780004E3F0@nat28.tlf.novell.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | string_32.c | s | 4.6K | 223 | Alexey Dobriyan | adobriyan@gmail.com | 1323711222 |  | x86/i386: Use less assembly in strlen(), speed things up a bit  Current i386 strlen() hardcodes NOT/DEC sequence. DEC is mentioned to be suboptimal on Core2. So, put only REPNE SCASB sequence in assembly, compiler can do the rest.  The difference in generated code is like below (MCORE2=y):  	<strlen>: 		push   %edi 		mov    $0xffffffff,%ecx 		mov    %eax,%edi 		xor    %eax,%eax 		repnz scas %es:(%edi),%al 		not    %ecx  	-	dec    %ecx 	-	mov    %ecx,%eax 	+	lea    -0x1(%ecx),%eax  		pop    %edi 		ret  Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Jan Beulich <JBeulich@suse.com> Link: http://lkml.kernel.org/r/20111211181319.GA17097@p183.telecom.by Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | csum-wrappers_64.c | s | 3.8K | 131 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1340009598 |  | x86: Fix kernel-doc warnings  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Peter Zijlstra <a.p.zijlstra@chello.nl> Cc: Jason Wessel <jason.wessel@windriver.com> Cc: Jan Kiszka <jan.kiszka@siemens.com> Cc: Gavin Shan <shangw@linux.vnet.ibm.com> Cc: Wanpeng Li <liwp.linux@gmail.com> Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | usercopy_64.c | s | 2.1K | 79 | Linus Torvalds | torvalds@linux-foundation.org | 1338057234 |  | x86: use the new generic strnlen_user() function  This throws away the old x86-specific functions in favor of the generic optimized version.  Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | Makefile | g | 1.4K |  | Linus Torvalds | torvalds@linux-foundation.org | 1311379344 |  | Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip  * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:   x86: Fix write lock scalability 64-bit issue   x86: Unify rwsem assembly implementation   x86: Unify rwlock assembly implementation   x86, asm: Fix binutils 2.16 issue with __USER32_CS   x86, asm: Cleanup thunk_64.S   x86, asm: Flip RESTORE_ARGS arguments logic   x86, asm: Flip SAVE_ARGS arguments logic   x86, asm: Thin down SAVE/RESTORE_* asm macros
f | copy_user_64.S | g | 6.1K |  | H. Peter Anvin | hpa@zytor.com | 1334955098 |  | x86, extable: Remove open-coded exception table entries in arch/x86/lib/copy_user_64.S  Remove open-coded exception table entries in arch/x86/lib/copy_user_64.S, and replace them with _ASM_EXTABLE() macros; this will allow us to change the format and type of the exception table entries.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
f | csum-partial_64.c | s | 3.4K | 138 | Lucas De Marchi | lucas.de.marchi@gmail.com | 1300441170 |  | x86: Fix common misspellings  They were generated by 'codespell' and then manually reviewed.  Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi> Cc: trivial@kernel.org LKML-Reference: <1300389856-1099-3-git-send-email-lucas.demarchi@profusion.mobi> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | atomic64_386_32.S | g | 2.6K |  | Jan Beulich | JBeulich@suse.com | 1327109389 |  | x86: atomic64 assembly improvements  In the "xchg" implementation, %ebx and %ecx don't need to be copied into %eax and %edx respectively (this is only necessary when desiring to only read the stored value).  In the "add_unless" implementation, swapping the use of %ecx and %esi for passing arguments allows %esi to become an input only (i.e. permitting the register to be re-used to address the same object without reload).  In "{add,sub}_return", doing the initial read64 through the passed in %ecx decreases a register dependency.  In "inc_not_zero", a branch can be eliminated by or-ing together the two halves of the current (64-bit) value, and code size can be further reduced by adjusting the arithmetic slightly.  v2: Undo the folding of "xchg" and "set".  Signed-off-by: Jan Beulich <jbeulich@suse.com> Link: http://lkml.kernel.org/r/4F19A2BC020000780006E0DC@nat28.tlf.novell.com Cc: Luca Barbieri <luca@luca-barbieri.com> Cc: Eric Dumazet <eric.dumazet@gmail.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | cmpxchg.c | s | 1.1K | 47 | H. Peter Anvin | hpa@linux.intel.com | 1280361911 |  | x86, asm: Merge cmpxchg_486_u64() and cmpxchg8b_emu()  We have two functions for doing exactly the same thing -- emulating cmpxchg8b on 486 and older hardware -- with different calling conventions, and yet doing the same thing.  Drop the C version and use the assembly version, via alternatives, for both the local and non-local versions of cmpxchg8b.  Signed-off-by: H. Peter Anvin <hpa@linux.intel.com> LKML-Reference: <AANLkTikAmaDPji-TVDarmG1yD=fwbffcsmEU=YEuP+8r@mail.gmail.com>
f | memcpy_32.c | s | 3.7K | 188 | Ma Ling | ling.ma@intel.com | 1285379831 |  | x86, mem: Optimize memmove for small size and unaligned cases  movs instruction will combine data to accelerate moving data, however we need to concern two cases about it.  1. movs instruction need long lantency to startup,    so here we use general mov instruction to copy data. 2. movs instruction is not good for unaligned case,    even if src offset is 0x10, dest offset is 0x0,    we avoid and handle the case by general mov instruction.  Signed-off-by: Ma Ling <ling.ma@intel.com> LKML-Reference: <1284664360-6138-1-git-send-email-ling.ma@intel.com> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | rwsem.S | g | 3.6K |  | Jan Beulich | JBeulich@novell.com | 1311231812 |  | x86: Unify rwsem assembly implementation  Rather than having two functionally identical implementations for 32- and 64-bit configurations, use the previously extended assembly abstractions to fold the rwsem two implementations into a shared one.  Signed-off-by: Jan Beulich <jbeulich@novell.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Peter Zijlstra <a.p.zijlstra@chello.nl> Link: http://lkml.kernel.org/r/4E258DF3020000780004E3ED@nat28.tlf.novell.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | delay.c | s | 2.8K | 120 | Thomas Gleixner | tglx@linutronix.de | 1331325807 |  | x86: Derandom delay_tsc for 64 bit  Commit f0fbf0abc093 ("x86: integrate delay functions") converted delay_tsc() into a random delay generator for 64 bit.  The reason is that it merged the mostly identical versions of delay_32.c and delay_64.c.  Though the subtle difference of the result was:   static void delay_tsc(unsigned long loops)  { -	unsigned bclock, now; +	unsigned long bclock, now;  Now the function uses rdtscl() which returns the lower 32bit of the TSC. On 32bit that's not problematic as unsigned long is 32bit. On 64 bit this fails when the lower 32bit are close to wrap around when bclock is read, because the following check         if ((now - bclock) >= loops)        	  	break;  evaluated to true on 64bit for e.g. bclock = 0xffffffff and now = 0 because the unsigned long (now - bclock) of these values results in 0xffffffff00000001 which is definitely larger than the loops value. That explains Tvortkos observation:  "Because I am seeing udelay(500) (_occasionally_) being short, and  that by delaying for some duration between 0us (yep) and 491us."  Make those variables explicitely u32 again, so this works for both 32 and 64 bit.  Reported-by: Tvrtko Ursulin <tvrtko.ursulin@onelan.co.uk> Signed-off-by: Thomas Gleixner <tglx@linutronix.de> Cc: stable@vger.kernel.org # >= 2.6.27 Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | msr.c | s | 382B | 19 | Borislav Petkov | petkovbb@googlemail.com | 1261006592 |  | x86, msr: msrs_alloc/free for CONFIG_SMP=n  Randy Dunlap reported the following build error:  "When CONFIG_SMP=n, CONFIG_X86_MSR=m:  ERROR: "msrs_free" [drivers/edac/amd64_edac_mod.ko] undefined! ERROR: "msrs_alloc" [drivers/edac/amd64_edac_mod.ko] undefined!"  This is due to the fact that <arch/x86/lib/msr.c> is conditioned on CONFIG_SMP and in the UP case we have only the stubs in the header. Fork off SMP functionality into a new file (msr-smp.c) and build msrs_{alloc,free} unconditionally.  Reported-by: Randy Dunlap <randy.dunlap@oracle.com> Cc: H. Peter Anvin <hpa@zytor.com> Signed-off-by: Borislav Petkov <petkovbb@gmail.com> LKML-Reference: <20091216231625.GD27228@liondog.tnic> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | getuser.S | g | 2.0K |  | H. Peter Anvin | hpa@zytor.com | 1334955099 |  | x86, extable: Remove open-coded exception table entries in arch/x86/lib/getuser.S  Remove open-coded exception table entries in arch/x86/lib/getuser.S, and replace them with _ASM_EXTABLE() macros; this will allow us to change the format and type of the exception table entries.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
f | cache-smp.c | s | 322B | 16 | Borislav Petkov | borislav.petkov@amd.com | 1264205142 |  | x86, lib: Add wbinvd smp helpers  Add wbinvd_on_cpu and wbinvd_on_all_cpus stubs for executing wbinvd on a particular CPU.  [ hpa: renamed lib/smp.c to lib/cache-smp.c ] [ hpa: wbinvd_on_all_cpus() returns int, but wbinvd() returns   void.  Thus, the former cannot be a macro for the latter,   replace with an inline function. ]  Signed-off-by: Borislav Petkov <borislav.petkov@amd.com> LKML-Reference: <1264172467-25155-2-git-send-email-bp@amd64.org> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | usercopy.c | s | 929B | 39 | Stephane Eranian | eranian@google.com | 1339592428 |  | perf/x86: Fix broken LBR fixup code  I noticed that the LBR fixups were not working anymore on programs where they used to. I tracked this down to a recent change to copy_from_user_nmi():   db0dc75d640 ("perf/x86: Check user address explicitly in copy_from_user_nmi()")  This commit added a call to __range_not_ok() to the copy_from_user_nmi() routine. The problem is that the logic of the test must be reversed. __range_not_ok() returns 0 if the range is VALID. We want to return early from copy_from_user_nmi() if the range is NOT valid.  Signed-off-by: Stephane Eranian <eranian@google.com> Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl> Acked-by: Arun Sharma <asharma@fb.com> Link: http://lkml.kernel.org/r/20120611134426.GA7542@quad Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | memmove_64.S | g | 3.7K |  | Andy Lutomirski | luto@mit.edu | 1310581376 |  | x86: Make alternative instruction pointers relative  This save a few bytes on x86-64 and means that future patches can apply alternatives to unrelocated code.  Signed-off-by: Andy Lutomirski <luto@mit.edu> Link: http://lkml.kernel.org/r/ff64a6b9a1a3860ca4a7b8b6dc7b4754f9491cd7.1310563276.git.luto@mit.edu Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | clear_page_64.S | g | 1.4K |  | Fenghua Yu | fenghua.yu@intel.com | 1305672027 |  | x86, mem: clear_page_64.S: Support clear_page() with enhanced REP MOVSB/STOSB  Intel processors are adding enhancements to REP MOVSB/STOSB and the use of REP MOVSB/STOSB for optimal memcpy/memset or similar functions is recommended. Enhancement availability is indicated by CPUID.7.0.EBX[9] (Enhanced REP MOVSB/ STOSB).  Support clear_page() with rep stosb for processor supporting enhanced REP MOVSB /STOSB. On processors supporting enhanced REP MOVSB/STOSB, the alternative clear_page_c_e function using enhanced REP STOSB overrides the original function and the fast string function.  Signed-off-by: Fenghua Yu <fenghua.yu@intel.com> Link: http://lkml.kernel.org/r/1305671358-14478-6-git-send-email-fenghua.yu@intel.com Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | copy_user_nocache_64.S | g | 2.5K |  | H. Peter Anvin | hpa@zytor.com | 1334955098 |  | x86, extable: Remove open-coded exception table entries in arch/x86/lib/copy_user_nocache_64.S  Remove open-coded exception table entries in arch/x86/lib/copy_user_nocache_64.S, and replace them with _ASM_EXTABLE() macros; this will allow us to change the format and type of the exception table entries.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
f | memcpy_64.S | g | 4.1K |  | Jan Beulich | JBeulich@suse.com | 1327609160 |  | x86-64: Handle byte-wise tail copying in memcpy() without a loop  While hard to measure, reducing the number of possibly/likely mis-predicted branches can generally be expected to be slightly better.  Other than apparent at the first glance, this also doesn't grow the function size (the alignment gap to the next function just gets smaller).  Signed-off-by: Jan Beulich <jbeulich@suse.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Link: http://lkml.kernel.org/r/4F218584020000780006F422@nat28.tlf.novell.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | memset_64.S | g | 3.1K |  | Jan Beulich | JBeulich@suse.com | 1327575004 |  | x86-64: Fix memset() to support sizes of 4Gb and above  While currently there doesn't appear to be any reachable in-tree case where such large memory blocks may be passed to memset() (alloc_bootmem() being the primary non-reachable one, as it gets called with suitably large sizes in FLATMEM configurations), we have recently hit the problem a second time in our Xen kernels.  Rather than working around it a second time, prevent others from falling into the same trap by fixing this long standing limitation.  Signed-off-by: Jan Beulich <jbeulich@suse.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Link: http://lkml.kernel.org/r/4F05D992020000780006AA09@nat28.tlf.novell.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | rwlock.S | g | 783B |  | Jan Beulich | JBeulich@novell.com | 1311231816 |  | x86: Fix write lock scalability 64-bit issue  With the write lock path simply subtracting RW_LOCK_BIAS there is, on large systems, the theoretical possibility of overflowing the 32-bit value that was used so far (namely if 128 or more CPUs manage to do the subtraction, but don't get to do the inverse addition in the failure path quickly enough).  A first measure is to modify RW_LOCK_BIAS itself - with the new value chosen, it is good for up to 2048 CPUs each allowed to nest over 2048 times on the read path without causing an issue. Quite possibly it would even be sufficient to adjust the bias a little further, assuming that allowing for significantly less nesting would suffice.  However, as the original value chosen allowed for even more nesting levels, to support more than 2048 CPUs (possible currently only for 64-bit kernels) the lock itself gets widened to 64 bits.  Signed-off-by: Jan Beulich <jbeulich@novell.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Peter Zijlstra <a.p.zijlstra@chello.nl> Link: http://lkml.kernel.org/r/4E258E0D020000780004E3F0@nat28.tlf.novell.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | cmpxchg16b_emu.S | g | 1.3K |  | Christoph Lameter | cl@linux.com | 1301279136 |  | percpu: Omit segment prefix in the UP case for cmpxchg_double  Omit the segment prefix in the UP case. GS is not used then and we will generate segfaults if cmpxchg16b is used otherwise.  Signed-off-by: Christoph Lameter <cl@linux.com> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | usercopy_32.c | s | 21K | 711 | Linus Torvalds | torvalds@linux-foundation.org | 1338057234 |  | x86: use the new generic strnlen_user() function  This throws away the old x86-specific functions in favor of the generic optimized version.  Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | x86-opcode-map.txt | g | 23K |  | Masami Hiramatsu | masami.hiramatsu.pt@hitachi.com | 1338965658 |  | x86/decoder: Fix bsr/bsf/jmpe decoding with operand-size prefix  Fix the x86 instruction decoder to decode bsr/bsf/jmpe with operand-size prefix (66h). This fixes the test case failure reported by Linus, attached below.  bsf/bsr/jmpe have a special encoding. Opcode map in Intel Software Developers Manual vol2 says they have TZCNT/LZCNT variants if it has F3h prefix. However, there is no information if it has other 66h or F2h prefixes. Current instruction decoder supposes that those are bad instructions, but it actually accepts at least operand-size prefixes.  H. Peter Anvin further explains:   " TZCNT/LZCNT are F3 + BSF/BSR exactly because the F2 and    F3 prefixes have historically been no-ops with most instructions.    This allows software to unconditionally use the prefixed versions    and get TZCNT/LZCNT on the processors that have them if they don't    care about the difference. "  This fixes errors reported by test_get_len:    Warning: arch/x86/tools/test_get_len found difference at <em_bsf>:ffffffff81036d87   Warning: ffffffff81036de5:	66 0f bc c2          	bsf    %dx,%ax   Warning: objdump says 4 bytes, but insn_get_length() says 3   Warning: arch/x86/tools/test_get_len found difference at <em_bsr>:ffffffff81036ea6   Warning: ffffffff81036f04:	66 0f bd c2          	bsr    %dx,%ax   Warning: objdump says 4 bytes, but insn_get_length() says 3   Warning: decoded and checked 13298882 instructions with 2 warnings  Reported-by: Linus Torvalds <torvalds@linux-foundation.org> Reported-by: Pekka Enberg <penberg@kernel.org> Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com> Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: <yrl.pp-manager.tt@hitachi.com> Link: http://lkml.kernel.org/r/20120604150911.22338.43296.stgit@localhost.localdomain Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | msr-reg.S | g | 1.9K |  | Ingo Molnar | mingo@elte.hu | 1252005994 |  | x86, msr: Fix msr-reg.S compilation with gas 2.16.1, on 32-bit too  The macro was defined in the 32-bit path as well - breaking the build on 32-bit platforms:    arch/x86/lib/msr-reg.S: Assembler messages:   arch/x86/lib/msr-reg.S:53: Error: Bad macro parameter list   arch/x86/lib/msr-reg.S:100: Error: invalid character '_' in mnemonic   arch/x86/lib/msr-reg.S:101: Error: invalid character '_' in mnemonic  Cc: Borislav Petkov <petkovbb@googlemail.com> Cc: H. Peter Anvin <hpa@zytor.com> LKML-Reference: <tip-f6909f394c2d4a0a71320797df72d54c49c5927e@git.kernel.org> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | strstr_32.c | s | 674B | 29 | Paolo Ciarrocchi | paolo.ciarrocchi@gmail.com | 1218812004 |  | x86: coding style fixes to arch/x86/lib/strstr_32.c  Before: total: 3 errors, 0 warnings, 31 lines checked  After: total: 0 errors, 0 warnings, 31 lines checked  paolo@paolo-desktop:~/linux.trees.git$ md5sum /tmp/strstr_32.o.* c96006ec3387862e5bacb139207a3098  /tmp/strstr_32.o.after c96006ec3387862e5bacb139207a3098  /tmp/strstr_32.o.before  Signed-off-by: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | insn.c | s | 14K | 526 | Masami Hiramatsu | masami.hiramatsu.pt@hitachi.com | 1334559371 |  | x86: Handle failures of parsing immediate operands in the instruction decoder  This can happen if the instruction is much longer than the maximum length, or if insn->opnd_bytes is manually changed.  This patch also fixes warnings from -Wswitch-default flag.  Reported-by: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com> Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com> Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com> Cc: Linux-mm <linux-mm@kvack.org> Cc: Oleg Nesterov <oleg@redhat.com> Cc: Andi Kleen <andi@firstfloor.org> Cc: Christoph Hellwig <hch@infradead.org> Cc: Steven Rostedt <rostedt@goodmis.org> Cc: Arnaldo Carvalho de Melo <acme@infradead.org> Cc: Anton Arapov <anton@redhat.com> Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com> Cc: yrl.pp-manager.tt@hitachi.com Cc: Peter Zijlstra <peterz@infradead.org> Link: http://lkml.kernel.org/r/20120413032427.32577.42602.stgit@localhost.localdomain Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | thunk_32.S | g | 656B |  | Jan Beulich | JBeulich@novell.com | 1298912782 |  | x86: Remove unused bits from lib/thunk_*.S  Some of the items removed were apparently never used, others simply didn't get removed with their last user.  Signed-off-by: Jan Beulich <jbeulich@novell.com> LKML-Reference: <4D6BD3A002000078000341F1@vpn.id2.novell.com> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | msr-reg-export.c | s | 126B | 4 | H. Peter Anvin | hpa@zytor.com | 1252083609 |  | x86, msr: change msr-reg.o to obj-y, and export its symbols  Change msr-reg.o to obj-y (it will be included in virtually every kernel since it is used by the initialization code for AMD processors) and add a separate C file to export its symbols to modules, so that msr.ko can use them; on uniprocessors we bypass the helper functions in msr.o and use the accessor functions directly via inlines.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> LKML-Reference: <20090904140834.GA15789@elte.hu> Cc: Borislav Petkov <petkovbb@googlemail.com>
f | iomap_copy_64.S | g | 933B |  | Thomas Gleixner | tglx@linutronix.de | 1192094228 |  | x86_64: move lib  Signed-off-by: Thomas Gleixner <tglx@linutronix.de> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | inat.c | s | 2.6K | 85 | Masami Hiramatsu | masami.hiramatsu.pt@hitachi.com | 1328969495 |  | x86: Fix to decode grouped AVX with VEX pp bits  Fix to decode grouped AVX with VEX pp bits which should be handled as same as last-prefixes. This fixes below warnings in posttest with CONFIG_CRYPTO_SHA1_SSSE3=y.   Warning: arch/x86/tools/test_get_len found difference at <sha1_transform_avx>:ffffffff810d5fc0  Warning: ffffffff810d6069:	c5 f9 73 de 04       	vpsrldq $0x4,%xmm6,%xmm0  Warning: objdump says 5 bytes, but insn_get_length() says 4  ...  With this change, test_get_len can decode it correctly.   $ arch/x86/tools/test_get_len -v -y  ffffffff810d6069:       c5 f9 73 de 04          vpsrldq $0x4,%xmm6,%xmm0  Succeed: decoded and checked 1 instructions  Reported-by: Ingo Molnar <mingo@elte.hu> Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com> Cc: yrl.pp-manager.tt@hitachi.com Link: http://lkml.kernel.org/r/20120210053340.30429.73410.stgit@localhost.localdomain Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | atomic64_cx8_32.S | g | 3.1K |  | Jan Beulich | JBeulich@suse.com | 1327109389 |  | x86: atomic64 assembly improvements  In the "xchg" implementation, %ebx and %ecx don't need to be copied into %eax and %edx respectively (this is only necessary when desiring to only read the stored value).  In the "add_unless" implementation, swapping the use of %ecx and %esi for passing arguments allows %esi to become an input only (i.e. permitting the register to be re-used to address the same object without reload).  In "{add,sub}_return", doing the initial read64 through the passed in %ecx decreases a register dependency.  In "inc_not_zero", a branch can be eliminated by or-ing together the two halves of the current (64-bit) value, and code size can be further reduced by adjusting the arithmetic slightly.  v2: Undo the folding of "xchg" and "set".  Signed-off-by: Jan Beulich <jbeulich@suse.com> Link: http://lkml.kernel.org/r/4F19A2BC020000780006E0DC@nat28.tlf.novell.com Cc: Luca Barbieri <luca@luca-barbieri.com> Cc: Eric Dumazet <eric.dumazet@gmail.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | csum-copy_64.S | g | 4.0K |  | H. Peter Anvin | hpa@zytor.com | 1334955099 |  | x86, extable: Remove open-coded exception table entries in arch/x86/lib/csum-copy_64.S  Remove open-coded exception table entries in arch/x86/lib/csum-copy_64.S, and replace them with _ASM_EXTABLE() macros; this will allow us to change the format and type of the exception table entries.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
f | mmx_32.c | s | 8.0K | 333 | Ingo Molnar | mingo@elte.hu | 1208446847 |  | x86: clean up mmx_32.c  checkpatch.pl --file cleanups:    before:     total: 74 errors, 3 warnings, 386 lines checked    after:     total: 0 errors, 0 warnings, 377 lines checked  no code changed:  arch/x86/lib/mmx_32.o:    text    data     bss     dec     hex filename    1323       0       8    1331     533 mmx_32.o.before    1323       0       8    1331     533 mmx_32.o.after  md5:    4cc39f1017dc40a5ebf02ce0ff7312bc  mmx_32.o.before.asm    4cc39f1017dc40a5ebf02ce0ff7312bc  mmx_32.o.after.asm  Signed-off-by: Ingo Molnar <mingo@elte.hu> Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
f | checksum_32.S | g | 10K |  | H. Peter Anvin | hpa@zytor.com | 1334955098 |  | x86, extable: Remove open-coded exception table entries in arch/x86/lib/checksum_32.S  Remove open-coded exception table entries in arch/x86/lib/checksum_32.S, and replace them with _ASM_EXTABLE() macros; this will allow us to change the format and type of the exception table entries.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
f | putuser.S | g | 1.8K |  | H. Peter Anvin | hpa@zytor.com | 1334955099 |  | x86, extable: Remove open-coded exception table entries in arch/x86/lib/putuser.S  Remove open-coded exception table entries in arch/x86/lib/putuser.S, and replace them with _ASM_EXTABLE() macros; this will allow us to change the format and type of the exception table entries.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
f | msr-smp.c | s | 4.0K | 160 | Borislav Petkov | petkovbb@googlemail.com | 1261006592 |  | x86, msr: msrs_alloc/free for CONFIG_SMP=n  Randy Dunlap reported the following build error:  "When CONFIG_SMP=n, CONFIG_X86_MSR=m:  ERROR: "msrs_free" [drivers/edac/amd64_edac_mod.ko] undefined! ERROR: "msrs_alloc" [drivers/edac/amd64_edac_mod.ko] undefined!"  This is due to the fact that <arch/x86/lib/msr.c> is conditioned on CONFIG_SMP and in the UP case we have only the stubs in the header. Fork off SMP functionality into a new file (msr-smp.c) and build msrs_{alloc,free} unconditionally.  Reported-by: Randy Dunlap <randy.dunlap@oracle.com> Cc: H. Peter Anvin <hpa@zytor.com> Signed-off-by: Borislav Petkov <petkovbb@gmail.com> LKML-Reference: <20091216231625.GD27228@liondog.tnic> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | cmpxchg8b_emu.S | g | 972B |  | Arjan van de Ven | arjan@infradead.org | 1254344159 |  | x86: Provide an alternative() based cmpxchg64()  cmpxchg64() today generates, to quote Linus, "barf bag" code.  cmpxchg64() is about to get used in the scheduler to fix a bug there, but it's a prerequisite that cmpxchg64() first be made non-sucking.  This patch turns cmpxchg64() into an efficient implementation that uses the alternative() mechanism to just use the raw instruction on all modern systems.  Note: the fallback is NOT smp safe, just like the current fallback is not SMP safe. (Interested parties with i486 based SMP systems are welcome to submit fix patches for that.)  Signed-off-by: Arjan van de Ven <arjan@linux.intel.com> Acked-by: Linus Torvalds <torvalds@linux-foundation.org> [ fixed asm constraint bug ] Fixed-by: Eric Dumazet <eric.dumazet@gmail.com> Cc: Martin Schwidefsky <schwidefsky@de.ibm.com> Cc: John Stultz <johnstul@us.ibm.com> Cc: Peter Zijlstra <a.p.zijlstra@chello.nl> LKML-Reference: <20090930170754.0886ff2e@infradead.org> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | atomic64_32.c | s | 91B | 3 | Jan Beulich | JBeulich@suse.com | 1327109371 |  | x86: Adjust asm constraints in atomic64 wrappers  Eric pointed out overly restrictive constraints in atomic64_set(), but there are issues throughout the file. In the cited case, %ebx and %ecx are inputs only (don't get changed by either of the two low level implementations). This was also the case elsewhere.  Further in many cases early-clobber indicators were missing.  Finally, the previous implementation rolled a custom alternative instruction macro from scratch, rather than using alternative_call() (which was introduced with the commit that the description of the change in question actually refers to). Adjusting has the benefit of not hiding referenced symbols from the compiler, which however requires them to be declared not just in the exporting source file (which, as a desirable side effect, in turn allows that exporting file to become a real 5-line stub).  This patch does not eliminate the overly restrictive memory clobbers, however: Doing so would occasionally make the compiler set up a second register for accessing the memory object (to satisfy the added "m" constraint), and it's not clear which of the two non-optimal alternatives is better.  v2: Re-do the declaration and exporting of the internal symbols.  Reported-by: Eric Dumazet <eric.dumazet@gmail.com> Signed-off-by: Jan Beulich <jbeulich@suse.com> Link: http://lkml.kernel.org/r/4F19A2A5020000780006E0D9@nat28.tlf.novell.com Cc: Luca Barbieri <luca@luca-barbieri.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
