f | init.c | s | 11K | 359 | Yinghai Lu | yinghai@kernel.org | 1339148450 |  | x86/mm: Only add extra pages count for the first memory range during pre-allocation early page table space  Robin found this regression:  || I just tried to boot an 8TB system.  It fails very early in boot with: || Kernel panic - not syncing: Cannot find space for the kernel page tables  git bisect commit 722bc6b16771ed80871e1fd81c86d3627dda2ac8.  A git revert of that commit does boot past that point on the 8TB configuration.  That commit will add up extra pages for all memory range even above 4g.  Try to limit that extra page count adding to first entry only.  Bisected-by: Robin Holt <holt@sgi.com> Tested-by: Robin Holt <holt@sgi.com> Signed-off-by: Yinghai Lu <yinghai@kernel.org> Cc: WANG Cong <xiyou.wangcong@gmail.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Peter Zijlstra <a.p.zijlstra@chello.nl> Link: http://lkml.kernel.org/r/CAE9FiQUj3wyzQxtq9yzBNc9u220p8JZ1FYHG7t%3DMOzJ%3D9BZMYA@mail.gmail.com Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | Makefile | g | 921B |  | Tejun Heo | tj@kernel.org | 1310669273 |  | memblock, x86: Replace memblock_x86_reserve/free_range() with generic ones  Other than sanity check and debug message, the x86 specific version of memblock reserve/free functions are simple wrappers around the generic versions - memblock_reserve/free().  This patch adds debug messages with caller identification to the generic versions and replaces x86 specific ones and kills them. arch/x86/include/asm/memblock.h and arch/x86/mm/memblock.c are empty after this change and removed.  Signed-off-by: Tejun Heo <tj@kernel.org> Link: http://lkml.kernel.org/r/1310462166-31469-14-git-send-email-tj@kernel.org Cc: Yinghai Lu <yinghai@kernel.org> Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Ingo Molnar <mingo@redhat.com> Cc: "H. Peter Anvin" <hpa@zytor.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | mmio-mod.c | s | 11K | 418 | Rusty Russell | rusty@rustcorp.com.au | 1326409338 |  | module_param: make bool parameters really bool (arch)  module_param(bool) used to counter-intuitively take an int.  In fddd5201 (mid-2009) we allowed bool or int/unsigned int using a messy trick.  It's time to remove the int/unsigned int option.  For this version it'll simply give a warning, but it'll break next kernel version.  Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
f | physaddr.h | s | 192B | 9 | Jeremy Fitzhardinge | jeremy.fitzhardinge@citrix.com | 1252608535 |  | x86: split __phys_addr out into separate file  Split __phys_addr out into its own file so we can disable -fstack-protector in a fine-grained fashion.  Also it doesn't have terribly much to do with the rest of ioremap.c.  Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
f | pageattr.c | s | 33K | 1168 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1339404885 |  | x86/mm: Fix some kernel-doc warnings  Fix kernel-doc warnings in arch/x86/mm/ioremap.c and arch/x86/mm/pageattr.c, just like this one:    Warning(arch/x86/mm/ioremap.c:204):      No description found for parameter 'phys_addr'   Warning(arch/x86/mm/ioremap.c:204):      Excess function parameter 'offset' description in 'ioremap_nocache'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Gavin Shan <shangw@linux.vnet.ibm.com> Cc: Wanpeng Li <liwp.linux@gmail.com> Link: http://lkml.kernel.org/r/1339296652-2935-1-git-send-email-liwp.linux@gmail.com Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | numa_32.c | s | 8.3K | 228 | Tejun Heo | tj@kernel.org | 1310669273 |  | memblock, x86: Replace memblock_x86_reserve/free_range() with generic ones  Other than sanity check and debug message, the x86 specific version of memblock reserve/free functions are simple wrappers around the generic versions - memblock_reserve/free().  This patch adds debug messages with caller identification to the generic versions and replaces x86 specific ones and kills them. arch/x86/include/asm/memblock.h and arch/x86/mm/memblock.c are empty after this change and removed.  Signed-off-by: Tejun Heo <tj@kernel.org> Link: http://lkml.kernel.org/r/1310462166-31469-14-git-send-email-tj@kernel.org Cc: Yinghai Lu <yinghai@kernel.org> Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Ingo Molnar <mingo@redhat.com> Cc: "H. Peter Anvin" <hpa@zytor.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | memtest.c | s | 3.1K | 108 | Tejun Heo | tj@kernel.org | 1310669273 |  | memblock, x86: Replace memblock_x86_reserve/free_range() with generic ones  Other than sanity check and debug message, the x86 specific version of memblock reserve/free functions are simple wrappers around the generic versions - memblock_reserve/free().  This patch adds debug messages with caller identification to the generic versions and replaces x86 specific ones and kills them. arch/x86/include/asm/memblock.h and arch/x86/mm/memblock.c are empty after this change and removed.  Signed-off-by: Tejun Heo <tj@kernel.org> Link: http://lkml.kernel.org/r/1310462166-31469-14-git-send-email-tj@kernel.org Cc: Yinghai Lu <yinghai@kernel.org> Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Ingo Molnar <mingo@redhat.com> Cc: "H. Peter Anvin" <hpa@zytor.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | numa_64.c | s | 446B | 19 | Tejun Heo | tj@kernel.org | 1310669269 |  | memblock, x86: Make free_all_memory_core_early() explicitly free lowmem only  nomemblock is currently used only by x86 and on x86_32 free_all_memory_core_early() silently freed only the low mem because get_free_all_memory_range() in arch/x86/mm/memblock.c implicitly limited range to max_low_pfn.  Rename free_all_memory_core_early() to free_low_memory_core_early() and make it call __get_free_all_memory_range() and limit the range to max_low_pfn explicitly.  This makes things clearer and also is consistent with the bootmem behavior.  This leaves get_free_all_memory_range() without any user.  Kill it.  Signed-off-by: Tejun Heo <tj@kernel.org> Link: http://lkml.kernel.org/r/1310462166-31469-9-git-send-email-tj@kernel.org Cc: Yinghai Lu <yinghai@kernel.org> Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Ingo Molnar <mingo@redhat.com> Cc: "H. Peter Anvin" <hpa@zytor.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | init_64.c | s | 23K | 828 | Linus Torvalds | torvalds@linux-foundation.org | 1337796419 |  | Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip  Pull x86 mm changes from Ingo Molnar:  "This tree includes a micro-optimization that avoids cr3 switches   during idling; it fixes corner cases and there's also small cleanups"  Fix up trivial context conflict with the percpu_xx -> this_cpu_xx changes.  * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:   x86-64: Fix accounting in kernel_physical_mapping_init()   x86/tlb: Clean up and unify TLB_FLUSH_ALL definition   x86: Drop obsolete ARCH_BOOTMEM support   x86, tlb: Switch cr3 in leave_mm() only when needed   x86/mm: Fix the size calculation of mapping tables
f | mmap.c | s | 3.1K | 109 | Ludwig Nussel | ludwig.nussel@suse.de | 1323101243 |  | x86: Fix mmap random address range  On x86_32 casting the unsigned int result of get_random_int() to long may result in a negative value.  On x86_32 the range of mmap_rnd() therefore was -255 to 255.  The 32bit mode on x86_64 used 0 to 255 as intended.  The bug was introduced by 675a081 ("x86: unify mmap_{32||64}.c") in January 2008.  Signed-off-by: Ludwig Nussel <ludwig.nussel@suse.de> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: harvey.harrison@gmail.com Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: Harvey Harrison <harvey.harrison@gmail.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Link: http://lkml.kernel.org/r/201111152246.pAFMklOB028527@wpaz5.hot.corp.google.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | dump_pagetables.c | s | 9.1K | 328 | Andres Salomon | dilinger@queued.net | 1279670179 |  | x86, mm: Create symbolic index into address_markers array  Without this, adding entries into the address_markers array means adding more and more of an #ifdef maze in pt_dump_init().  By using indices, we can keep it a bit saner.  Signed-off-by: Andres Salomon <dilinger@queued.net> LKML-Reference: <201007202219.o6KMJkUs021052@imap1.linux-foundation.org> Cc: Jordan Crouse <jordan.crouse@amd.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | pf_in.h | s | 1.4K | 35 | Pekka Paalanen | pq@iki.fi | 1211621137 |  | x86 mmiotrace: move files into arch/x86/mm/.  Signed-off-by: Pekka Paalanen <pq@iki.fi> Signed-off-by: Ingo Molnar <mingo@elte.hu> Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
f | gup.c | s | 10K | 349 | Youquan Song | youquan.song@intel.com | 1323445828 |  | thp: add compound tail page _mapcount when mapped  With the 3.2-rc kernel, IOMMU 2M pages in KVM works.  But when I tried to use IOMMU 1GB pages in KVM, I encountered an oops and the 1GB page failed to be used.  The root cause is that 1GB page allocation calls gup_huge_pud() while 2M page calls gup_huge_pmd.  If compound pages are used and the page is a tail page, gup_huge_pmd() increases _mapcount to record tail page are mapped while gup_huge_pud does not do that.  So when the mapped page is relesed, it will result in kernel oops because the page is not marked mapped.  This patch add tail process for compound page in 1GB huge page which keeps the same process as 2M page.  Reproduce like: 1. Add grub boot option: hugepagesz=1G hugepages=8 2. mount -t hugetlbfs -o pagesize=1G hugetlbfs /dev/hugepages 3. qemu-kvm -m 2048 -hda os-kvm.img -cpu kvm64 -smp 4 -mem-path /dev/hugepages 	-net none -device pci-assign,host=07:00.1    kernel BUG at mm/swap.c:114!   invalid opcode: 0000 [#1] SMP   Call Trace:     put_page+0x15/0x37     kvm_release_pfn_clean+0x31/0x36     kvm_iommu_put_pages+0x94/0xb1     kvm_iommu_unmap_memslots+0x80/0xb6     kvm_assign_device+0xba/0x117     kvm_vm_ioctl_assigned_device+0x301/0xa47     kvm_vm_ioctl+0x36c/0x3a2     do_vfs_ioctl+0x49e/0x4e4     sys_ioctl+0x5a/0x7c     system_call_fastpath+0x16/0x1b   RIP  put_compound_page+0xd4/0x168  Signed-off-by: Youquan Song <youquan.song@intel.com> Reviewed-by: Andrea Arcangeli <aarcange@redhat.com> Cc: Andi Kleen <andi@firstfloor.org> Cc: <stable@vger.kernel.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | iomap_32.c | s | 3.3K | 101 | Peter Zijlstra | a.p.zijlstra@chello.nl | 1288227785 |  | mm: fix race in kunmap_atomic()  Christoph reported a nice splat which illustrated a race in the new stack based kmap_atomic implementation.  The problem is that we pop our stack slot before we're completely done resetting its state -- in particular clearing the PTE (sometimes that's CONFIG_DEBUG_HIGHMEM).  If an interrupt happens before we actually clear the PTE used for the last slot, that interrupt can reuse the slot in a dirty state, which triggers a BUG in kmap_atomic().  Fix this by introducing kmap_atomic_idx() which reports the current slot index without actually releasing it and use that to find the PTE and delay the _pop() until after we're completely done.  Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl> Reported-by: Christoph Hellwig <hch@infradead.org> Acked-by: Rik van Riel <riel@redhat.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | pf_in.c | s | 10K | 471 | Gustavo F. Padovan | padovan@profusion.mobi | 1305997833 |  | x86: Eliminate various 'set but not used' warnings  Signed-off-by: Gustavo F. Padovan <padovan@profusion.mobi> Cc: Joerg Roedel <joerg.roedel@amd.com> (supporter:AMD IOMMU (AMD-VI)) Cc: iommu@lists.linux-foundation.org (open list:AMD IOMMU (AMD-VI)) Link: http://lkml.kernel.org/r/1305918786-7239-3-git-send-email-padovan@profusion.mobi Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | pgtable_32.c | s | 3.2K | 120 | David Howells | dhowells@redhat.com | 1332954672 |  | Disintegrate asm/system.h for X86  Disintegrate asm/system.h for X86.  Signed-off-by: David Howells <dhowells@redhat.com> Acked-by: H. Peter Anvin <hpa@zytor.com> cc: x86@kernel.org
f | tlb.c | s | 8.9K | 293 | Linus Torvalds | torvalds@linux-foundation.org | 1337796419 |  | Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip  Pull x86 mm changes from Ingo Molnar:  "This tree includes a micro-optimization that avoids cr3 switches   during idling; it fixes corner cases and there's also small cleanups"  Fix up trivial context conflict with the percpu_xx -> this_cpu_xx changes.  * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:   x86-64: Fix accounting in kernel_physical_mapping_init()   x86/tlb: Clean up and unify TLB_FLUSH_ALL definition   x86: Drop obsolete ARCH_BOOTMEM support   x86, tlb: Switch cr3 in leave_mm() only when needed   x86/mm: Fix the size calculation of mapping tables
f | pat_rbtree.c | s | 5.9K | 206 | Peter Zijlstra | peterz@infradead.org | 1278333830 |  | rbtree: Undo augmented trees performance damage and regression  Reimplement augmented RB-trees without sprinkling extra branches all over the RB-tree code (which lives in the scheduler hot path).  This approach is 'borrowed' from Fabio's BFQ implementation and relies on traversing the rebalance path after the RB-tree-op to correct the heap property for insertion/removal and make up for the damage done by the tree rotations.  For insertion the rebalance path is trivially that from the new node upwards to the root, for removal it is that from the deepest node in the path from the to be removed node that will still be around after the removal.  [ This patch also fixes a video driver regression reported by   Ali Gholami Rudi - the memtype->subtree_max_end was updated   incorrectly. ]  Acked-by: Suresh Siddha <suresh.b.siddha@intel.com> Acked-by: Venkatesh Pallipadi <venki@google.com> Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl> Tested-by: Ali Gholami Rudi <ali@rudi.ir> Cc: Fabio Checconi <fabio@gandalf.sssup.it> Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Linus Torvalds <torvalds@linux-foundation.org> LKML-Reference: <1275414172.27810.27961.camel@twins> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | setup_nx.c | s | 1.3K | 55 | Kees Cook | kees.cook@canonical.com | 1289432595 |  | x86, cpu: Only CPU features determine NX capabilities  Fix the NX feature boot warning when NX is missing to correctly reflect that BIOSes cannot disable NX now.  Signed-off-by: Kees Cook <kees.cook@canonical.com> LKML-Reference: <1289414154-7829-5-git-send-email-kees.cook@canonical.com> Acked-by: Pekka Enberg <penberg@kernel.org> Acked-by: Alan Cox <alan@lxorguk.ukuu.org.uk> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | pageattr-test.c | s | 5.3K | 226 | Joe Perches | joe@perches.com | 1306605237 |  | x86: Convert vmalloc()+memset() to vzalloc()  Signed-off-by: Joe Perches <joe@perches.com> Cc: Jiri Kosina <trivial@kernel.org> Link: http://lkml.kernel.org/r/10e35243fda0b8739c89ac32a7bdf348ec4752e1.1306603968.git.joe@perches.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | pat.c | s | 20K | 703 | H. Peter Anvin | hpa@zytor.com | 1338405092 |  | Merge branch 'x86/trampoline' into x86/urgent  x86/trampoline contains an urgent commit which is necessarily on a newer baseline.  Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | extable.c | s | 3.9K | 149 | H. Peter Anvin | hpa@zytor.com | 1334967754 |  | x86, extable: Switch to relative exception table entries  Switch to using relative exception table entries on x86.  On i386, this has the advantage that the exception table entries don't need to be relocated; on x86-64 this means the exception table entries take up only half the space.  In either case, a 32-bit delta is sufficient, as the range of kernel code addresses is limited.  Since part of the goal is to avoid needing to adjust the entries when the kernel is relocated, the old trick of using addresses in the NULL pointer range to indicate uaccess_err no longer works (and unlike RISC architectures we can't use a flag bit); instead use an delta just below +2G to indicate these special entries.  The reach is still limited to a single instruction.  Signed-off-by: H. Peter Anvin <hpa@zytor.com> Cc: David Daney <david.daney@cavium.com> Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com
d | kmemcheck |  | 12 items |  | Paul Gortmaker | paul.gortmaker@windriver.com | 1330553708 |  | bug.h: add include of it to various implicit C users  With bug.h currently living right in linux/kernel.h there are files that use BUG_ON and friends but are not including the header explicitly.  Fix them up so we can remove the presence in kernel.h file.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | pat_internal.h | s | 1.3K | 40 | Xiaotian Feng | dfeng@redhat.com | 1274898364 |  | x86, pat: Fix memory leak in free_memtype  Reserve_memtype will allocate memory for new memtype, but in free_memtype, after the memtype erased from rbtree, the memory is not freed.  Changes since V1: 	make rbt_memtype_erase return erased memtype so that 	it can be freed in free_memtype.  [ hpa: not for -stable: 2.6.34 and earlier not affected ]  Signed-off-by: Xiaotian Feng <dfeng@redhat.com> LKML-Reference: <1274838670-8731-1-git-send-email-dfeng@redhat.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Ingo Molnar <mingo@redhat.com> Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com> Cc: Jack Steiner <steiner@sgi.com> Acked-by: Suresh Siddha <suresh.b.siddha@intel.com> Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
f | init_32.c | s | 24K | 821 | David Howells | dhowells@redhat.com | 1332954672 |  | Disintegrate asm/system.h for X86  Disintegrate asm/system.h for X86.  Signed-off-by: David Howells <dhowells@redhat.com> Acked-by: H. Peter Anvin <hpa@zytor.com> cc: x86@kernel.org
f | highmem_32.c | s | 3.3K | 118 | Cong Wang | amwang@redhat.com | 1332251310 |  | highmem: kill all __kmap_atomic() [swarren@nvidia.com: highmem: Fix ARM build break due to __kmap_atomic rename]  Signed-off-by: Stephen Warren <swarren@nvidia.com> Signed-off-by: Cong Wang <amwang@redhat.com>
f | amdtopology.c | s | 4.3K | 167 | Tejun Heo | tj@kernel.org | 1304349888 |  | x86, NUMA: Enable CONFIG_AMD_NUMA on 32bit too  Now that NUMA init path is unified, amdtopology can be enabled on 32bit.  Make amdtopology.c safe on 32bit by explicitly using u64 and drop X86_64 dependency from Kconfig.  Inclusion of bootmem.h is added for max_pfn declaration.  Signed-off-by: Tejun Heo <tj@kernel.org> Cc: Ingo Molnar <mingo@redhat.com> Cc: Yinghai Lu <yinghai@kernel.org> Cc: David Rientjes <rientjes@google.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: "H. Peter Anvin" <hpa@zytor.com>
f | ioremap.c | s | 15K | 538 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1339404885 |  | x86/mm: Fix some kernel-doc warnings  Fix kernel-doc warnings in arch/x86/mm/ioremap.c and arch/x86/mm/pageattr.c, just like this one:    Warning(arch/x86/mm/ioremap.c:204):      No description found for parameter 'phys_addr'   Warning(arch/x86/mm/ioremap.c:204):      Excess function parameter 'offset' description in 'ioremap_nocache'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Gavin Shan <shangw@linux.vnet.ibm.com> Cc: Wanpeng Li <liwp.linux@gmail.com> Link: http://lkml.kernel.org/r/1339296652-2935-1-git-send-email-liwp.linux@gmail.com Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | srat.c | s | 4.6K | 171 | Yasuaki Ishimatsu | isimatu.yasuaki@jp.fujitsu.com | 1338976719 |  | x86/numa: Set numa_nodes_parsed at acpi_numa_memory_affinity_init()  When hot-adding a CPU, the system outputs following messages since node_to_cpumask_map[2] was not allocated memory.  Booting Node 2 Processor 32 APIC 0xc0 node_to_cpumask_map[2] NULL Pid: 0, comm: swapper/32 Tainted: G       A     3.3.5-acd #21 Call Trace:  [<ffffffff81048845>] debug_cpumask_set_cpu+0x155/0x160  [<ffffffff8105e28a>] ? add_timer_on+0xaa/0x120  [<ffffffff8150665f>] numa_add_cpu+0x1e/0x22  [<ffffffff815020bb>] identify_cpu+0x1df/0x1e4  [<ffffffff815020d6>] identify_econdary_cpu+0x16/0x1d  [<ffffffff81504614>] smp_store_cpu_info+0x3c/0x3e  [<ffffffff81505263>] smp_callin+0x139/0x1be  [<ffffffff815052fb>] start_secondary+0x13/0xeb  The reason is that the bit of node 2 was not set at numa_nodes_parsed. numa_nodes_parsed is set by only acpi_numa_processor_affinity_init / acpi_numa_x2apic_affinity_init. Thus even if hot-added memory which is same PXM as hot-added CPU is written in ACPI SRAT Table, if the hot-added CPU is not written in ACPI SRAT table, numa_nodes_parsed is not set.  But according to ACPI Spec Rev 5.0, it says about ACPI SRAT table as follows: This optional table provides information that allows OSPM to associate processors and memory ranges, including ranges of memory provided by hot-added memory devices, with system localities / proximity domains and clock domains.  It means that ACPI SRAT table only provides information for CPUs present at boot time and for memory including hot-added memory. So hot-added memory is written in ACPI SRAT table, but hot-added CPU is not written in it. Thus numa_nodes_parsed should be set by not only acpi_numa_processor_affinity_init / acpi_numa_x2apic_affinity_init but also acpi_numa_memory_affinity_init for the case.  Additionally, if system has cpuless memory node, acpi_numa_processor_affinity_init / acpi_numa_x2apic_affinity_init cannot set numa_nodes_parseds since these functions cannot find cpu description for the node. In this case, numa_nodes_parsed needs to be set by acpi_numa_memory_affinity_init.  Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com> Acked-by: David Rientjes <rientjes@google.com> Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: liuj97@gmail.com Cc: kosaki.motohiro@gmail.com Link: http://lkml.kernel.org/r/4FCC2098.4030007@jp.fujitsu.com [ merged it ] Signed-off-by: Ingo Molnar <mingo@kernel.org>
f | numa_internal.h | s | 889B | 31 | Tejun Heo | tj@kernel.org | 1304338734 |  | x86, NUMA: Initialize and use remap allocator from setup_node_bootmem()  setup_node_bootmem() is taken from 64bit and doesn't use remap allocator.  It's about to be shared with 32bit so add support for it. If NODE_DATA is remapped, it's noted in the debug message and node locality check is skipped as the __pa() of the remapped address doesn't reflect the actual physical address.  On 64bit, remap allocator becomes noop and doesn't affect the behavior.  Signed-off-by: Tejun Heo <tj@kernel.org> Cc: Ingo Molnar <mingo@redhat.com> Cc: Yinghai Lu <yinghai@kernel.org> Cc: David Rientjes <rientjes@google.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: "H. Peter Anvin" <hpa@zytor.com>
f | pgtable.c | s | 10.0K | 360 | Shaohua Li | shaohua.li@intel.com | 1300445041 |  | x86: Flush TLB if PGD entry is changed in i386 PAE mode  According to intel CPU manual, every time PGD entry is changed in i386 PAE mode, we need do a full TLB flush. Current code follows this and there is comment for this too in the code.  But current code misses the multi-threaded case. A changed page table might be used by several CPUs, every such CPU should flush TLB. Usually this isn't a problem, because we prepopulate all PGD entries at process fork. But when the process does munmap and follows new mmap, this issue will be triggered.  When it happens, some CPUs keep doing page faults:    http://marc.info/?l=linux-kernel&m=129915020508238&w=2  Reported-by: Yasunori Goto<y-goto@jp.fujitsu.com> Tested-by: Yasunori Goto<y-goto@jp.fujitsu.com> Reviewed-by: Rik van Riel <riel@redhat.com> Signed-off-by: Shaohua Li<shaohua.li@intel.com> Cc: Mallick Asit K <asit.k.mallick@intel.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: linux-mm <linux-mm@kvack.org> Cc: stable <stable@kernel.org> LKML-Reference: <1300246649.2337.95.camel@sli10-conroe> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | kmmio.c | s | 15K | 517 | Marcin Slusarz | marcin.slusarz@gmail.com | 1276853409 |  | x86, kmmio/mmiotrace: Fix double free of kmmio_fault_pages  After every iounmap mmiotrace has to free kmmio_fault_pages, but it can't do it directly, so it defers freeing by RCU.  It usually works, but when mmiotraced code calls ioremap-iounmap multiple times without sleeping between (so RCU won't kick in and start freeing) it can be given the same virtual address, so at every iounmap mmiotrace will schedule the same pages for release. Obviously it will explode on second free.  Fix it by marking kmmio_fault_pages which are scheduled for release and not adding them second time.  Signed-off-by: Marcin Slusarz <marcin.slusarz@gmail.com> Tested-by: Marcin Kocielnicki <koriakin@0x04.net> Tested-by: Shinpei KATO <shinpei@il.is.s.u-tokyo.ac.jp> Acked-by: Pekka Paalanen <pq@iki.fi> Cc: Stuart Bennett <stuart@freedesktop.org> Cc: Marcin Kocielnicki <koriakin@0x04.net> Cc: nouveau@lists.freedesktop.org Cc: <stable@kernel.org> LKML-Reference: <20100613215654.GA3829@joi.lan> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | hugetlbpage.c | s | 10K | 381 | Xiao Guangrong | xiaoguangrong@linux.vnet.ibm.com | 1332377699 |  | hugetlb: remove prev_vma from hugetlb_get_unmapped_area_topdown()  After looking up the vma which covers or follows the cached search address, the following condition is always true:  	!prev_vma |||| (addr >= prev_vma->vm_end)  so we can stop checking the previous VMA altogether.  Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | fault.c | s | 28K | 1021 | Eric W. Biederman | ebiederm@xmission.com | 1336040918 |  | userns: Store uid and gid values in struct cred with kuid_t and kgid_t types  cred.h and a few trivial users of struct cred are changed.  The rest of the users of struct cred are left for other patches as there are too many changes to make in one go and leave the change reviewable.  If the user namespace is disabled and CONFIG_UIDGID_STRICT_TYPE_CHECKS are disabled the code will contiue to compile and behave correctly.  Acked-by: Serge Hallyn <serge.hallyn@canonical.com> Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
f | numa.c | s | 20K | 718 | Bjorn Helgaas | bhelgaas@google.com | 1338333741 |  | x86: print physical addresses consistently with other parts of kernel  Print physical address info in a style consistent with the %pR style used elsewhere in the kernel.  For example:      -found SMP MP-table at [ffff8800000fce90] fce90     +found SMP MP-table at [mem 0x000fce90-0x000fce9f] mapped at [ffff8800000fce90]     -initial memory mapped : 0 - 20000000     +initial memory mapped: [mem 0x00000000-0x1fffffff]     -Base memory trampoline at [ffff88000009c000] 9c000 size 8192     +Base memory trampoline [mem 0x0009c000-0x0009dfff] mapped at [ffff88000009c000]     -SRAT: Node 0 PXM 0 0-80000000     +SRAT: Node 0 PXM 0 [mem 0x00000000-0x7fffffff]  Signed-off-by: Bjorn Helgaas <bhelgaas@google.com> Cc: Yinghai Lu <yinghai@kernel.org> Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: Thomas Gleixner <tglx@linutronix.de> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | testmmiotrace.c | s | 3.0K | 114 | Marcin Slusarz | marcin.slusarz@gmail.com | 1276853409 |  | x86, kmmio/mmiotrace: Fix double free of kmmio_fault_pages  After every iounmap mmiotrace has to free kmmio_fault_pages, but it can't do it directly, so it defers freeing by RCU.  It usually works, but when mmiotraced code calls ioremap-iounmap multiple times without sleeping between (so RCU won't kick in and start freeing) it can be given the same virtual address, so at every iounmap mmiotrace will schedule the same pages for release. Obviously it will explode on second free.  Fix it by marking kmmio_fault_pages which are scheduled for release and not adding them second time.  Signed-off-by: Marcin Slusarz <marcin.slusarz@gmail.com> Tested-by: Marcin Kocielnicki <koriakin@0x04.net> Tested-by: Shinpei KATO <shinpei@il.is.s.u-tokyo.ac.jp> Acked-by: Pekka Paalanen <pq@iki.fi> Cc: Stuart Bennett <stuart@freedesktop.org> Cc: Marcin Kocielnicki <koriakin@0x04.net> Cc: nouveau@lists.freedesktop.org Cc: <stable@kernel.org> LKML-Reference: <20100613215654.GA3829@joi.lan> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | numa_emulation.c | s | 13K | 433 | Bjorn Helgaas | bhelgaas@google.com | 1338333741 |  | x86: print physical addresses consistently with other parts of kernel  Print physical address info in a style consistent with the %pR style used elsewhere in the kernel.  For example:      -found SMP MP-table at [ffff8800000fce90] fce90     +found SMP MP-table at [mem 0x000fce90-0x000fce9f] mapped at [ffff8800000fce90]     -initial memory mapped : 0 - 20000000     +initial memory mapped: [mem 0x00000000-0x1fffffff]     -Base memory trampoline at [ffff88000009c000] 9c000 size 8192     +Base memory trampoline [mem 0x0009c000-0x0009dfff] mapped at [ffff88000009c000]     -SRAT: Node 0 PXM 0 0-80000000     +SRAT: Node 0 PXM 0 [mem 0x00000000-0x7fffffff]  Signed-off-by: Bjorn Helgaas <bhelgaas@google.com> Cc: Yinghai Lu <yinghai@kernel.org> Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: Thomas Gleixner <tglx@linutronix.de> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | physaddr.c | s | 1.4K | 60 | Jeremy Fitzhardinge | jeremy.fitzhardinge@citrix.com | 1252608535 |  | x86: split __phys_addr out into separate file  Split __phys_addr out into its own file so we can disable -fstack-protector in a fine-grained fashion.  Also it doesn't have terribly much to do with the rest of ioremap.c.  Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
