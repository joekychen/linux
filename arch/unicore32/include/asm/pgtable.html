<!DOCTYPE html>
<html><head><title>joekychen/linux » arch › unicore32 › include › asm › pgtable.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../../../../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../../../../index.html"></a><h1>pgtable.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cm">/*</span>
<span class="cm"> * linux/arch/unicore32/include/asm/pgtable.h</span>
<span class="cm"> *</span>
<span class="cm"> * Code specific to PKUnity SoC and UniCore ISA</span>
<span class="cm"> *</span>
<span class="cm"> * Copyright (C) 2001-2010 GUAN Xue-tao</span>
<span class="cm"> *</span>
<span class="cm"> * This program is free software; you can redistribute it and/or modify</span>
<span class="cm"> * it under the terms of the GNU General Public License version 2 as</span>
<span class="cm"> * published by the Free Software Foundation.</span>
<span class="cm"> */</span>
<span class="cp">#ifndef __UNICORE_PGTABLE_H__</span>
<span class="cp">#define __UNICORE_PGTABLE_H__</span>

<span class="cp">#include &lt;asm-generic/pgtable-nopmd.h&gt;</span>
<span class="cp">#include &lt;asm/cpu-single.h&gt;</span>

<span class="cp">#include &lt;asm/memory.h&gt;</span>
<span class="cp">#include &lt;asm/pgtable-hwdef.h&gt;</span>

<span class="cm">/*</span>
<span class="cm"> * Just any arbitrary offset to the start of the vmalloc VM area: the</span>
<span class="cm"> * current 8MB value just means that there will be a 8MB &quot;hole&quot; after the</span>
<span class="cm"> * physical memory until the kernel virtual memory starts.  That means that</span>
<span class="cm"> * any out-of-bounds memory accesses will hopefully be caught.</span>
<span class="cm"> * The vmalloc() routines leaves a hole of 4kB between each vmalloced</span>
<span class="cm"> * area for the same reason. ;)</span>
<span class="cm"> *</span>
<span class="cm"> * Note that platforms may override VMALLOC_START, but they must provide</span>
<span class="cm"> * VMALLOC_END.  VMALLOC_END defines the (exclusive) limit of this space,</span>
<span class="cm"> * which may not overlap IO space.</span>
<span class="cm"> */</span>
<span class="cp">#ifndef VMALLOC_START</span>
<span class="cp">#define VMALLOC_OFFSET		SZ_8M</span>
<span class="cp">#define VMALLOC_START		(((unsigned long)high_memory + VMALLOC_OFFSET) \</span>
<span class="cp">					&amp; ~(VMALLOC_OFFSET-1))</span>
<span class="cp">#define VMALLOC_END		(0xff000000UL)</span>
<span class="cp">#endif</span>

<span class="cp">#define PTRS_PER_PTE		1024</span>
<span class="cp">#define PTRS_PER_PGD		1024</span>

<span class="cm">/*</span>
<span class="cm"> * PGDIR_SHIFT determines what a third-level page table entry can map</span>
<span class="cm"> */</span>
<span class="cp">#define PGDIR_SHIFT		22</span>

<span class="cp">#ifndef __ASSEMBLY__</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">__pte_error</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">file</span><span class="p">,</span> <span class="kt">int</span> <span class="n">line</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">val</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">__pgd_error</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">file</span><span class="p">,</span> <span class="kt">int</span> <span class="n">line</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">val</span><span class="p">);</span>

<span class="cp">#define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))</span>
<span class="cp">#define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))</span>
<span class="cp">#endif </span><span class="cm">/* !__ASSEMBLY__ */</span><span class="cp"></span>

<span class="cp">#define PGDIR_SIZE		(1UL &lt;&lt; PGDIR_SHIFT)</span>
<span class="cp">#define PGDIR_MASK		(~(PGDIR_SIZE-1))</span>

<span class="cm">/*</span>
<span class="cm"> * This is the lowest virtual address we can permit any user space</span>
<span class="cm"> * mapping to be mapped at.  This is particularly important for</span>
<span class="cm"> * non-high vector CPUs.</span>
<span class="cm"> */</span>
<span class="cp">#define FIRST_USER_ADDRESS	PAGE_SIZE</span>

<span class="cp">#define FIRST_USER_PGD_NR	1</span>
<span class="cp">#define USER_PTRS_PER_PGD	((TASK_SIZE/PGDIR_SIZE) - FIRST_USER_PGD_NR)</span>

<span class="cm">/*</span>
<span class="cm"> * section address mask and size definitions.</span>
<span class="cm"> */</span>
<span class="cp">#define SECTION_SHIFT		22</span>
<span class="cp">#define SECTION_SIZE		(1UL &lt;&lt; SECTION_SHIFT)</span>
<span class="cp">#define SECTION_MASK		(~(SECTION_SIZE-1))</span>

<span class="cp">#ifndef __ASSEMBLY__</span>

<span class="cm">/*</span>
<span class="cm"> * The pgprot_* and protection_map entries will be fixed up in runtime</span>
<span class="cm"> * to include the cachable bits based on memory policy, as well as any</span>
<span class="cm"> * architecture dependent bits.</span>
<span class="cm"> */</span>
<span class="cp">#define _PTE_DEFAULT		(PTE_PRESENT | PTE_YOUNG | PTE_CACHEABLE)</span>

<span class="k">extern</span> <span class="n">pgprot_t</span> <span class="n">pgprot_user</span><span class="p">;</span>
<span class="k">extern</span> <span class="n">pgprot_t</span> <span class="n">pgprot_kernel</span><span class="p">;</span>

<span class="cp">#define PAGE_NONE		pgprot_user</span>
<span class="cp">#define PAGE_SHARED		__pgprot(pgprot_val(pgprot_user | PTE_READ \</span>
<span class="cp">								| PTE_WRITE)</span>
<span class="cp">#define PAGE_SHARED_EXEC	__pgprot(pgprot_val(pgprot_user | PTE_READ \</span>
<span class="cp">								| PTE_WRITE \</span>
<span class="cp">								| PTE_EXEC)</span>
<span class="cp">#define PAGE_COPY		__pgprot(pgprot_val(pgprot_user | PTE_READ)</span>
<span class="cp">#define PAGE_COPY_EXEC		__pgprot(pgprot_val(pgprot_user | PTE_READ \</span>
<span class="cp">								| PTE_EXEC)</span>
<span class="cp">#define PAGE_READONLY		__pgprot(pgprot_val(pgprot_user | PTE_READ)</span>
<span class="cp">#define PAGE_READONLY_EXEC	__pgprot(pgprot_val(pgprot_user | PTE_READ \</span>
<span class="cp">								| PTE_EXEC)</span>
<span class="cp">#define PAGE_KERNEL		pgprot_kernel</span>
<span class="cp">#define PAGE_KERNEL_EXEC	__pgprot(pgprot_val(pgprot_kernel | PTE_EXEC))</span>

<span class="cp">#define __PAGE_NONE		__pgprot(_PTE_DEFAULT)</span>
<span class="cp">#define __PAGE_SHARED		__pgprot(_PTE_DEFAULT | PTE_READ \</span>
<span class="cp">							| PTE_WRITE)</span>
<span class="cp">#define __PAGE_SHARED_EXEC	__pgprot(_PTE_DEFAULT | PTE_READ \</span>
<span class="cp">							| PTE_WRITE \</span>
<span class="cp">							| PTE_EXEC)</span>
<span class="cp">#define __PAGE_COPY		__pgprot(_PTE_DEFAULT | PTE_READ)</span>
<span class="cp">#define __PAGE_COPY_EXEC	__pgprot(_PTE_DEFAULT | PTE_READ \</span>
<span class="cp">							| PTE_EXEC)</span>
<span class="cp">#define __PAGE_READONLY		__pgprot(_PTE_DEFAULT | PTE_READ)</span>
<span class="cp">#define __PAGE_READONLY_EXEC	__pgprot(_PTE_DEFAULT | PTE_READ \</span>
<span class="cp">							| PTE_EXEC)</span>

<span class="cp">#endif </span><span class="cm">/* __ASSEMBLY__ */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * The table below defines the page protection levels that we insert into our</span>
<span class="cm"> * Linux page table version.  These get translated into the best that the</span>
<span class="cm"> * architecture can perform.  Note that on UniCore hardware:</span>
<span class="cm"> *  1) We cannot do execute protection</span>
<span class="cm"> *  2) If we could do execute protection, then read is implied</span>
<span class="cm"> *  3) write implies read permissions</span>
<span class="cm"> */</span>
<span class="cp">#define __P000  __PAGE_NONE</span>
<span class="cp">#define __P001  __PAGE_READONLY</span>
<span class="cp">#define __P010  __PAGE_COPY</span>
<span class="cp">#define __P011  __PAGE_COPY</span>
<span class="cp">#define __P100  __PAGE_READONLY_EXEC</span>
<span class="cp">#define __P101  __PAGE_READONLY_EXEC</span>
<span class="cp">#define __P110  __PAGE_COPY_EXEC</span>
<span class="cp">#define __P111  __PAGE_COPY_EXEC</span>

<span class="cp">#define __S000  __PAGE_NONE</span>
<span class="cp">#define __S001  __PAGE_READONLY</span>
<span class="cp">#define __S010  __PAGE_SHARED</span>
<span class="cp">#define __S011  __PAGE_SHARED</span>
<span class="cp">#define __S100  __PAGE_READONLY_EXEC</span>
<span class="cp">#define __S101  __PAGE_READONLY_EXEC</span>
<span class="cp">#define __S110  __PAGE_SHARED_EXEC</span>
<span class="cp">#define __S111  __PAGE_SHARED_EXEC</span>

<span class="cp">#ifndef __ASSEMBLY__</span>
<span class="cm">/*</span>
<span class="cm"> * ZERO_PAGE is a global shared page that is always zero: used</span>
<span class="cm"> * for zero-mapped memory areas etc..</span>
<span class="cm"> */</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">empty_zero_page</span><span class="p">;</span>
<span class="cp">#define ZERO_PAGE(vaddr)		(empty_zero_page)</span>

<span class="cp">#define pte_pfn(pte)			(pte_val(pte) &gt;&gt; PAGE_SHIFT)</span>
<span class="cp">#define pfn_pte(pfn, prot)		(__pte(((pfn) &lt;&lt; PAGE_SHIFT) \</span>
<span class="cp">						| pgprot_val(prot)))</span>

<span class="cp">#define pte_none(pte)			(!pte_val(pte))</span>
<span class="cp">#define pte_clear(mm, addr, ptep)	set_pte(ptep, __pte(0))</span>
<span class="cp">#define pte_page(pte)			(pfn_to_page(pte_pfn(pte)))</span>
<span class="cp">#define pte_offset_kernel(dir, addr)	(pmd_page_vaddr(*(dir)) \</span>
<span class="cp">						+ __pte_index(addr))</span>

<span class="cp">#define pte_offset_map(dir, addr)	(pmd_page_vaddr(*(dir)) \</span>
<span class="cp">						+ __pte_index(addr))</span>
<span class="cp">#define pte_unmap(pte)			do { } while (0)</span>

<span class="cp">#define set_pte(ptep, pte)	cpu_set_pte(ptep, pte)</span>

<span class="cp">#define set_pte_at(mm, addr, ptep, pteval)	\</span>
<span class="cp">	do {					\</span>
<span class="cp">		set_pte(ptep, pteval);          \</span>
<span class="cp">	} while (0)</span>

<span class="cm">/*</span>
<span class="cm"> * The following only work if pte_present() is true.</span>
<span class="cm"> * Undefined behaviour if not..</span>
<span class="cm"> */</span>
<span class="cp">#define pte_present(pte)	(pte_val(pte) &amp; PTE_PRESENT)</span>
<span class="cp">#define pte_write(pte)		(pte_val(pte) &amp; PTE_WRITE)</span>
<span class="cp">#define pte_dirty(pte)		(pte_val(pte) &amp; PTE_DIRTY)</span>
<span class="cp">#define pte_young(pte)		(pte_val(pte) &amp; PTE_YOUNG)</span>
<span class="cp">#define pte_exec(pte)		(pte_val(pte) &amp; PTE_EXEC)</span>
<span class="cp">#define pte_special(pte)	(0)</span>

<span class="cp">#define PTE_BIT_FUNC(fn, op) \</span>
<span class="cp">static inline pte_t pte_##fn(pte_t pte) { pte_val(pte) op; return pte; }</span>

<span class="n">PTE_BIT_FUNC</span><span class="p">(</span><span class="n">wrprotect</span><span class="p">,</span> <span class="o">&amp;=</span> <span class="o">~</span><span class="n">PTE_WRITE</span><span class="p">);</span>
<span class="n">PTE_BIT_FUNC</span><span class="p">(</span><span class="n">mkwrite</span><span class="p">,</span>   <span class="o">|=</span> <span class="n">PTE_WRITE</span><span class="p">);</span>
<span class="n">PTE_BIT_FUNC</span><span class="p">(</span><span class="n">mkclean</span><span class="p">,</span>   <span class="o">&amp;=</span> <span class="o">~</span><span class="n">PTE_DIRTY</span><span class="p">);</span>
<span class="n">PTE_BIT_FUNC</span><span class="p">(</span><span class="n">mkdirty</span><span class="p">,</span>   <span class="o">|=</span> <span class="n">PTE_DIRTY</span><span class="p">);</span>
<span class="n">PTE_BIT_FUNC</span><span class="p">(</span><span class="n">mkold</span><span class="p">,</span>     <span class="o">&amp;=</span> <span class="o">~</span><span class="n">PTE_YOUNG</span><span class="p">);</span>
<span class="n">PTE_BIT_FUNC</span><span class="p">(</span><span class="n">mkyoung</span><span class="p">,</span>   <span class="o">|=</span> <span class="n">PTE_YOUNG</span><span class="p">);</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">pte_t</span> <span class="nf">pte_mkspecial</span><span class="p">(</span><span class="n">pte_t</span> <span class="n">pte</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">pte</span><span class="p">;</span> <span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Mark the prot value as uncacheable.</span>
<span class="cm"> */</span>
<span class="cp">#define pgprot_noncached(prot)		\</span>
<span class="cp">	__pgprot(pgprot_val(prot) &amp; ~PTE_CACHEABLE)</span>
<span class="cp">#define pgprot_writecombine(prot)	\</span>
<span class="cp">	__pgprot(pgprot_val(prot) &amp; ~PTE_CACHEABLE)</span>
<span class="cp">#define pgprot_dmacoherent(prot)	\</span>
<span class="cp">	__pgprot(pgprot_val(prot) &amp; ~PTE_CACHEABLE)</span>

<span class="cp">#define pmd_none(pmd)		(!pmd_val(pmd))</span>
<span class="cp">#define pmd_present(pmd)	(pmd_val(pmd) &amp; PMD_PRESENT)</span>
<span class="cp">#define pmd_bad(pmd)		(((pmd_val(pmd) &amp;		\</span>
<span class="cp">				(PMD_PRESENT | PMD_TYPE_MASK))	\</span>
<span class="cp">				!= (PMD_PRESENT | PMD_TYPE_TABLE)))</span>

<span class="cp">#define set_pmd(pmdpd, pmdval)		\</span>
<span class="cp">	do {				\</span>
<span class="cp">		*(pmdpd) = pmdval;	\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#define pmd_clear(pmdp)			\</span>
<span class="cp">	do {				\</span>
<span class="cp">		set_pmd(pmdp, __pmd(0));\</span>
<span class="cp">		clean_pmd_entry(pmdp);	\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#define pmd_page_vaddr(pmd) ((pte_t *)__va(pmd_val(pmd) &amp; PAGE_MASK))</span>
<span class="cp">#define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd)))</span>

<span class="cm">/*</span>
<span class="cm"> * Conversion functions: convert a page and protection to a page entry,</span>
<span class="cm"> * and a page entry and page directory to the page they refer to.</span>
<span class="cm"> */</span>
<span class="cp">#define mk_pte(page, prot)	pfn_pte(page_to_pfn(page), prot)</span>

<span class="cm">/* to find an entry in a page-table-directory */</span>
<span class="cp">#define pgd_index(addr)		((addr) &gt;&gt; PGDIR_SHIFT)</span>

<span class="cp">#define pgd_offset(mm, addr)	((mm)-&gt;pgd+pgd_index(addr))</span>

<span class="cm">/* to find an entry in a kernel page-table-directory */</span>
<span class="cp">#define pgd_offset_k(addr)	pgd_offset(&amp;init_mm, addr)</span>

<span class="cm">/* Find an entry in the third-level page table.. */</span>
<span class="cp">#define __pte_index(addr)	(((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">pte_t</span> <span class="nf">pte_modify</span><span class="p">(</span><span class="n">pte_t</span> <span class="n">pte</span><span class="p">,</span> <span class="n">pgprot_t</span> <span class="n">newprot</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">PTE_EXEC</span> <span class="o">|</span> <span class="n">PTE_WRITE</span> <span class="o">|</span> <span class="n">PTE_READ</span><span class="p">;</span>
	<span class="n">pte_val</span><span class="p">(</span><span class="n">pte</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">pte_val</span><span class="p">(</span><span class="n">pte</span><span class="p">)</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">pgprot_val</span><span class="p">(</span><span class="n">newprot</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">pte</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="n">pgd_t</span> <span class="n">swapper_pg_dir</span><span class="p">[</span><span class="n">PTRS_PER_PGD</span><span class="p">];</span>

<span class="cm">/*</span>
<span class="cm"> * Encode and decode a swap entry.  Swap entries are stored in the Linux</span>
<span class="cm"> * page tables as follows:</span>
<span class="cm"> *</span>
<span class="cm"> *   3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1</span>
<span class="cm"> *   1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0</span>
<span class="cm"> *   &lt;--------------- offset --------------&gt; &lt;--- type --&gt; 0 0 0 0 0</span>
<span class="cm"> *</span>
<span class="cm"> * This gives us up to 127 swap files and 32GB per swap file.  Note that</span>
<span class="cm"> * the offset field is always non-zero.</span>
<span class="cm"> */</span>
<span class="cp">#define __SWP_TYPE_SHIFT	5</span>
<span class="cp">#define __SWP_TYPE_BITS		7</span>
<span class="cp">#define __SWP_TYPE_MASK		((1 &lt;&lt; __SWP_TYPE_BITS) - 1)</span>
<span class="cp">#define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)</span>

<span class="cp">#define __swp_type(x)		(((x).val &gt;&gt; __SWP_TYPE_SHIFT)		\</span>
<span class="cp">				&amp; __SWP_TYPE_MASK)</span>
<span class="cp">#define __swp_offset(x)		((x).val &gt;&gt; __SWP_OFFSET_SHIFT)</span>
<span class="cp">#define __swp_entry(type, offset) ((swp_entry_t) {			\</span>
<span class="cp">				((type) &lt;&lt; __SWP_TYPE_SHIFT) |		\</span>
<span class="cp">				((offset) &lt;&lt; __SWP_OFFSET_SHIFT) })</span>

<span class="cp">#define __pte_to_swp_entry(pte)	((swp_entry_t) { pte_val(pte) })</span>
<span class="cp">#define __swp_entry_to_pte(swp)	((pte_t) { (swp).val })</span>

<span class="cm">/*</span>
<span class="cm"> * It is an error for the kernel to have more swap files than we can</span>
<span class="cm"> * encode in the PTEs.  This ensures that we know when MAX_SWAPFILES</span>
<span class="cm"> * is increased beyond what we presently support.</span>
<span class="cm"> */</span>
<span class="cp">#define MAX_SWAPFILES_CHECK()	\</span>
<span class="cp">	BUILD_BUG_ON(MAX_SWAPFILES_SHIFT &gt; __SWP_TYPE_BITS)</span>

<span class="cm">/*</span>
<span class="cm"> * Encode and decode a file entry.  File entries are stored in the Linux</span>
<span class="cm"> * page tables as follows:</span>
<span class="cm"> *</span>
<span class="cm"> *   3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1</span>
<span class="cm"> *   1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0</span>
<span class="cm"> *   &lt;----------------------- offset ----------------------&gt; 1 0 0 0</span>
<span class="cm"> */</span>
<span class="cp">#define pte_file(pte)		(pte_val(pte) &amp; PTE_FILE)</span>
<span class="cp">#define pte_to_pgoff(x)		(pte_val(x) &gt;&gt; 4)</span>
<span class="cp">#define pgoff_to_pte(x)		__pte(((x) &lt;&lt; 4) | PTE_FILE)</span>

<span class="cp">#define PTE_FILE_MAX_BITS	28</span>

<span class="cm">/* Needs to be defined here and not in linux/mm.h, as it is arch dependent */</span>
<span class="cm">/* FIXME: this is not correct */</span>
<span class="cp">#define kern_addr_valid(addr)	(1)</span>

<span class="cp">#include &lt;asm-generic/pgtable.h&gt;</span>

<span class="cm">/*</span>
<span class="cm"> * remap a physical page `pfn&#39; of size `size&#39; with page protection `prot&#39;</span>
<span class="cm"> * into virtual address `from&#39;</span>
<span class="cm"> */</span>
<span class="cp">#define io_remap_pfn_range(vma, from, pfn, size, prot)	\</span>
<span class="cp">		remap_pfn_range(vma, from, pfn, size, prot)</span>

<span class="cp">#define pgtable_cache_init() do { } while (0)</span>

<span class="cp">#endif </span><span class="cm">/* !__ASSEMBLY__ */</span><span class="cp"></span>

<span class="cp">#endif </span><span class="cm">/* __UNICORE_PGTABLE_H__ */</span><span class="cp"></span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:4}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../../../../javascript/docco.min.js"></script>
</html>
