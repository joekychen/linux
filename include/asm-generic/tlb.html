<!DOCTYPE html>
<html><head><title>joekychen/linux » include › asm-generic › tlb.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../../index.html"></a><h1>tlb.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* include/asm-generic/tlb.h</span>
<span class="cm"> *</span>
<span class="cm"> *	Generic TLB shootdown code</span>
<span class="cm"> *</span>
<span class="cm"> * Copyright 2001 Red Hat, Inc.</span>
<span class="cm"> * Based on code from mm/memory.c Copyright Linus Torvalds and others.</span>
<span class="cm"> *</span>
<span class="cm"> * Copyright 2011 Red Hat, Inc., Peter Zijlstra &lt;pzijlstr@redhat.com&gt;</span>
<span class="cm"> *</span>
<span class="cm"> * This program is free software; you can redistribute it and/or</span>
<span class="cm"> * modify it under the terms of the GNU General Public License</span>
<span class="cm"> * as published by the Free Software Foundation; either version</span>
<span class="cm"> * 2 of the License, or (at your option) any later version.</span>
<span class="cm"> */</span>
<span class="cp">#ifndef _ASM_GENERIC__TLB_H</span>
<span class="cp">#define _ASM_GENERIC__TLB_H</span>

<span class="cp">#include &lt;linux/swap.h&gt;</span>
<span class="cp">#include &lt;asm/pgalloc.h&gt;</span>
<span class="cp">#include &lt;asm/tlbflush.h&gt;</span>

<span class="cp">#ifdef CONFIG_HAVE_RCU_TABLE_FREE</span>
<span class="cm">/*</span>
<span class="cm"> * Semi RCU freeing of the page directories.</span>
<span class="cm"> *</span>
<span class="cm"> * This is needed by some architectures to implement software pagetable walkers.</span>
<span class="cm"> *</span>
<span class="cm"> * gup_fast() and other software pagetable walkers do a lockless page-table</span>
<span class="cm"> * walk and therefore needs some synchronization with the freeing of the page</span>
<span class="cm"> * directories. The chosen means to accomplish that is by disabling IRQs over</span>
<span class="cm"> * the walk.</span>
<span class="cm"> *</span>
<span class="cm"> * Architectures that use IPIs to flush TLBs will then automagically DTRT,</span>
<span class="cm"> * since we unlink the page, flush TLBs, free the page. Since the disabling of</span>
<span class="cm"> * IRQs delays the completion of the TLB flush we can never observe an already</span>
<span class="cm"> * freed page.</span>
<span class="cm"> *</span>
<span class="cm"> * Architectures that do not have this (PPC) need to delay the freeing by some</span>
<span class="cm"> * other means, this is that means.</span>
<span class="cm"> *</span>
<span class="cm"> * What we do is batch the freed directory pages (tables) and RCU free them.</span>
<span class="cm"> * We use the sched RCU variant, as that guarantees that IRQ/preempt disabling</span>
<span class="cm"> * holds off grace periods.</span>
<span class="cm"> *</span>
<span class="cm"> * However, in order to batch these pages we need to allocate storage, this</span>
<span class="cm"> * allocation is deep inside the MM code and can thus easily fail on memory</span>
<span class="cm"> * pressure. To guarantee progress we fall back to single table freeing, see</span>
<span class="cm"> * the implementation of tlb_remove_table_one().</span>
<span class="cm"> *</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">mmu_table_batch</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_head</span>		<span class="n">rcu</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">nr</span><span class="p">;</span>
	<span class="kt">void</span>			<span class="o">*</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">};</span>

<span class="cp">#define MAX_TABLE_BATCH		\</span>
<span class="cp">	((PAGE_SIZE - sizeof(struct mmu_table_batch)) / sizeof(void *))</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">tlb_table_flush</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">tlb_remove_table</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">table</span><span class="p">);</span>

<span class="cp">#endif</span>

<span class="cm">/*</span>
<span class="cm"> * If we can&#39;t allocate a page to make a big batch of page pointers</span>
<span class="cm"> * to work on, then just handle a few from the on-stack structure.</span>
<span class="cm"> */</span>
<span class="cp">#define MMU_GATHER_BUNDLE	8</span>

<span class="k">struct</span> <span class="n">mmu_gather_batch</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">mmu_gather_batch</span>	<span class="o">*</span><span class="n">next</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">nr</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">max</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">page</span>		<span class="o">*</span><span class="n">pages</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">};</span>

<span class="cp">#define MAX_GATHER_BATCH	\</span>
<span class="cp">	((PAGE_SIZE - sizeof(struct mmu_gather_batch)) / sizeof(void *))</span>

<span class="cm">/* struct mmu_gather is an opaque type used by the mm code for passing around</span>
<span class="cm"> * any data needed by arch specific code for tlb_remove_page.</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">mmu_gather</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">mm_struct</span>	<span class="o">*</span><span class="n">mm</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_HAVE_RCU_TABLE_FREE</span>
	<span class="k">struct</span> <span class="n">mmu_table_batch</span>	<span class="o">*</span><span class="n">batch</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">need_flush</span> <span class="o">:</span> <span class="mi">1</span><span class="p">,</span>	<span class="cm">/* Did free PTEs */</span>
				<span class="n">fast_mode</span>  <span class="o">:</span> <span class="mi">1</span><span class="p">;</span> <span class="cm">/* No batching   */</span>

	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">fullmm</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">mmu_gather_batch</span> <span class="o">*</span><span class="n">active</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">mmu_gather_batch</span>	<span class="n">local</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">page</span>		<span class="o">*</span><span class="n">__pages</span><span class="p">[</span><span class="n">MMU_GATHER_BUNDLE</span><span class="p">];</span>
<span class="p">};</span>

<span class="cp">#define HAVE_GENERIC_MMU_GATHER</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">tlb_fast_mode</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="k">return</span> <span class="n">tlb</span><span class="o">-&gt;</span><span class="n">fast_mode</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="cm">/*</span>
<span class="cm">	 * For UP we don&#39;t need to worry about TLB flush</span>
<span class="cm">	 * and page free order so much..</span>
<span class="cm">	 */</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">tlb_gather_mmu</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">,</span> <span class="k">struct</span> <span class="n">mm_struct</span> <span class="o">*</span><span class="n">mm</span><span class="p">,</span> <span class="n">bool</span> <span class="n">fullmm</span><span class="p">);</span>
<span class="kt">void</span> <span class="n">tlb_flush_mmu</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">);</span>
<span class="kt">void</span> <span class="n">tlb_finish_mmu</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">start</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">end</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">__tlb_remove_page</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">,</span> <span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">page</span><span class="p">);</span>

<span class="cm">/* tlb_remove_page</span>
<span class="cm"> *	Similar to __tlb_remove_page but will call tlb_flush_mmu() itself when</span>
<span class="cm"> *	required.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">tlb_remove_page</span><span class="p">(</span><span class="k">struct</span> <span class="n">mmu_gather</span> <span class="o">*</span><span class="n">tlb</span><span class="p">,</span> <span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">page</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">__tlb_remove_page</span><span class="p">(</span><span class="n">tlb</span><span class="p">,</span> <span class="n">page</span><span class="p">))</span>
		<span class="n">tlb_flush_mmu</span><span class="p">(</span><span class="n">tlb</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * tlb_remove_tlb_entry - remember a pte unmapping for later tlb invalidation.</span>
<span class="cm"> *</span>
<span class="cm"> * Record the fact that pte&#39;s were really umapped in -&gt;need_flush, so we can</span>
<span class="cm"> * later optimise away the tlb invalidate.   This helps when userspace is</span>
<span class="cm"> * unmapping already-unmapped pages, which happens quite a lot.</span>
<span class="cm"> */</span>
<span class="cp">#define tlb_remove_tlb_entry(tlb, ptep, address)		\</span>
<span class="cp">	do {							\</span>
<span class="cp">		tlb-&gt;need_flush = 1;				\</span>
<span class="cp">		__tlb_remove_tlb_entry(tlb, ptep, address);	\</span>
<span class="cp">	} while (0)</span>

<span class="cm">/**</span>
<span class="cm"> * tlb_remove_pmd_tlb_entry - remember a pmd mapping for later tlb invalidation</span>
<span class="cm"> * This is a nop so far, because only x86 needs it.</span>
<span class="cm"> */</span>
<span class="cp">#ifndef __tlb_remove_pmd_tlb_entry</span>
<span class="cp">#define __tlb_remove_pmd_tlb_entry(tlb, pmdp, address) do {} while (0)</span>
<span class="cp">#endif</span>

<span class="cp">#define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)		\</span>
<span class="cp">	do {							\</span>
<span class="cp">		tlb-&gt;need_flush = 1;				\</span>
<span class="cp">		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);	\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#define pte_free_tlb(tlb, ptep, address)			\</span>
<span class="cp">	do {							\</span>
<span class="cp">		tlb-&gt;need_flush = 1;				\</span>
<span class="cp">		__pte_free_tlb(tlb, ptep, address);		\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#ifndef __ARCH_HAS_4LEVEL_HACK</span>
<span class="cp">#define pud_free_tlb(tlb, pudp, address)			\</span>
<span class="cp">	do {							\</span>
<span class="cp">		tlb-&gt;need_flush = 1;				\</span>
<span class="cp">		__pud_free_tlb(tlb, pudp, address);		\</span>
<span class="cp">	} while (0)</span>
<span class="cp">#endif</span>

<span class="cp">#define pmd_free_tlb(tlb, pmdp, address)			\</span>
<span class="cp">	do {							\</span>
<span class="cp">		tlb-&gt;need_flush = 1;				\</span>
<span class="cp">		__pmd_free_tlb(tlb, pmdp, address);		\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#define tlb_migrate_finish(mm) do {} while (0)</span>

<span class="cp">#endif </span><span class="cm">/* _ASM_GENERIC__TLB_H */</span><span class="cp"></span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:2}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../../javascript/docco.min.js"></script>
</html>
