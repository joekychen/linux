f | ext2-atomic.h | s | 561B | 22 | Akinobu Mita | akinobu.mita@gmail.com | 1311724186 |  | asm-generic: add another generic ext2 atomic bitops  The majority of architectures implement ext2 atomic bitops as test_and_{set,clear}_bit() without spinlock.  This adds this type of generic implementation in ext2-atomic-setbit.h and use it wherever possible.  Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com> Suggested-by: Andreas Dilger <adilger@dilger.ca> Suggested-by: Arnd Bergmann <arnd@arndb.de> Acked-by: Arnd Bergmann <arnd@arndb.de> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | fls64.h | s | 821B | 33 | Andi Kleen | andi@firstfloor.org | 1231869390 |  | x86, generic: mark complex bitops.h inlines as __always_inline  Impact: reduce kernel image size  Hugh Dickins noticed that older gcc versions when the kernel is built for code size didn't inline some of the bitops.  Mark all complex x86 bitops that have more than a single asm statement or two as always inline to avoid this problem.  Probably should be done for other architectures too.  Ingo then found a better fix that only requires a single line change, but it unfortunately only works on gcc 4.3.  On older gccs the original patch still makes a ~0.3% defconfig difference with CONFIG_OPTIMIZE_INLINING=y.  With gcc 4.1 and a defconfig like build:      6116998 1138540  883788 8139326  7c323e vmlinux-oi-with-patch     6137043 1138540  883788 8159371  7c808b vmlinux-optimize-inlining  ~20k / 0.3% difference.  Signed-off-by: Andi Kleen <ak@linux.intel.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | non-atomic.h | s | 3.0K | 93 | Jiri Slaby | jirislaby@gmail.com | 1192820022 |  | define first set of BIT* macros  define first set of BIT* macros  - move BITOP_MASK and BITOP_WORD from asm-generic/bitops/atomic.h to   include/linux/bitops.h and rename it to BIT_MASK and BIT_WORD - move BITS_TO_LONGS and BITS_PER_BYTE to bitops.h too and allow easily   define another BITS_TO_something (e.g. in event.c) by BITS_TO_TYPE macro Remaining (and common) BIT macro will be defined after all occurences and conflicts will be sorted out in the patches.  Signed-off-by: Jiri Slaby <jirislaby@gmail.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | lock.h | s | 1.3K | 40 | Nick Piggin | npiggin@suse.de | 1192743449 |  | bitops: introduce lock ops  Introduce test_and_set_bit_lock / clear_bit_unlock bitops with lock semantics. Convert all architectures to use the generic implementation.  Signed-off-by: Nick Piggin <npiggin@suse.de> Acked-By: David Howells <dhowells@redhat.com> Cc: Richard Henderson <rth@twiddle.net> Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru> Cc: Russell King <rmk@arm.linux.org.uk> Cc: Haavard Skinnemoen <hskinnemoen@atmel.com> Cc: Bryan Wu <bryan.wu@analog.com> Cc: Mikael Starvik <starvik@axis.com> Cc: David Howells <dhowells@redhat.com> Cc: Yoshinori Sato <ysato@users.sourceforge.jp> Cc: "Luck, Tony" <tony.luck@intel.com> Cc: Hirokazu Takata <takata@linux-m32r.org> Cc: Geert Uytterhoeven <geert@linux-m68k.org> Cc: Roman Zippel <zippel@linux-m68k.org> Cc: Greg Ungerer <gerg@uclinux.org> Cc: Ralf Baechle <ralf@linux-mips.org> Cc: Kyle McMartin <kyle@mcmartin.ca> Cc: Matthew Wilcox <willy@debian.org> Cc: Paul Mackerras <paulus@samba.org> Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org> Cc: Heiko Carstens <heiko.carstens@de.ibm.com> Cc: Martin Schwidefsky <schwidefsky@de.ibm.com> Cc: Paul Mundt <lethal@linux-sh.org> Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp> Cc: Richard Curnow <rc@rc0.org.uk> Cc: William Lee Irwin III <wli@holomorphy.com> Cc: "David S. Miller" <davem@davemloft.net> Cc: Jeff Dike <jdike@addtoit.com> Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it> Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp> Cc: Andi Kleen <ak@muc.de> Cc: Chris Zankel <chris@zankel.net> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | find.h | s | 1.6K | 46 | Akinobu Mita | akinobu.mita@gmail.com | 1306455158 |  | bitops: add #ifndef for each of find bitops  The style that we normally use in asm-generic is to test the macro itself for existence, so in asm-generic, do:  	#ifndef find_next_zero_bit_le 	extern unsigned long find_next_zero_bit_le(const void *addr, 		unsigned long size, unsigned long offset); 	#endif  and in the architectures, write  	static inline unsigned long find_next_zero_bit_le(const void *addr, 		unsigned long size, unsigned long offset) 	#define find_next_zero_bit_le find_next_zero_bit_le  This adds the #ifndef for each of the find bitops in the generic header and source files.  Suggested-by: Arnd Bergmann <arnd@arndb.de> Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com> Acked-by: Russell King <rmk+kernel@arm.linux.org.uk> Cc: Martin Schwidefsky <schwidefsky@de.ibm.com> Cc: Heiko Carstens <heiko.carstens@de.ibm.com> Cc: Greg Ungerer <gerg@uclinux.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | arch_hweight.h | s | 516B | 20 | Borislav Petkov | borislav.petkov@amd.com | 1272993927 |  | arch, hweight: Fix compilation errors  Fix function prototype visibility issues when compiling for non-x86 architectures. Tested with crosstool (ftp://ftp.kernel.org/pub/tools/crosstool/) with alpha, ia64 and sparc targets.  Signed-off-by: Borislav Petkov <borislav.petkov@amd.com> LKML-Reference: <20100503130736.GD26107@aftab> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | fls.h | s | 635B | 37 | Andi Kleen | andi@firstfloor.org | 1231869390 |  | x86, generic: mark complex bitops.h inlines as __always_inline  Impact: reduce kernel image size  Hugh Dickins noticed that older gcc versions when the kernel is built for code size didn't inline some of the bitops.  Mark all complex x86 bitops that have more than a single asm statement or two as always inline to avoid this problem.  Probably should be done for other architectures too.  Ingo then found a better fix that only requires a single line change, but it unfortunately only works on gcc 4.3.  On older gccs the original patch still makes a ~0.3% defconfig difference with CONFIG_OPTIMIZE_INLINING=y.  With gcc 4.1 and a defconfig like build:      6116998 1138540  883788 8139326  7c323e vmlinux-oi-with-patch     6137043 1138540  883788 8159371  7c808b vmlinux-optimize-inlining  ~20k / 0.3% difference.  Signed-off-by: Andi Kleen <ak@linux.intel.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | ext2-atomic-setbit.h | s | 364B | 8 | Akinobu Mita | akinobu.mita@gmail.com | 1311724186 |  | asm-generic: add another generic ext2 atomic bitops  The majority of architectures implement ext2 atomic bitops as test_and_{set,clear}_bit() without spinlock.  This adds this type of generic implementation in ext2-atomic-setbit.h and use it wherever possible.  Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com> Suggested-by: Andreas Dilger <adilger@dilger.ca> Suggested-by: Arnd Bergmann <arnd@arndb.de> Acked-by: Arnd Bergmann <arnd@arndb.de> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | le.h | s | 2.0K | 67 | Akinobu Mita | akinobu.mita@gmail.com | 1306455158 |  | bitops: add #ifndef for each of find bitops  The style that we normally use in asm-generic is to test the macro itself for existence, so in asm-generic, do:  	#ifndef find_next_zero_bit_le 	extern unsigned long find_next_zero_bit_le(const void *addr, 		unsigned long size, unsigned long offset); 	#endif  and in the architectures, write  	static inline unsigned long find_next_zero_bit_le(const void *addr, 		unsigned long size, unsigned long offset) 	#define find_next_zero_bit_le find_next_zero_bit_le  This adds the #ifndef for each of the find bitops in the generic header and source files.  Suggested-by: Arnd Bergmann <arnd@arndb.de> Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com> Acked-by: Russell King <rmk+kernel@arm.linux.org.uk> Cc: Martin Schwidefsky <schwidefsky@de.ibm.com> Cc: Heiko Carstens <heiko.carstens@de.ibm.com> Cc: Greg Ungerer <gerg@uclinux.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | __fls.h | s | 881B | 39 | Andi Kleen | andi@firstfloor.org | 1231869390 |  | x86, generic: mark complex bitops.h inlines as __always_inline  Impact: reduce kernel image size  Hugh Dickins noticed that older gcc versions when the kernel is built for code size didn't inline some of the bitops.  Mark all complex x86 bitops that have more than a single asm statement or two as always inline to avoid this problem.  Probably should be done for other architectures too.  Ingo then found a better fix that only requires a single line change, but it unfortunately only works on gcc 4.3.  On older gccs the original patch still makes a ~0.3% defconfig difference with CONFIG_OPTIMIZE_INLINING=y.  With gcc 4.1 and a defconfig like build:      6116998 1138540  883788 8139326  7c323e vmlinux-oi-with-patch     6137043 1138540  883788 8159371  7c808b vmlinux-optimize-inlining  ~20k / 0.3% difference.  Signed-off-by: Andi Kleen <ak@linux.intel.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | const_hweight.h | s | 1.6K | 36 | Peter Zijlstra | a.p.zijlstra@chello.nl | 1270594331 |  | bitops: Optimize hweight() by making use of compile-time evaluation  Rename the extisting runtime hweight() implementations to __arch_hweight(), rename the compile-time versions to __const_hweight() and then have hweight() pick between them.  Suggested-by: H. Peter Anvin <hpa@zytor.com> Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl> LKML-Reference: <20100318111929.GB11152@aftab> Acked-by: H. Peter Anvin <hpa@zytor.com> LKML-Reference: <1265028224.24455.154.camel@laptop> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | ffz.h | s | 286B | 10 | Akinobu Mita | mita@miraclelinux.com | 1143392230 |  | [PATCH] bitops: generic ffz()  This patch introduces the C-language equivalent of the function: unsigned long ffz(unsigned long word);  In include/asm-generic/bitops/ffz.h  This code largely copied from: include/asm-parisc/bitops.h  Signed-off-by: Akinobu Mita <mita@miraclelinux.com> Signed-off-by: Andrew Morton <akpm@osdl.org> Signed-off-by: Linus Torvalds <torvalds@osdl.org>
f | atomic.h | s | 5.5K | 164 | David Howells | dhowells@redhat.com | 1332955803 |  | Add #includes needed to permit the removal of asm/system.h  asm/system.h is a cause of circular dependency problems because it contains commonly used primitive stuff like barrier definitions and uncommonly used stuff like switch_to() that might require MMU definitions.  asm/system.h has been disintegrated by this point on all arches into the following common segments:   (1) asm/barrier.h       Moved memory barrier definitions here.   (2) asm/cmpxchg.h       Moved xchg() and cmpxchg() here.  #included in asm/atomic.h.   (3) asm/bug.h       Moved die() and similar here.   (4) asm/exec.h       Moved arch_align_stack() here.   (5) asm/elf.h       Moved AT_VECTOR_SIZE_ARCH here.   (6) asm/switch_to.h       Moved switch_to() here.  Signed-off-by: David Howells <dhowells@redhat.com>
f | __ffs.h | s | 738B | 39 | Andi Kleen | andi@firstfloor.org | 1231869390 |  | x86, generic: mark complex bitops.h inlines as __always_inline  Impact: reduce kernel image size  Hugh Dickins noticed that older gcc versions when the kernel is built for code size didn't inline some of the bitops.  Mark all complex x86 bitops that have more than a single asm statement or two as always inline to avoid this problem.  Probably should be done for other architectures too.  Ingo then found a better fix that only requires a single line change, but it unfortunately only works on gcc 4.3.  On older gccs the original patch still makes a ~0.3% defconfig difference with CONFIG_OPTIMIZE_INLINING=y.  With gcc 4.1 and a defconfig like build:      6116998 1138540  883788 8139326  7c323e vmlinux-oi-with-patch     6137043 1138540  883788 8159371  7c808b vmlinux-optimize-inlining  ~20k / 0.3% difference.  Signed-off-by: Andi Kleen <ak@linux.intel.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | hweight.h | s | 215B | 5 | Peter Zijlstra | a.p.zijlstra@chello.nl | 1270594331 |  | bitops: Optimize hweight() by making use of compile-time evaluation  Rename the extisting runtime hweight() implementations to __arch_hweight(), rename the compile-time versions to __const_hweight() and then have hweight() pick between them.  Suggested-by: H. Peter Anvin <hpa@zytor.com> Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl> LKML-Reference: <20100318111929.GB11152@aftab> Acked-by: H. Peter Anvin <hpa@zytor.com> LKML-Reference: <1265028224.24455.154.camel@laptop> Signed-off-by: H. Peter Anvin <hpa@zytor.com>
f | ffs.h | s | 615B | 38 | Akinobu Mita | mita@miraclelinux.com | 1143392231 |  | [PATCH] bitops: generic ffs()  This patch introduces the C-language equivalent of the function: int ffs(int x);  In include/asm-generic/bitops/ffs.h  This code largely copied from: include/linux/bitops.h  Signed-off-by: Akinobu Mita <mita@miraclelinux.com> Signed-off-by: Andrew Morton <akpm@osdl.org> Signed-off-by: Linus Torvalds <torvalds@osdl.org>
f | sched.h | s | 721B | 28 | Mike Galbraith | efault@gmx.de | 1183999920 |  | sched: simplify sched_find_first_bit()  simplify sched_rt.c's sched_find_first_bit() function: there are only 100 RT priority levels left.  Signed-off-by: Ingo Molnar <mingo@elte.hu>
