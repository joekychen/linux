<!DOCTYPE html>
<html><head><title>joekychen/linux » include › linux › lru_cache.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../../index.html"></a><h1>lru_cache.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cm">/*</span>
<span class="cm">   lru_cache.c</span>

<span class="cm">   This file is part of DRBD by Philipp Reisner and Lars Ellenberg.</span>

<span class="cm">   Copyright (C) 2003-2008, LINBIT Information Technologies GmbH.</span>
<span class="cm">   Copyright (C) 2003-2008, Philipp Reisner &lt;philipp.reisner@linbit.com&gt;.</span>
<span class="cm">   Copyright (C) 2003-2008, Lars Ellenberg &lt;lars.ellenberg@linbit.com&gt;.</span>

<span class="cm">   drbd is free software; you can redistribute it and/or modify</span>
<span class="cm">   it under the terms of the GNU General Public License as published by</span>
<span class="cm">   the Free Software Foundation; either version 2, or (at your option)</span>
<span class="cm">   any later version.</span>

<span class="cm">   drbd is distributed in the hope that it will be useful,</span>
<span class="cm">   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="cm">   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="cm">   GNU General Public License for more details.</span>

<span class="cm">   You should have received a copy of the GNU General Public License</span>
<span class="cm">   along with drbd; see the file COPYING.  If not, write to</span>
<span class="cm">   the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.</span>

<span class="cm"> */</span>

<span class="cp">#ifndef LRU_CACHE_H</span>
<span class="cp">#define LRU_CACHE_H</span>

<span class="cp">#include &lt;linux/list.h&gt;</span>
<span class="cp">#include &lt;linux/slab.h&gt;</span>
<span class="cp">#include &lt;linux/bitops.h&gt;</span>
<span class="cp">#include &lt;linux/string.h&gt; </span><span class="cm">/* for memset */</span><span class="cp"></span>
<span class="cp">#include &lt;linux/seq_file.h&gt;</span>

<span class="cm">/*</span>
<span class="cm">This header file (and its .c file; kernel-doc of functions see there)</span>
<span class="cm">  define a helper framework to easily keep track of index:label associations,</span>
<span class="cm">  and changes to an &quot;active set&quot; of objects, as well as pending transactions,</span>
<span class="cm">  to persistently record those changes.</span>

<span class="cm">  We use an LRU policy if it is necessary to &quot;cool down&quot; a region currently in</span>
<span class="cm">  the active set before we can &quot;heat&quot; a previously unused region.</span>

<span class="cm">  Because of this later property, it is called &quot;lru_cache&quot;.</span>
<span class="cm">  As it actually Tracks Objects in an Active SeT, we could also call it</span>
<span class="cm">  toast (incidentally that is what may happen to the data on the</span>
<span class="cm">  backend storage uppon next resync, if we don&#39;t get it right).</span>

<span class="cm">What for?</span>

<span class="cm">We replicate IO (more or less synchronously) to local and remote disk.</span>

<span class="cm">For crash recovery after replication node failure,</span>
<span class="cm">  we need to resync all regions that have been target of in-flight WRITE IO</span>
<span class="cm">  (in use, or &quot;hot&quot;, regions), as we don&#39;t know wether or not those WRITEs have</span>
<span class="cm">  made it to stable storage.</span>

<span class="cm">  To avoid a &quot;full resync&quot;, we need to persistently track these regions.</span>

<span class="cm">  This is known as &quot;write intent log&quot;, and can be implemented as on-disk</span>
<span class="cm">  (coarse or fine grained) bitmap, or other meta data.</span>

<span class="cm">  To avoid the overhead of frequent extra writes to this meta data area,</span>
<span class="cm">  usually the condition is softened to regions that _may_ have been target of</span>
<span class="cm">  in-flight WRITE IO, e.g. by only lazily clearing the on-disk write-intent</span>
<span class="cm">  bitmap, trading frequency of meta data transactions against amount of</span>
<span class="cm">  (possibly unnecessary) resync traffic.</span>

<span class="cm">  If we set a hard limit on the area that may be &quot;hot&quot; at any given time, we</span>
<span class="cm">  limit the amount of resync traffic needed for crash recovery.</span>

<span class="cm">For recovery after replication link failure,</span>
<span class="cm">  we need to resync all blocks that have been changed on the other replica</span>
<span class="cm">  in the mean time, or, if both replica have been changed independently [*],</span>
<span class="cm">  all blocks that have been changed on either replica in the mean time.</span>
<span class="cm">  [*] usually as a result of a cluster split-brain and insufficient protection.</span>
<span class="cm">      but there are valid use cases to do this on purpose.</span>

<span class="cm">  Tracking those blocks can be implemented as &quot;dirty bitmap&quot;.</span>
<span class="cm">  Having it fine-grained reduces the amount of resync traffic.</span>
<span class="cm">  It should also be persistent, to allow for reboots (or crashes)</span>
<span class="cm">  while the replication link is down.</span>

<span class="cm">There are various possible implementations for persistently storing</span>
<span class="cm">write intent log information, three of which are mentioned here.</span>

<span class="cm">&quot;Chunk dirtying&quot;</span>
<span class="cm">  The on-disk &quot;dirty bitmap&quot; may be re-used as &quot;write-intent&quot; bitmap as well.</span>
<span class="cm">  To reduce the frequency of bitmap updates for write-intent log purposes,</span>
<span class="cm">  one could dirty &quot;chunks&quot; (of some size) at a time of the (fine grained)</span>
<span class="cm">  on-disk bitmap, while keeping the in-memory &quot;dirty&quot; bitmap as clean as</span>
<span class="cm">  possible, flushing it to disk again when a previously &quot;hot&quot; (and on-disk</span>
<span class="cm">  dirtied as full chunk) area &quot;cools down&quot; again (no IO in flight anymore,</span>
<span class="cm">  and none expected in the near future either).</span>

<span class="cm">&quot;Explicit (coarse) write intent bitmap&quot;</span>
<span class="cm">  An other implementation could chose a (probably coarse) explicit bitmap,</span>
<span class="cm">  for write-intent log purposes, additionally to the fine grained dirty bitmap.</span>

<span class="cm">&quot;Activity log&quot;</span>
<span class="cm">  Yet an other implementation may keep track of the hot regions, by starting</span>
<span class="cm">  with an empty set, and writing down a journal of region numbers that have</span>
<span class="cm">  become &quot;hot&quot;, or have &quot;cooled down&quot; again.</span>

<span class="cm">  To be able to use a ring buffer for this journal of changes to the active</span>
<span class="cm">  set, we not only record the actual changes to that set, but also record the</span>
<span class="cm">  not changing members of the set in a round robin fashion. To do so, we use a</span>
<span class="cm">  fixed (but configurable) number of slots which we can identify by index, and</span>
<span class="cm">  associate region numbers (labels) with these indices.</span>
<span class="cm">  For each transaction recording a change to the active set, we record the</span>
<span class="cm">  change itself (index: -old_label, +new_label), and which index is associated</span>
<span class="cm">  with which label (index: current_label) within a certain sliding window that</span>
<span class="cm">  is moved further over the available indices with each such transaction.</span>

<span class="cm">  Thus, for crash recovery, if the ringbuffer is sufficiently large, we can</span>
<span class="cm">  accurately reconstruct the active set.</span>

<span class="cm">  Sufficiently large depends only on maximum number of active objects, and the</span>
<span class="cm">  size of the sliding window recording &quot;index: current_label&quot; associations within</span>
<span class="cm">  each transaction.</span>

<span class="cm">  This is what we call the &quot;activity log&quot;.</span>

<span class="cm">  Currently we need one activity log transaction per single label change, which</span>
<span class="cm">  does not give much benefit over the &quot;dirty chunks of bitmap&quot; approach, other</span>
<span class="cm">  than potentially less seeks.</span>

<span class="cm">  We plan to change the transaction format to support multiple changes per</span>
<span class="cm">  transaction, which then would reduce several (disjoint, &quot;random&quot;) updates to</span>
<span class="cm">  the bitmap into one transaction to the activity log ring buffer.</span>
<span class="cm">*/</span>

<span class="cm">/* this defines an element in a tracked set</span>
<span class="cm"> * .colision is for hash table lookup.</span>
<span class="cm"> * When we process a new IO request, we know its sector, thus can deduce the</span>
<span class="cm"> * region number (label) easily.  To do the label -&gt; object lookup without a</span>
<span class="cm"> * full list walk, we use a simple hash table.</span>
<span class="cm"> *</span>
<span class="cm"> * .list is on one of three lists:</span>
<span class="cm"> *  in_use: currently in use (refcnt &gt; 0, lc_number != LC_FREE)</span>
<span class="cm"> *     lru: unused but ready to be reused or recycled</span>
<span class="cm"> *          (lc_refcnt == 0, lc_number != LC_FREE),</span>
<span class="cm"> *    free: unused but ready to be recycled</span>
<span class="cm"> *          (lc_refcnt == 0, lc_number == LC_FREE),</span>
<span class="cm"> *</span>
<span class="cm"> * an element is said to be &quot;in the active set&quot;,</span>
<span class="cm"> * if either on &quot;in_use&quot; or &quot;lru&quot;, i.e. lc_number != LC_FREE.</span>
<span class="cm"> *</span>
<span class="cm"> * DRBD currently (May 2009) only uses 61 elements on the resync lru_cache</span>
<span class="cm"> * (total memory usage 2 pages), and up to 3833 elements on the act_log</span>
<span class="cm"> * lru_cache, totalling ~215 kB for 64bit architecture, ~53 pages.</span>
<span class="cm"> *</span>
<span class="cm"> * We usually do not actually free these objects again, but only &quot;recycle&quot;</span>
<span class="cm"> * them, as the change &quot;index: -old_label, +LC_FREE&quot; would need a transaction</span>
<span class="cm"> * as well.  Which also means that using a kmem_cache to allocate the objects</span>
<span class="cm"> * from wastes some resources.</span>
<span class="cm"> * But it avoids high order page allocations in kmalloc.</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">lc_element</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">hlist_node</span> <span class="n">colision</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">list</span><span class="p">;</span>		 <span class="cm">/* LRU list or free list */</span>
	<span class="kt">unsigned</span> <span class="n">refcnt</span><span class="p">;</span>
	<span class="cm">/* back &quot;pointer&quot; into lc_cache-&gt;element[index],</span>
<span class="cm">	 * for paranoia, and for &quot;lc_element_to_index&quot; */</span>
	<span class="kt">unsigned</span> <span class="n">lc_index</span><span class="p">;</span>
	<span class="cm">/* if we want to track a larger set of objects,</span>
<span class="cm">	 * it needs to become arch independend u64 */</span>
	<span class="kt">unsigned</span> <span class="n">lc_number</span><span class="p">;</span>

	<span class="cm">/* special label when on free list */</span>
<span class="cp">#define LC_FREE (~0U)</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">lru_cache</span> <span class="p">{</span>
	<span class="cm">/* the least recently used item is kept at lru-&gt;prev */</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">lru</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">free</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">in_use</span><span class="p">;</span>

	<span class="cm">/* the pre-created kmem cache to allocate the objects from */</span>
	<span class="k">struct</span> <span class="n">kmem_cache</span> <span class="o">*</span><span class="n">lc_cache</span><span class="p">;</span>

	<span class="cm">/* size of tracked objects, used to memset(,0,) them in lc_reset */</span>
	<span class="kt">size_t</span> <span class="n">element_size</span><span class="p">;</span>
	<span class="cm">/* offset of struct lc_element member in the tracked object */</span>
	<span class="kt">size_t</span> <span class="n">element_off</span><span class="p">;</span>

	<span class="cm">/* number of elements (indices) */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>  <span class="n">nr_elements</span><span class="p">;</span>
	<span class="cm">/* Arbitrary limit on maximum tracked objects. Practical limit is much</span>
<span class="cm">	 * lower due to allocation failures, probably. For typical use cases,</span>
<span class="cm">	 * nr_elements should be a few thousand at most.</span>
<span class="cm">	 * This also limits the maximum value of lc_element.lc_index, allowing the</span>
<span class="cm">	 * 8 high bits of .lc_index to be overloaded with flags in the future. */</span>
<span class="cp">#define LC_MAX_ACTIVE	(1&lt;&lt;24)</span>

	<span class="cm">/* statistics */</span>
	<span class="kt">unsigned</span> <span class="n">used</span><span class="p">;</span> <span class="cm">/* number of lelements currently on in_use list */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">hits</span><span class="p">,</span> <span class="n">misses</span><span class="p">,</span> <span class="n">starving</span><span class="p">,</span> <span class="n">dirty</span><span class="p">,</span> <span class="n">changed</span><span class="p">;</span>

	<span class="cm">/* see below: flag-bits for lru_cache */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>

	<span class="cm">/* when changing the label of an index element */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>  <span class="n">new_number</span><span class="p">;</span>

	<span class="cm">/* for paranoia when changing the label of an index element */</span>
	<span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">changing_element</span><span class="p">;</span>

	<span class="kt">void</span>  <span class="o">*</span><span class="n">lc_private</span><span class="p">;</span>
	<span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">name</span><span class="p">;</span>

	<span class="cm">/* nr_elements there */</span>
	<span class="k">struct</span> <span class="n">hlist_head</span> <span class="o">*</span><span class="n">lc_slot</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">lc_element</span> <span class="o">**</span><span class="n">lc_element</span><span class="p">;</span>
<span class="p">};</span>


<span class="cm">/* flag-bits for lru_cache */</span>
<span class="k">enum</span> <span class="p">{</span>
	<span class="cm">/* debugging aid, to catch concurrent access early.</span>
<span class="cm">	 * user needs to guarantee exclusive access by proper locking! */</span>
	<span class="n">__LC_PARANOIA</span><span class="p">,</span>
	<span class="cm">/* if we need to change the set, but currently there is a changing</span>
<span class="cm">	 * transaction pending, we are &quot;dirty&quot;, and must deferr further</span>
<span class="cm">	 * changing requests */</span>
	<span class="n">__LC_DIRTY</span><span class="p">,</span>
	<span class="cm">/* if we need to change the set, but currently there is no free nor</span>
<span class="cm">	 * unused element available, we are &quot;starving&quot;, and must not give out</span>
<span class="cm">	 * further references, to guarantee that eventually some refcnt will</span>
<span class="cm">	 * drop to zero and we will be able to make progress again, changing</span>
<span class="cm">	 * the set, writing the transaction.</span>
<span class="cm">	 * if the statistics say we are frequently starving,</span>
<span class="cm">	 * nr_elements is too small. */</span>
	<span class="n">__LC_STARVING</span><span class="p">,</span>
<span class="p">};</span>
<span class="cp">#define LC_PARANOIA (1&lt;&lt;__LC_PARANOIA)</span>
<span class="cp">#define LC_DIRTY    (1&lt;&lt;__LC_DIRTY)</span>
<span class="cp">#define LC_STARVING (1&lt;&lt;__LC_STARVING)</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc_create</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">name</span><span class="p">,</span> <span class="k">struct</span> <span class="n">kmem_cache</span> <span class="o">*</span><span class="n">cache</span><span class="p">,</span>
		<span class="kt">unsigned</span> <span class="n">e_count</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">e_size</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">e_off</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">lc_reset</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">lc_destroy</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">lc_set</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">enr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">index</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">lc_del</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">element</span><span class="p">);</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">lc_try_get</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">enr</span><span class="p">);</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">lc_find</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">enr</span><span class="p">);</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">lc_get</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">enr</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">lc_put</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">e</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">lc_changed</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">e</span><span class="p">);</span>

<span class="k">struct</span> <span class="n">seq_file</span><span class="p">;</span>
<span class="k">extern</span> <span class="kt">size_t</span> <span class="n">lc_seq_printf_stats</span><span class="p">(</span><span class="k">struct</span> <span class="n">seq_file</span> <span class="o">*</span><span class="n">seq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">lc_seq_dump_details</span><span class="p">(</span><span class="k">struct</span> <span class="n">seq_file</span> <span class="o">*</span><span class="n">seq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">utext</span><span class="p">,</span>
				<span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">detail</span><span class="p">)</span> <span class="p">(</span><span class="k">struct</span> <span class="n">seq_file</span> <span class="o">*</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="p">));</span>

<span class="cm">/**</span>
<span class="cm"> * lc_try_lock - can be used to stop lc_get() from changing the tracked set</span>
<span class="cm"> * @lc: the lru cache to operate on</span>
<span class="cm"> *</span>
<span class="cm"> * Note that the reference counts and order on the active and lru lists may</span>
<span class="cm"> * still change.  Returns true if we acquired the lock.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">lc_try_lock</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="o">!</span><span class="n">test_and_set_bit</span><span class="p">(</span><span class="n">__LC_DIRTY</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">lc</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * lc_unlock - unlock @lc, allow lc_get() to change the set again</span>
<span class="cm"> * @lc: the lru cache to operate on</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">lc_unlock</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">clear_bit</span><span class="p">(</span><span class="n">__LC_DIRTY</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">lc</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">smp_mb__after_clear_bit</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">lc_is_used</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">enr</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">e</span> <span class="o">=</span> <span class="n">lc_find</span><span class="p">(</span><span class="n">lc</span><span class="p">,</span> <span class="n">enr</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">e</span> <span class="o">&amp;&amp;</span> <span class="n">e</span><span class="o">-&gt;</span><span class="n">refcnt</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#define lc_entry(ptr, type, member) \</span>
<span class="cp">	container_of(ptr, type, member)</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">lc_element_by_index</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">i</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">lc_index_of</span><span class="p">(</span><span class="k">struct</span> <span class="n">lru_cache</span> <span class="o">*</span><span class="n">lc</span><span class="p">,</span> <span class="k">struct</span> <span class="n">lc_element</span> <span class="o">*</span><span class="n">e</span><span class="p">);</span>

<span class="cp">#endif</span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:2}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../../javascript/docco.min.js"></script>
</html>
