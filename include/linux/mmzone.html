<!DOCTYPE html>
<html><head><title>joekychen/linux » include › linux › mmzone.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../../index.html"></a><h1>mmzone.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cp">#ifndef _LINUX_MMZONE_H</span>
<span class="cp">#define _LINUX_MMZONE_H</span>

<span class="cp">#ifndef __ASSEMBLY__</span>
<span class="cp">#ifndef __GENERATING_BOUNDS_H</span>

<span class="cp">#include &lt;linux/spinlock.h&gt;</span>
<span class="cp">#include &lt;linux/list.h&gt;</span>
<span class="cp">#include &lt;linux/wait.h&gt;</span>
<span class="cp">#include &lt;linux/bitops.h&gt;</span>
<span class="cp">#include &lt;linux/cache.h&gt;</span>
<span class="cp">#include &lt;linux/threads.h&gt;</span>
<span class="cp">#include &lt;linux/numa.h&gt;</span>
<span class="cp">#include &lt;linux/init.h&gt;</span>
<span class="cp">#include &lt;linux/seqlock.h&gt;</span>
<span class="cp">#include &lt;linux/nodemask.h&gt;</span>
<span class="cp">#include &lt;linux/pageblock-flags.h&gt;</span>
<span class="cp">#include &lt;generated/bounds.h&gt;</span>
<span class="cp">#include &lt;linux/atomic.h&gt;</span>
<span class="cp">#include &lt;asm/page.h&gt;</span>

<span class="cm">/* Free memory management - zoned buddy allocator.  */</span>
<span class="cp">#ifndef CONFIG_FORCE_MAX_ZONEORDER</span>
<span class="cp">#define MAX_ORDER 11</span>
<span class="cp">#else</span>
<span class="cp">#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER</span>
<span class="cp">#endif</span>
<span class="cp">#define MAX_ORDER_NR_PAGES (1 &lt;&lt; (MAX_ORDER - 1))</span>

<span class="cm">/*</span>
<span class="cm"> * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed</span>
<span class="cm"> * costly to service.  That is between allocation orders which should</span>
<span class="cm"> * coalesce naturally under reasonable reclaim pressure and those which</span>
<span class="cm"> * will not.</span>
<span class="cm"> */</span>
<span class="cp">#define PAGE_ALLOC_COSTLY_ORDER 3</span>

<span class="k">enum</span> <span class="p">{</span>
	<span class="n">MIGRATE_UNMOVABLE</span><span class="p">,</span>
	<span class="n">MIGRATE_RECLAIMABLE</span><span class="p">,</span>
	<span class="n">MIGRATE_MOVABLE</span><span class="p">,</span>
	<span class="n">MIGRATE_PCPTYPES</span><span class="p">,</span>	<span class="cm">/* the number of types on the pcp lists */</span>
	<span class="n">MIGRATE_RESERVE</span> <span class="o">=</span> <span class="n">MIGRATE_PCPTYPES</span><span class="p">,</span>
<span class="cp">#ifdef CONFIG_CMA</span>
	<span class="cm">/*</span>
<span class="cm">	 * MIGRATE_CMA migration type is designed to mimic the way</span>
<span class="cm">	 * ZONE_MOVABLE works.  Only movable pages can be allocated</span>
<span class="cm">	 * from MIGRATE_CMA pageblocks and page allocator never</span>
<span class="cm">	 * implicitly change migration type of MIGRATE_CMA pageblock.</span>
<span class="cm">	 *</span>
<span class="cm">	 * The way to use it is to change migratetype of a range of</span>
<span class="cm">	 * pageblocks to MIGRATE_CMA which can be done by</span>
<span class="cm">	 * __free_pageblock_cma() function.  What is important though</span>
<span class="cm">	 * is that a range of pageblocks must be aligned to</span>
<span class="cm">	 * MAX_ORDER_NR_PAGES should biggest page be bigger then</span>
<span class="cm">	 * a single pageblock.</span>
<span class="cm">	 */</span>
	<span class="n">MIGRATE_CMA</span><span class="p">,</span>
<span class="cp">#endif</span>
	<span class="n">MIGRATE_ISOLATE</span><span class="p">,</span>	<span class="cm">/* can&#39;t allocate from here */</span>
	<span class="n">MIGRATE_TYPES</span>
<span class="p">};</span>

<span class="cp">#ifdef CONFIG_CMA</span>
<span class="cp">#  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)</span>
<span class="cp">#  define cma_wmark_pages(zone)	zone-&gt;min_cma_pages</span>
<span class="cp">#else</span>
<span class="cp">#  define is_migrate_cma(migratetype) false</span>
<span class="cp">#  define cma_wmark_pages(zone) 0</span>
<span class="cp">#endif</span>

<span class="cp">#define for_each_migratetype_order(order, type) \</span>
<span class="cp">	for (order = 0; order &lt; MAX_ORDER; order++) \</span>
<span class="cp">		for (type = 0; type &lt; MIGRATE_TYPES; type++)</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">page_group_by_mobility_disabled</span><span class="p">;</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">get_pageblock_migratetype</span><span class="p">(</span><span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">page</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">get_pageblock_flags_group</span><span class="p">(</span><span class="n">page</span><span class="p">,</span> <span class="n">PB_migrate</span><span class="p">,</span> <span class="n">PB_migrate_end</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">struct</span> <span class="n">free_area</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">list_head</span>	<span class="n">free_list</span><span class="p">[</span><span class="n">MIGRATE_TYPES</span><span class="p">];</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">nr_free</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">pglist_data</span><span class="p">;</span>

<span class="cm">/*</span>
<span class="cm"> * zone-&gt;lock and zone-&gt;lru_lock are two of the hottest locks in the kernel.</span>
<span class="cm"> * So add a wild amount of padding here to ensure that they fall into separate</span>
<span class="cm"> * cachelines.  There are very few zone structures in the machine, so space</span>
<span class="cm"> * consumption is not a concern here.</span>
<span class="cm"> */</span>
<span class="cp">#if defined(CONFIG_SMP)</span>
<span class="k">struct</span> <span class="n">zone_padding</span> <span class="p">{</span>
	<span class="kt">char</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span> <span class="n">____cacheline_internodealigned_in_smp</span><span class="p">;</span>
<span class="cp">#define ZONE_PADDING(name)	struct zone_padding name;</span>
<span class="cp">#else</span>
<span class="cp">#define ZONE_PADDING(name)</span>
<span class="cp">#endif</span>

<span class="k">enum</span> <span class="n">zone_stat_item</span> <span class="p">{</span>
	<span class="cm">/* First 128 byte cacheline (assuming 64 bit words) */</span>
	<span class="n">NR_FREE_PAGES</span><span class="p">,</span>
	<span class="n">NR_LRU_BASE</span><span class="p">,</span>
	<span class="n">NR_INACTIVE_ANON</span> <span class="o">=</span> <span class="n">NR_LRU_BASE</span><span class="p">,</span> <span class="cm">/* must match order of LRU_[IN]ACTIVE */</span>
	<span class="n">NR_ACTIVE_ANON</span><span class="p">,</span>		<span class="cm">/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
	<span class="n">NR_INACTIVE_FILE</span><span class="p">,</span>	<span class="cm">/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
	<span class="n">NR_ACTIVE_FILE</span><span class="p">,</span>		<span class="cm">/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
	<span class="n">NR_UNEVICTABLE</span><span class="p">,</span>		<span class="cm">/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
	<span class="n">NR_MLOCK</span><span class="p">,</span>		<span class="cm">/* mlock()ed pages found and moved off LRU */</span>
	<span class="n">NR_ANON_PAGES</span><span class="p">,</span>	<span class="cm">/* Mapped anonymous pages */</span>
	<span class="n">NR_FILE_MAPPED</span><span class="p">,</span>	<span class="cm">/* pagecache pages mapped into pagetables.</span>
<span class="cm">			   only modified from process context */</span>
	<span class="n">NR_FILE_PAGES</span><span class="p">,</span>
	<span class="n">NR_FILE_DIRTY</span><span class="p">,</span>
	<span class="n">NR_WRITEBACK</span><span class="p">,</span>
	<span class="n">NR_SLAB_RECLAIMABLE</span><span class="p">,</span>
	<span class="n">NR_SLAB_UNRECLAIMABLE</span><span class="p">,</span>
	<span class="n">NR_PAGETABLE</span><span class="p">,</span>		<span class="cm">/* used for pagetables */</span>
	<span class="n">NR_KERNEL_STACK</span><span class="p">,</span>
	<span class="cm">/* Second 128 byte cacheline */</span>
	<span class="n">NR_UNSTABLE_NFS</span><span class="p">,</span>	<span class="cm">/* NFS unstable pages */</span>
	<span class="n">NR_BOUNCE</span><span class="p">,</span>
	<span class="n">NR_VMSCAN_WRITE</span><span class="p">,</span>
	<span class="n">NR_VMSCAN_IMMEDIATE</span><span class="p">,</span>	<span class="cm">/* Prioritise for reclaim when writeback ends */</span>
	<span class="n">NR_WRITEBACK_TEMP</span><span class="p">,</span>	<span class="cm">/* Writeback using temporary buffers */</span>
	<span class="n">NR_ISOLATED_ANON</span><span class="p">,</span>	<span class="cm">/* Temporary isolated pages from anon lru */</span>
	<span class="n">NR_ISOLATED_FILE</span><span class="p">,</span>	<span class="cm">/* Temporary isolated pages from file lru */</span>
	<span class="n">NR_SHMEM</span><span class="p">,</span>		<span class="cm">/* shmem pages (included tmpfs/GEM pages) */</span>
	<span class="n">NR_DIRTIED</span><span class="p">,</span>		<span class="cm">/* page dirtyings since bootup */</span>
	<span class="n">NR_WRITTEN</span><span class="p">,</span>		<span class="cm">/* page writings since bootup */</span>
<span class="cp">#ifdef CONFIG_NUMA</span>
	<span class="n">NUMA_HIT</span><span class="p">,</span>		<span class="cm">/* allocated in intended node */</span>
	<span class="n">NUMA_MISS</span><span class="p">,</span>		<span class="cm">/* allocated in non intended node */</span>
	<span class="n">NUMA_FOREIGN</span><span class="p">,</span>		<span class="cm">/* was intended here, hit elsewhere */</span>
	<span class="n">NUMA_INTERLEAVE_HIT</span><span class="p">,</span>	<span class="cm">/* interleaver preferred this zone */</span>
	<span class="n">NUMA_LOCAL</span><span class="p">,</span>		<span class="cm">/* allocation from local node */</span>
	<span class="n">NUMA_OTHER</span><span class="p">,</span>		<span class="cm">/* allocation from other node */</span>
<span class="cp">#endif</span>
	<span class="n">NR_ANON_TRANSPARENT_HUGEPAGES</span><span class="p">,</span>
	<span class="n">NR_VM_ZONE_STAT_ITEMS</span> <span class="p">};</span>

<span class="cm">/*</span>
<span class="cm"> * We do arithmetic on the LRU lists in various places in the code,</span>
<span class="cm"> * so it is important to keep the active lists LRU_ACTIVE higher in</span>
<span class="cm"> * the array than the corresponding inactive lists, and to keep</span>
<span class="cm"> * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.</span>
<span class="cm"> *</span>
<span class="cm"> * This has to be kept in sync with the statistics in zone_stat_item</span>
<span class="cm"> * above and the descriptions in vmstat_text in mm/vmstat.c</span>
<span class="cm"> */</span>
<span class="cp">#define LRU_BASE 0</span>
<span class="cp">#define LRU_ACTIVE 1</span>
<span class="cp">#define LRU_FILE 2</span>

<span class="k">enum</span> <span class="n">lru_list</span> <span class="p">{</span>
	<span class="n">LRU_INACTIVE_ANON</span> <span class="o">=</span> <span class="n">LRU_BASE</span><span class="p">,</span>
	<span class="n">LRU_ACTIVE_ANON</span> <span class="o">=</span> <span class="n">LRU_BASE</span> <span class="o">+</span> <span class="n">LRU_ACTIVE</span><span class="p">,</span>
	<span class="n">LRU_INACTIVE_FILE</span> <span class="o">=</span> <span class="n">LRU_BASE</span> <span class="o">+</span> <span class="n">LRU_FILE</span><span class="p">,</span>
	<span class="n">LRU_ACTIVE_FILE</span> <span class="o">=</span> <span class="n">LRU_BASE</span> <span class="o">+</span> <span class="n">LRU_FILE</span> <span class="o">+</span> <span class="n">LRU_ACTIVE</span><span class="p">,</span>
	<span class="n">LRU_UNEVICTABLE</span><span class="p">,</span>
	<span class="n">NR_LRU_LISTS</span>
<span class="p">};</span>

<span class="cp">#define for_each_lru(lru) for (lru = 0; lru &lt; NR_LRU_LISTS; lru++)</span>

<span class="cp">#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_file_lru</span><span class="p">(</span><span class="k">enum</span> <span class="n">lru_list</span> <span class="n">lru</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">lru</span> <span class="o">==</span> <span class="n">LRU_INACTIVE_FILE</span> <span class="o">||</span> <span class="n">lru</span> <span class="o">==</span> <span class="n">LRU_ACTIVE_FILE</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_active_lru</span><span class="p">(</span><span class="k">enum</span> <span class="n">lru_list</span> <span class="n">lru</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">lru</span> <span class="o">==</span> <span class="n">LRU_ACTIVE_ANON</span> <span class="o">||</span> <span class="n">lru</span> <span class="o">==</span> <span class="n">LRU_ACTIVE_FILE</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_unevictable_lru</span><span class="p">(</span><span class="k">enum</span> <span class="n">lru_list</span> <span class="n">lru</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">lru</span> <span class="o">==</span> <span class="n">LRU_UNEVICTABLE</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">struct</span> <span class="n">zone_reclaim_stat</span> <span class="p">{</span>
	<span class="cm">/*</span>
<span class="cm">	 * The pageout code in vmscan.c keeps track of how many of the</span>
<span class="cm">	 * mem/swap backed and file backed pages are refeferenced.</span>
<span class="cm">	 * The higher the rotated/scanned ratio, the more valuable</span>
<span class="cm">	 * that cache is.</span>
<span class="cm">	 *</span>
<span class="cm">	 * The anon LRU stats live in [0], file LRU stats in [1]</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">recent_rotated</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">recent_scanned</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">lruvec</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">lists</span><span class="p">[</span><span class="n">NR_LRU_LISTS</span><span class="p">];</span>
	<span class="k">struct</span> <span class="n">zone_reclaim_stat</span> <span class="n">reclaim_stat</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_CGROUP_MEM_RES_CTLR</span>
	<span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="cm">/* Mask used at gathering information at once (see memcontrol.c) */</span>
<span class="cp">#define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))</span>
<span class="cp">#define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))</span>
<span class="cp">#define LRU_ALL_EVICTABLE (LRU_ALL_FILE | LRU_ALL_ANON)</span>
<span class="cp">#define LRU_ALL	     ((1 &lt;&lt; NR_LRU_LISTS) - 1)</span>

<span class="cm">/* Isolate clean file */</span>
<span class="cp">#define ISOLATE_CLEAN		((__force isolate_mode_t)0x1)</span>
<span class="cm">/* Isolate unmapped file */</span>
<span class="cp">#define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x2)</span>
<span class="cm">/* Isolate for asynchronous migration */</span>
<span class="cp">#define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x4)</span>

<span class="cm">/* LRU Isolation modes. */</span>
<span class="k">typedef</span> <span class="kt">unsigned</span> <span class="n">__bitwise__</span> <span class="n">isolate_mode_t</span><span class="p">;</span>

<span class="k">enum</span> <span class="n">zone_watermarks</span> <span class="p">{</span>
	<span class="n">WMARK_MIN</span><span class="p">,</span>
	<span class="n">WMARK_LOW</span><span class="p">,</span>
	<span class="n">WMARK_HIGH</span><span class="p">,</span>
	<span class="n">NR_WMARK</span>
<span class="p">};</span>

<span class="cp">#define min_wmark_pages(z) (z-&gt;watermark[WMARK_MIN])</span>
<span class="cp">#define low_wmark_pages(z) (z-&gt;watermark[WMARK_LOW])</span>
<span class="cp">#define high_wmark_pages(z) (z-&gt;watermark[WMARK_HIGH])</span>

<span class="k">struct</span> <span class="n">per_cpu_pages</span> <span class="p">{</span>
	<span class="kt">int</span> <span class="n">count</span><span class="p">;</span>		<span class="cm">/* number of pages in the list */</span>
	<span class="kt">int</span> <span class="n">high</span><span class="p">;</span>		<span class="cm">/* high watermark, emptying needed */</span>
	<span class="kt">int</span> <span class="n">batch</span><span class="p">;</span>		<span class="cm">/* chunk size for buddy add/remove */</span>

	<span class="cm">/* Lists of pages, one per migrate type stored on the pcp-lists */</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">lists</span><span class="p">[</span><span class="n">MIGRATE_PCPTYPES</span><span class="p">];</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">per_cpu_pageset</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">per_cpu_pages</span> <span class="n">pcp</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_NUMA</span>
	<span class="n">s8</span> <span class="n">expire</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="n">s8</span> <span class="n">stat_threshold</span><span class="p">;</span>
	<span class="n">s8</span> <span class="n">vm_stat_diff</span><span class="p">[</span><span class="n">NR_VM_ZONE_STAT_ITEMS</span><span class="p">];</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="cp">#endif </span><span class="cm">/* !__GENERATING_BOUNDS.H */</span><span class="cp"></span>

<span class="k">enum</span> <span class="n">zone_type</span> <span class="p">{</span>
<span class="cp">#ifdef CONFIG_ZONE_DMA</span>
	<span class="cm">/*</span>
<span class="cm">	 * ZONE_DMA is used when there are devices that are not able</span>
<span class="cm">	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we</span>
<span class="cm">	 * carve out the portion of memory that is needed for these devices.</span>
<span class="cm">	 * The range is arch specific.</span>
<span class="cm">	 *</span>
<span class="cm">	 * Some examples</span>
<span class="cm">	 *</span>
<span class="cm">	 * Architecture		Limit</span>
<span class="cm">	 * ---------------------------</span>
<span class="cm">	 * parisc, ia64, sparc	&lt;4G</span>
<span class="cm">	 * s390			&lt;2G</span>
<span class="cm">	 * arm			Various</span>
<span class="cm">	 * alpha		Unlimited or 0-16MB.</span>
<span class="cm">	 *</span>
<span class="cm">	 * i386, x86_64 and multiple other arches</span>
<span class="cm">	 * 			&lt;16M.</span>
<span class="cm">	 */</span>
	<span class="n">ZONE_DMA</span><span class="p">,</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_ZONE_DMA32</span>
	<span class="cm">/*</span>
<span class="cm">	 * x86_64 needs two ZONE_DMAs because it supports devices that are</span>
<span class="cm">	 * only able to do DMA to the lower 16M but also 32 bit devices that</span>
<span class="cm">	 * can only do DMA areas below 4G.</span>
<span class="cm">	 */</span>
	<span class="n">ZONE_DMA32</span><span class="p">,</span>
<span class="cp">#endif</span>
	<span class="cm">/*</span>
<span class="cm">	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be</span>
<span class="cm">	 * performed on pages in ZONE_NORMAL if the DMA devices support</span>
<span class="cm">	 * transfers to all addressable memory.</span>
<span class="cm">	 */</span>
	<span class="n">ZONE_NORMAL</span><span class="p">,</span>
<span class="cp">#ifdef CONFIG_HIGHMEM</span>
	<span class="cm">/*</span>
<span class="cm">	 * A memory area that is only addressable by the kernel through</span>
<span class="cm">	 * mapping portions into its own address space. This is for example</span>
<span class="cm">	 * used by i386 to allow the kernel to address the memory beyond</span>
<span class="cm">	 * 900MB. The kernel will set up special mappings (page</span>
<span class="cm">	 * table entries on i386) for each page that the kernel needs to</span>
<span class="cm">	 * access.</span>
<span class="cm">	 */</span>
	<span class="n">ZONE_HIGHMEM</span><span class="p">,</span>
<span class="cp">#endif</span>
	<span class="n">ZONE_MOVABLE</span><span class="p">,</span>
	<span class="n">__MAX_NR_ZONES</span>
<span class="p">};</span>

<span class="cp">#ifndef __GENERATING_BOUNDS_H</span>

<span class="cm">/*</span>
<span class="cm"> * When a memory allocation must conform to specific limitations (such</span>
<span class="cm"> * as being suitable for DMA) the caller will pass in hints to the</span>
<span class="cm"> * allocator in the gfp_mask, in the zone modifier bits.  These bits</span>
<span class="cm"> * are used to select a priority ordered list of memory zones which</span>
<span class="cm"> * match the requested limits. See gfp_zone() in include/linux/gfp.h</span>
<span class="cm"> */</span>

<span class="cp">#if MAX_NR_ZONES &lt; 2</span>
<span class="cp">#define ZONES_SHIFT 0</span>
<span class="cp">#elif MAX_NR_ZONES &lt;= 2</span>
<span class="cp">#define ZONES_SHIFT 1</span>
<span class="cp">#elif MAX_NR_ZONES &lt;= 4</span>
<span class="cp">#define ZONES_SHIFT 2</span>
<span class="cp">#else</span>
<span class="cp">#error ZONES_SHIFT -- too many zones configured adjust calculation</span>
<span class="cp">#endif</span>

<span class="k">struct</span> <span class="n">zone</span> <span class="p">{</span>
	<span class="cm">/* Fields commonly accessed by the page allocator */</span>

	<span class="cm">/* zone watermarks, access with *_wmark_pages(zone) macros */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">watermark</span><span class="p">[</span><span class="n">NR_WMARK</span><span class="p">];</span>

	<span class="cm">/*</span>
<span class="cm">	 * When free pages are below this point, additional steps are taken</span>
<span class="cm">	 * when reading the number of free pages to avoid per-cpu counter</span>
<span class="cm">	 * drift allowing watermarks to be breached</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">percpu_drift_mark</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * We don&#39;t know if the memory that we&#39;re going to allocate will be freeable</span>
<span class="cm">	 * or/and it will be released eventually, so to avoid totally wasting several</span>
<span class="cm">	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk</span>
<span class="cm">	 * to run OOM on the lower zones despite there&#39;s tons of freeable ram</span>
<span class="cm">	 * on the higher zones). This array is recalculated at runtime if the</span>
<span class="cm">	 * sysctl_lowmem_reserve_ratio sysctl changes.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">lowmem_reserve</span><span class="p">[</span><span class="n">MAX_NR_ZONES</span><span class="p">];</span>

	<span class="cm">/*</span>
<span class="cm">	 * This is a per-zone reserve of pages that should not be</span>
<span class="cm">	 * considered dirtyable memory.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">dirty_balance_reserve</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_NUMA</span>
	<span class="kt">int</span> <span class="n">node</span><span class="p">;</span>
	<span class="cm">/*</span>
<span class="cm">	 * zone reclaim becomes active if more unmapped pages exist.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">min_unmapped_pages</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">min_slab_pages</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="k">struct</span> <span class="n">per_cpu_pageset</span> <span class="n">__percpu</span> <span class="o">*</span><span class="n">pageset</span><span class="p">;</span>
	<span class="cm">/*</span>
<span class="cm">	 * free areas of different sizes</span>
<span class="cm">	 */</span>
	<span class="n">spinlock_t</span>		<span class="n">lock</span><span class="p">;</span>
	<span class="kt">int</span>                     <span class="n">all_unreclaimable</span><span class="p">;</span> <span class="cm">/* All pages pinned */</span>
<span class="cp">#ifdef CONFIG_MEMORY_HOTPLUG</span>
	<span class="cm">/* see spanned/present_pages for more description */</span>
	<span class="n">seqlock_t</span>		<span class="n">span_seqlock</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_CMA</span>
	<span class="cm">/*</span>
<span class="cm">	 * CMA needs to increase watermark levels during the allocation</span>
<span class="cm">	 * process to make sure that the system is not starved.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">min_cma_pages</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="k">struct</span> <span class="n">free_area</span>	<span class="n">free_area</span><span class="p">[</span><span class="n">MAX_ORDER</span><span class="p">];</span>

<span class="cp">#ifndef CONFIG_SPARSEMEM</span>
	<span class="cm">/*</span>
<span class="cm">	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.</span>
<span class="cm">	 * In SPARSEMEM, this map is stored in struct mem_section</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="o">*</span><span class="n">pageblock_flags</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_SPARSEMEM */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_COMPACTION</span>
	<span class="cm">/*</span>
<span class="cm">	 * On compaction failure, 1&lt;&lt;compact_defer_shift compactions</span>
<span class="cm">	 * are skipped before trying again. The number attempted since</span>
<span class="cm">	 * last failure is tracked with compact_considered.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">compact_considered</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span>		<span class="n">compact_defer_shift</span><span class="p">;</span>
	<span class="kt">int</span>			<span class="n">compact_order_failed</span><span class="p">;</span>
<span class="cp">#endif</span>

	<span class="n">ZONE_PADDING</span><span class="p">(</span><span class="n">_pad1_</span><span class="p">)</span>

	<span class="cm">/* Fields commonly accessed by the page reclaim scanner */</span>
	<span class="n">spinlock_t</span>		<span class="n">lru_lock</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">lruvec</span>		<span class="n">lruvec</span><span class="p">;</span>

	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">pages_scanned</span><span class="p">;</span>	   <span class="cm">/* since last reclaim */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">flags</span><span class="p">;</span>		   <span class="cm">/* zone flags, see below */</span>

	<span class="cm">/* Zone statistics */</span>
	<span class="n">atomic_long_t</span>		<span class="n">vm_stat</span><span class="p">[</span><span class="n">NR_VM_ZONE_STAT_ITEMS</span><span class="p">];</span>

	<span class="cm">/*</span>
<span class="cm">	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on</span>
<span class="cm">	 * this zone&#39;s LRU.  Maintained by the pageout code.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">inactive_ratio</span><span class="p">;</span>


	<span class="n">ZONE_PADDING</span><span class="p">(</span><span class="n">_pad2_</span><span class="p">)</span>
	<span class="cm">/* Rarely used or read-mostly fields */</span>

	<span class="cm">/*</span>
<span class="cm">	 * wait_table		-- the array holding the hash table</span>
<span class="cm">	 * wait_table_hash_nr_entries	-- the size of the hash table array</span>
<span class="cm">	 * wait_table_bits	-- wait_table_size == (1 &lt;&lt; wait_table_bits)</span>
<span class="cm">	 *</span>
<span class="cm">	 * The purpose of all these is to keep track of the people</span>
<span class="cm">	 * waiting for a page to become available and make them</span>
<span class="cm">	 * runnable again when possible. The trouble is that this</span>
<span class="cm">	 * consumes a lot of space, especially when so few things</span>
<span class="cm">	 * wait on pages at a given time. So instead of using</span>
<span class="cm">	 * per-page waitqueues, we use a waitqueue hash table.</span>
<span class="cm">	 *</span>
<span class="cm">	 * The bucket discipline is to sleep on the same queue when</span>
<span class="cm">	 * colliding and wake all in that wait queue when removing.</span>
<span class="cm">	 * When something wakes, it must check to be sure its page is</span>
<span class="cm">	 * truly available, a la thundering herd. The cost of a</span>
<span class="cm">	 * collision is great, but given the expected load of the</span>
<span class="cm">	 * table, they should be so rare as to be outweighed by the</span>
<span class="cm">	 * benefits from the saved space.</span>
<span class="cm">	 *</span>
<span class="cm">	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the</span>
<span class="cm">	 * primary users of these fields, and in mm/page_alloc.c</span>
<span class="cm">	 * free_area_init_core() performs the initialization of them.</span>
<span class="cm">	 */</span>
	<span class="n">wait_queue_head_t</span>	<span class="o">*</span> <span class="n">wait_table</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">wait_table_hash_nr_entries</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">wait_table_bits</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * Discontig memory support fields.</span>
<span class="cm">	 */</span>
	<span class="k">struct</span> <span class="n">pglist_data</span>	<span class="o">*</span><span class="n">zone_pgdat</span><span class="p">;</span>
	<span class="cm">/* zone_start_pfn == zone_start_paddr &gt;&gt; PAGE_SHIFT */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">zone_start_pfn</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * zone_start_pfn, spanned_pages and present_pages are all</span>
<span class="cm">	 * protected by span_seqlock.  It is a seqlock because it has</span>
<span class="cm">	 * to be read outside of zone-&gt;lock, and it is done in the main</span>
<span class="cm">	 * allocator path.  But, it is written quite infrequently.</span>
<span class="cm">	 *</span>
<span class="cm">	 * The lock is declared along with zone-&gt;lock because it is</span>
<span class="cm">	 * frequently read in proximity to zone-&gt;lock.  It&#39;s good to</span>
<span class="cm">	 * give them a chance of being in the same cacheline.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">spanned_pages</span><span class="p">;</span>	<span class="cm">/* total size, including holes */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span>		<span class="n">present_pages</span><span class="p">;</span>	<span class="cm">/* amount of memory (excluding holes) */</span>

	<span class="cm">/*</span>
<span class="cm">	 * rarely used fields:</span>
<span class="cm">	 */</span>
	<span class="k">const</span> <span class="kt">char</span>		<span class="o">*</span><span class="n">name</span><span class="p">;</span>
<span class="p">}</span> <span class="n">____cacheline_internodealigned_in_smp</span><span class="p">;</span>

<span class="k">typedef</span> <span class="k">enum</span> <span class="p">{</span>
	<span class="n">ZONE_RECLAIM_LOCKED</span><span class="p">,</span>		<span class="cm">/* prevents concurrent reclaim */</span>
	<span class="n">ZONE_OOM_LOCKED</span><span class="p">,</span>		<span class="cm">/* zone is in OOM killer zonelist */</span>
	<span class="n">ZONE_CONGESTED</span><span class="p">,</span>			<span class="cm">/* zone has many dirty pages backed by</span>
<span class="cm">					 * a congested BDI</span>
<span class="cm">					 */</span>
<span class="p">}</span> <span class="n">zone_flags_t</span><span class="p">;</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">zone_set_flag</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">,</span> <span class="n">zone_flags_t</span> <span class="n">flag</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">set_bit</span><span class="p">(</span><span class="n">flag</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zone_test_and_set_flag</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">,</span> <span class="n">zone_flags_t</span> <span class="n">flag</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">test_and_set_bit</span><span class="p">(</span><span class="n">flag</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">zone_clear_flag</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">,</span> <span class="n">zone_flags_t</span> <span class="n">flag</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">clear_bit</span><span class="p">(</span><span class="n">flag</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zone_is_reclaim_congested</span><span class="p">(</span><span class="k">const</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">test_bit</span><span class="p">(</span><span class="n">ZONE_CONGESTED</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zone_is_reclaim_locked</span><span class="p">(</span><span class="k">const</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">test_bit</span><span class="p">(</span><span class="n">ZONE_RECLAIM_LOCKED</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zone_is_oom_locked</span><span class="p">(</span><span class="k">const</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">test_bit</span><span class="p">(</span><span class="n">ZONE_OOM_LOCKED</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * The &quot;priority&quot; of VM scanning is how much of the queues we will scan in one</span>
<span class="cm"> * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the</span>
<span class="cm"> * queues (&quot;queue_length &gt;&gt; 12&quot;) during an aging round.</span>
<span class="cm"> */</span>
<span class="cp">#define DEF_PRIORITY 12</span>

<span class="cm">/* Maximum number of zones on a zonelist */</span>
<span class="cp">#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)</span>

<span class="cp">#ifdef CONFIG_NUMA</span>

<span class="cm">/*</span>
<span class="cm"> * The NUMA zonelists are doubled because we need zonelists that restrict the</span>
<span class="cm"> * allocations to a single node for GFP_THISNODE.</span>
<span class="cm"> *</span>
<span class="cm"> * [0]	: Zonelist with fallback</span>
<span class="cm"> * [1]	: No fallback (GFP_THISNODE)</span>
<span class="cm"> */</span>
<span class="cp">#define MAX_ZONELISTS 2</span>


<span class="cm">/*</span>
<span class="cm"> * We cache key information from each zonelist for smaller cache</span>
<span class="cm"> * footprint when scanning for free pages in get_page_from_freelist().</span>
<span class="cm"> *</span>
<span class="cm"> * 1) The BITMAP fullzones tracks which zones in a zonelist have come</span>
<span class="cm"> *    up short of free memory since the last time (last_fullzone_zap)</span>
<span class="cm"> *    we zero&#39;d fullzones.</span>
<span class="cm"> * 2) The array z_to_n[] maps each zone in the zonelist to its node</span>
<span class="cm"> *    id, so that we can efficiently evaluate whether that node is</span>
<span class="cm"> *    set in the current tasks mems_allowed.</span>
<span class="cm"> *</span>
<span class="cm"> * Both fullzones and z_to_n[] are one-to-one with the zonelist,</span>
<span class="cm"> * indexed by a zones offset in the zonelist zones[] array.</span>
<span class="cm"> *</span>
<span class="cm"> * The get_page_from_freelist() routine does two scans.  During the</span>
<span class="cm"> * first scan, we skip zones whose corresponding bit in &#39;fullzones&#39;</span>
<span class="cm"> * is set or whose corresponding node in current-&gt;mems_allowed (which</span>
<span class="cm"> * comes from cpusets) is not set.  During the second scan, we bypass</span>
<span class="cm"> * this zonelist_cache, to ensure we look methodically at each zone.</span>
<span class="cm"> *</span>
<span class="cm"> * Once per second, we zero out (zap) fullzones, forcing us to</span>
<span class="cm"> * reconsider nodes that might have regained more free memory.</span>
<span class="cm"> * The field last_full_zap is the time we last zapped fullzones.</span>
<span class="cm"> *</span>
<span class="cm"> * This mechanism reduces the amount of time we waste repeatedly</span>
<span class="cm"> * reexaming zones for free memory when they just came up low on</span>
<span class="cm"> * memory momentarilly ago.</span>
<span class="cm"> *</span>
<span class="cm"> * The zonelist_cache struct members logically belong in struct</span>
<span class="cm"> * zonelist.  However, the mempolicy zonelists constructed for</span>
<span class="cm"> * MPOL_BIND are intentionally variable length (and usually much</span>
<span class="cm"> * shorter).  A general purpose mechanism for handling structs with</span>
<span class="cm"> * multiple variable length members is more mechanism than we want</span>
<span class="cm"> * here.  We resort to some special case hackery instead.</span>
<span class="cm"> *</span>
<span class="cm"> * The MPOL_BIND zonelists don&#39;t need this zonelist_cache (in good</span>
<span class="cm"> * part because they are shorter), so we put the fixed length stuff</span>
<span class="cm"> * at the front of the zonelist struct, ending in a variable length</span>
<span class="cm"> * zones[], as is needed by MPOL_BIND.</span>
<span class="cm"> *</span>
<span class="cm"> * Then we put the optional zonelist cache on the end of the zonelist</span>
<span class="cm"> * struct.  This optional stuff is found by a &#39;zlcache_ptr&#39; pointer in</span>
<span class="cm"> * the fixed length portion at the front of the struct.  This pointer</span>
<span class="cm"> * both enables us to find the zonelist cache, and in the case of</span>
<span class="cm"> * MPOL_BIND zonelists, (which will just set the zlcache_ptr to NULL)</span>
<span class="cm"> * to know that the zonelist cache is not there.</span>
<span class="cm"> *</span>
<span class="cm"> * The end result is that struct zonelists come in two flavors:</span>
<span class="cm"> *  1) The full, fixed length version, shown below, and</span>
<span class="cm"> *  2) The custom zonelists for MPOL_BIND.</span>
<span class="cm"> * The custom MPOL_BIND zonelists have a NULL zlcache_ptr and no zlcache.</span>
<span class="cm"> *</span>
<span class="cm"> * Even though there may be multiple CPU cores on a node modifying</span>
<span class="cm"> * fullzones or last_full_zap in the same zonelist_cache at the same</span>
<span class="cm"> * time, we don&#39;t lock it.  This is just hint data - if it is wrong now</span>
<span class="cm"> * and then, the allocator will still function, perhaps a bit slower.</span>
<span class="cm"> */</span>


<span class="k">struct</span> <span class="n">zonelist_cache</span> <span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">short</span> <span class="n">z_to_n</span><span class="p">[</span><span class="n">MAX_ZONES_PER_ZONELIST</span><span class="p">];</span>		<span class="cm">/* zone-&gt;nid */</span>
	<span class="n">DECLARE_BITMAP</span><span class="p">(</span><span class="n">fullzones</span><span class="p">,</span> <span class="n">MAX_ZONES_PER_ZONELIST</span><span class="p">);</span>	<span class="cm">/* zone full? */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">last_full_zap</span><span class="p">;</span>		<span class="cm">/* when last zap&#39;d (jiffies) */</span>
<span class="p">};</span>
<span class="cp">#else</span>
<span class="cp">#define MAX_ZONELISTS 1</span>
<span class="k">struct</span> <span class="n">zonelist_cache</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cm">/*</span>
<span class="cm"> * This struct contains information about a zone in a zonelist. It is stored</span>
<span class="cm"> * here to avoid dereferences into large structures and lookups of tables</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">zoneref</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">;</span>	<span class="cm">/* Pointer to actual zone */</span>
	<span class="kt">int</span> <span class="n">zone_idx</span><span class="p">;</span>		<span class="cm">/* zone_idx(zoneref-&gt;zone) */</span>
<span class="p">};</span>

<span class="cm">/*</span>
<span class="cm"> * One allocation request operates on a zonelist. A zonelist</span>
<span class="cm"> * is a list of zones, the first one is the &#39;goal&#39; of the</span>
<span class="cm"> * allocation, the other zones are fallback zones, in decreasing</span>
<span class="cm"> * priority.</span>
<span class="cm"> *</span>
<span class="cm"> * If zlcache_ptr is not NULL, then it is just the address of zlcache,</span>
<span class="cm"> * as explained above.  If zlcache_ptr is NULL, there is no zlcache.</span>
<span class="cm"> * *</span>
<span class="cm"> * To speed the reading of the zonelist, the zonerefs contain the zone index</span>
<span class="cm"> * of the entry being read. Helper functions to access information given</span>
<span class="cm"> * a struct zoneref are</span>
<span class="cm"> *</span>
<span class="cm"> * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs</span>
<span class="cm"> * zonelist_zone_idx()	- Return the index of the zone for an entry</span>
<span class="cm"> * zonelist_node_idx()	- Return the index of the node for an entry</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">zonelist</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">zonelist_cache</span> <span class="o">*</span><span class="n">zlcache_ptr</span><span class="p">;</span>		     <span class="c1">// NULL or &amp;zlcache</span>
	<span class="k">struct</span> <span class="n">zoneref</span> <span class="n">_zonerefs</span><span class="p">[</span><span class="n">MAX_ZONES_PER_ZONELIST</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
<span class="cp">#ifdef CONFIG_NUMA</span>
	<span class="k">struct</span> <span class="n">zonelist_cache</span> <span class="n">zlcache</span><span class="p">;</span>			     <span class="c1">// optional ...</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="cp">#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP</span>
<span class="k">struct</span> <span class="n">node_active_region</span> <span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">start_pfn</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">end_pfn</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">nid</span><span class="p">;</span>
<span class="p">};</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_HAVE_MEMBLOCK_NODE_MAP */</span><span class="cp"></span>

<span class="cp">#ifndef CONFIG_DISCONTIGMEM</span>
<span class="cm">/* The array of struct pages - for discontigmem use pgdat-&gt;lmem_map */</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">mem_map</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cm">/*</span>
<span class="cm"> * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM</span>
<span class="cm"> * (mostly NUMA machines?) to denote a higher-level memory zone than the</span>
<span class="cm"> * zone denotes.</span>
<span class="cm"> *</span>
<span class="cm"> * On NUMA machines, each NUMA node would have a pg_data_t to describe</span>
<span class="cm"> * it&#39;s memory layout.</span>
<span class="cm"> *</span>
<span class="cm"> * Memory statistics and page replacement data structures are maintained on a</span>
<span class="cm"> * per-zone basis.</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">bootmem_data</span><span class="p">;</span>
<span class="k">typedef</span> <span class="k">struct</span> <span class="n">pglist_data</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">zone</span> <span class="n">node_zones</span><span class="p">[</span><span class="n">MAX_NR_ZONES</span><span class="p">];</span>
	<span class="k">struct</span> <span class="n">zonelist</span> <span class="n">node_zonelists</span><span class="p">[</span><span class="n">MAX_ZONELISTS</span><span class="p">];</span>
	<span class="kt">int</span> <span class="n">nr_zones</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_FLAT_NODE_MEM_MAP	</span><span class="cm">/* means !SPARSEMEM */</span><span class="cp"></span>
	<span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">node_mem_map</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_CGROUP_MEM_RES_CTLR</span>
	<span class="k">struct</span> <span class="n">page_cgroup</span> <span class="o">*</span><span class="n">node_page_cgroup</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#endif</span>
<span class="cp">#ifndef CONFIG_NO_BOOTMEM</span>
	<span class="k">struct</span> <span class="n">bootmem_data</span> <span class="o">*</span><span class="n">bdata</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_MEMORY_HOTPLUG</span>
	<span class="cm">/*</span>
<span class="cm">	 * Must be held any time you expect node_start_pfn, node_present_pages</span>
<span class="cm">	 * or node_spanned_pages stay constant.  Holding this will also</span>
<span class="cm">	 * guarantee that any pfn_valid() stays that way.</span>
<span class="cm">	 *</span>
<span class="cm">	 * Nests above zone-&gt;lock and zone-&gt;size_seqlock.</span>
<span class="cm">	 */</span>
	<span class="n">spinlock_t</span> <span class="n">node_size_lock</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">node_start_pfn</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">node_present_pages</span><span class="p">;</span> <span class="cm">/* total number of physical pages */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">node_spanned_pages</span><span class="p">;</span> <span class="cm">/* total size of physical page</span>
<span class="cm">					     range, including holes */</span>
	<span class="kt">int</span> <span class="n">node_id</span><span class="p">;</span>
	<span class="n">wait_queue_head_t</span> <span class="n">kswapd_wait</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">kswapd</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">kswapd_max_order</span><span class="p">;</span>
	<span class="k">enum</span> <span class="n">zone_type</span> <span class="n">classzone_idx</span><span class="p">;</span>
<span class="p">}</span> <span class="n">pg_data_t</span><span class="p">;</span>

<span class="cp">#define node_present_pages(nid)	(NODE_DATA(nid)-&gt;node_present_pages)</span>
<span class="cp">#define node_spanned_pages(nid)	(NODE_DATA(nid)-&gt;node_spanned_pages)</span>
<span class="cp">#ifdef CONFIG_FLAT_NODE_MEM_MAP</span>
<span class="cp">#define pgdat_page_nr(pgdat, pagenr)	((pgdat)-&gt;node_mem_map + (pagenr))</span>
<span class="cp">#else</span>
<span class="cp">#define pgdat_page_nr(pgdat, pagenr)	pfn_to_page((pgdat)-&gt;node_start_pfn + (pagenr))</span>
<span class="cp">#endif</span>
<span class="cp">#define nid_page_nr(nid, pagenr) 	pgdat_page_nr(NODE_DATA(nid),(pagenr))</span>

<span class="cp">#define node_start_pfn(nid)	(NODE_DATA(nid)-&gt;node_start_pfn)</span>

<span class="cp">#define node_end_pfn(nid) ({\</span>
<span class="cp">	pg_data_t *__pgdat = NODE_DATA(nid);\</span>
<span class="cp">	__pgdat-&gt;node_start_pfn + __pgdat-&gt;node_spanned_pages;\</span>
<span class="cp">})</span>

<span class="cp">#include &lt;linux/memory_hotplug.h&gt;</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">mutex</span> <span class="n">zonelists_mutex</span><span class="p">;</span>
<span class="kt">void</span> <span class="n">build_all_zonelists</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">data</span><span class="p">);</span>
<span class="kt">void</span> <span class="n">wakeup_kswapd</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">,</span> <span class="kt">int</span> <span class="n">order</span><span class="p">,</span> <span class="k">enum</span> <span class="n">zone_type</span> <span class="n">classzone_idx</span><span class="p">);</span>
<span class="n">bool</span> <span class="n">zone_watermark_ok</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">z</span><span class="p">,</span> <span class="kt">int</span> <span class="n">order</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mark</span><span class="p">,</span>
		<span class="kt">int</span> <span class="n">classzone_idx</span><span class="p">,</span> <span class="kt">int</span> <span class="n">alloc_flags</span><span class="p">);</span>
<span class="n">bool</span> <span class="n">zone_watermark_ok_safe</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">z</span><span class="p">,</span> <span class="kt">int</span> <span class="n">order</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mark</span><span class="p">,</span>
		<span class="kt">int</span> <span class="n">classzone_idx</span><span class="p">,</span> <span class="kt">int</span> <span class="n">alloc_flags</span><span class="p">);</span>
<span class="k">enum</span> <span class="n">memmap_context</span> <span class="p">{</span>
	<span class="n">MEMMAP_EARLY</span><span class="p">,</span>
	<span class="n">MEMMAP_HOTPLUG</span><span class="p">,</span>
<span class="p">};</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">init_currently_empty_zone</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">start_pfn</span><span class="p">,</span>
				     <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">size</span><span class="p">,</span>
				     <span class="k">enum</span> <span class="n">memmap_context</span> <span class="n">context</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">lruvec_init</span><span class="p">(</span><span class="k">struct</span> <span class="n">lruvec</span> <span class="o">*</span><span class="n">lruvec</span><span class="p">,</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">);</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="nf">lruvec_zone</span><span class="p">(</span><span class="k">struct</span> <span class="n">lruvec</span> <span class="o">*</span><span class="n">lruvec</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_CGROUP_MEM_RES_CTLR</span>
	<span class="k">return</span> <span class="n">lruvec</span><span class="o">-&gt;</span><span class="n">zone</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="n">container_of</span><span class="p">(</span><span class="n">lruvec</span><span class="p">,</span> <span class="k">struct</span> <span class="n">zone</span><span class="p">,</span> <span class="n">lruvec</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HAVE_MEMORY_PRESENT</span>
<span class="kt">void</span> <span class="n">memory_present</span><span class="p">(</span><span class="kt">int</span> <span class="n">nid</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">start</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">end</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">memory_present</span><span class="p">(</span><span class="kt">int</span> <span class="n">nid</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">start</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">end</span><span class="p">)</span> <span class="p">{}</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_HAVE_MEMORYLESS_NODES</span>
<span class="kt">int</span> <span class="n">local_memory_node</span><span class="p">(</span><span class="kt">int</span> <span class="n">node_id</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">local_memory_node</span><span class="p">(</span><span class="kt">int</span> <span class="n">node_id</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">node_id</span><span class="p">;</span> <span class="p">};</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_NEED_NODE_MEMMAP_SIZE</span>
<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">__init</span> <span class="n">node_memmap_size_bytes</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span><span class="p">);</span>
<span class="cp">#endif</span>

<span class="cm">/*</span>
<span class="cm"> * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.</span>
<span class="cm"> */</span>
<span class="cp">#define zone_idx(zone)		((zone) - (zone)-&gt;zone_pgdat-&gt;node_zones)</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">populated_zone</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="o">!!</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">present_pages</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">movable_zone</span><span class="p">;</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zone_movable_is_highmem</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#if defined(CONFIG_HIGHMEM) &amp;&amp; defined(CONFIG_HAVE_MEMBLOCK_NODE)</span>
	<span class="k">return</span> <span class="n">movable_zone</span> <span class="o">==</span> <span class="n">ZONE_HIGHMEM</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_highmem_idx</span><span class="p">(</span><span class="k">enum</span> <span class="n">zone_type</span> <span class="n">idx</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_HIGHMEM</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">idx</span> <span class="o">==</span> <span class="n">ZONE_HIGHMEM</span> <span class="o">||</span>
		<span class="p">(</span><span class="n">idx</span> <span class="o">==</span> <span class="n">ZONE_MOVABLE</span> <span class="o">&amp;&amp;</span> <span class="n">zone_movable_is_highmem</span><span class="p">()));</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_normal_idx</span><span class="p">(</span><span class="k">enum</span> <span class="n">zone_type</span> <span class="n">idx</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">idx</span> <span class="o">==</span> <span class="n">ZONE_NORMAL</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * is_highmem - helper function to quickly check if a struct zone is a </span>
<span class="cm"> *              highmem zone or not.  This is an attempt to keep references</span>
<span class="cm"> *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.</span>
<span class="cm"> * @zone - pointer to struct zone variable</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_highmem</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_HIGHMEM</span>
	<span class="kt">int</span> <span class="n">zone_off</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">zone</span> <span class="o">-</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">zone_pgdat</span><span class="o">-&gt;</span><span class="n">node_zones</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">zone_off</span> <span class="o">==</span> <span class="n">ZONE_HIGHMEM</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="n">zone</span><span class="p">)</span> <span class="o">||</span>
	       <span class="p">(</span><span class="n">zone_off</span> <span class="o">==</span> <span class="n">ZONE_MOVABLE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="n">zone</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
		<span class="n">zone_movable_is_highmem</span><span class="p">());</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_normal</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">zone</span> <span class="o">==</span> <span class="n">zone</span><span class="o">-&gt;</span><span class="n">zone_pgdat</span><span class="o">-&gt;</span><span class="n">node_zones</span> <span class="o">+</span> <span class="n">ZONE_NORMAL</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_dma32</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_ZONE_DMA32</span>
	<span class="k">return</span> <span class="n">zone</span> <span class="o">==</span> <span class="n">zone</span><span class="o">-&gt;</span><span class="n">zone_pgdat</span><span class="o">-&gt;</span><span class="n">node_zones</span> <span class="o">+</span> <span class="n">ZONE_DMA32</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">is_dma</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_ZONE_DMA</span>
	<span class="k">return</span> <span class="n">zone</span> <span class="o">==</span> <span class="n">zone</span><span class="o">-&gt;</span><span class="n">zone_pgdat</span><span class="o">-&gt;</span><span class="n">node_zones</span> <span class="o">+</span> <span class="n">ZONE_DMA</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="cm">/* These two functions are used to setup the per zone pages min values */</span>
<span class="k">struct</span> <span class="n">ctl_table</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">min_free_kbytes_sysctl_handler</span><span class="p">(</span><span class="k">struct</span> <span class="n">ctl_table</span> <span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span>
					<span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="p">,</span> <span class="kt">size_t</span> <span class="o">*</span><span class="p">,</span> <span class="n">loff_t</span> <span class="o">*</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">sysctl_lowmem_reserve_ratio</span><span class="p">[</span><span class="n">MAX_NR_ZONES</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>
<span class="kt">int</span> <span class="n">lowmem_reserve_ratio_sysctl_handler</span><span class="p">(</span><span class="k">struct</span> <span class="n">ctl_table</span> <span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span>
					<span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="p">,</span> <span class="kt">size_t</span> <span class="o">*</span><span class="p">,</span> <span class="n">loff_t</span> <span class="o">*</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">percpu_pagelist_fraction_sysctl_handler</span><span class="p">(</span><span class="k">struct</span> <span class="n">ctl_table</span> <span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span>
					<span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="p">,</span> <span class="kt">size_t</span> <span class="o">*</span><span class="p">,</span> <span class="n">loff_t</span> <span class="o">*</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">sysctl_min_unmapped_ratio_sysctl_handler</span><span class="p">(</span><span class="k">struct</span> <span class="n">ctl_table</span> <span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span>
			<span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="p">,</span> <span class="kt">size_t</span> <span class="o">*</span><span class="p">,</span> <span class="n">loff_t</span> <span class="o">*</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">sysctl_min_slab_ratio_sysctl_handler</span><span class="p">(</span><span class="k">struct</span> <span class="n">ctl_table</span> <span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span>
			<span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="p">,</span> <span class="kt">size_t</span> <span class="o">*</span><span class="p">,</span> <span class="n">loff_t</span> <span class="o">*</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">numa_zonelist_order_handler</span><span class="p">(</span><span class="k">struct</span> <span class="n">ctl_table</span> <span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span>
			<span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="p">,</span> <span class="kt">size_t</span> <span class="o">*</span><span class="p">,</span> <span class="n">loff_t</span> <span class="o">*</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">char</span> <span class="n">numa_zonelist_order</span><span class="p">[];</span>
<span class="cp">#define NUMA_ZONELIST_ORDER_LEN 16	</span><span class="cm">/* string buffer size */</span><span class="cp"></span>

<span class="cp">#ifndef CONFIG_NEED_MULTIPLE_NODES</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">pglist_data</span> <span class="n">contig_page_data</span><span class="p">;</span>
<span class="cp">#define NODE_DATA(nid)		(&amp;contig_page_data)</span>
<span class="cp">#define NODE_MEM_MAP(nid)	mem_map</span>

<span class="cp">#else </span><span class="cm">/* CONFIG_NEED_MULTIPLE_NODES */</span><span class="cp"></span>

<span class="cp">#include &lt;asm/mmzone.h&gt;</span>

<span class="cp">#endif </span><span class="cm">/* !CONFIG_NEED_MULTIPLE_NODES */</span><span class="cp"></span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">pglist_data</span> <span class="o">*</span><span class="n">first_online_pgdat</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">pglist_data</span> <span class="o">*</span><span class="n">next_online_pgdat</span><span class="p">(</span><span class="k">struct</span> <span class="n">pglist_data</span> <span class="o">*</span><span class="n">pgdat</span><span class="p">);</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">next_zone</span><span class="p">(</span><span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * for_each_online_pgdat - helper macro to iterate over all online nodes</span>
<span class="cm"> * @pgdat - pointer to a pg_data_t variable</span>
<span class="cm"> */</span>
<span class="cp">#define for_each_online_pgdat(pgdat)			\</span>
<span class="cp">	for (pgdat = first_online_pgdat();		\</span>
<span class="cp">	     pgdat;					\</span>
<span class="cp">	     pgdat = next_online_pgdat(pgdat))</span>
<span class="cm">/**</span>
<span class="cm"> * for_each_zone - helper macro to iterate over all memory zones</span>
<span class="cm"> * @zone - pointer to struct zone variable</span>
<span class="cm"> *</span>
<span class="cm"> * The user only needs to declare the zone variable, for_each_zone</span>
<span class="cm"> * fills it in.</span>
<span class="cm"> */</span>
<span class="cp">#define for_each_zone(zone)			        \</span>
<span class="cp">	for (zone = (first_online_pgdat())-&gt;node_zones; \</span>
<span class="cp">	     zone;					\</span>
<span class="cp">	     zone = next_zone(zone))</span>

<span class="cp">#define for_each_populated_zone(zone)		        \</span>
<span class="cp">	for (zone = (first_online_pgdat())-&gt;node_zones; \</span>
<span class="cp">	     zone;					\</span>
<span class="cp">	     zone = next_zone(zone))			\</span>
<span class="cp">		if (!populated_zone(zone))		\</span>
<span class="cp">			; </span><span class="cm">/* do nothing */</span><span class="cp">		\</span>
<span class="cp">		else</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="nf">zonelist_zone</span><span class="p">(</span><span class="k">struct</span> <span class="n">zoneref</span> <span class="o">*</span><span class="n">zoneref</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">zoneref</span><span class="o">-&gt;</span><span class="n">zone</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zonelist_zone_idx</span><span class="p">(</span><span class="k">struct</span> <span class="n">zoneref</span> <span class="o">*</span><span class="n">zoneref</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">zoneref</span><span class="o">-&gt;</span><span class="n">zone_idx</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">zonelist_node_idx</span><span class="p">(</span><span class="k">struct</span> <span class="n">zoneref</span> <span class="o">*</span><span class="n">zoneref</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_NUMA</span>
	<span class="cm">/* zone_to_nid not available in this context */</span>
	<span class="k">return</span> <span class="n">zoneref</span><span class="o">-&gt;</span><span class="n">zone</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_NUMA */</span><span class="cp"></span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point</span>
<span class="cm"> * @z - The cursor used as a starting point for the search</span>
<span class="cm"> * @highest_zoneidx - The zone index of the highest zone to return</span>
<span class="cm"> * @nodes - An optional nodemask to filter the zonelist with</span>
<span class="cm"> * @zone - The first suitable zone found is returned via this parameter</span>
<span class="cm"> *</span>
<span class="cm"> * This function returns the next zone at or below a given zone index that is</span>
<span class="cm"> * within the allowed nodemask using a cursor as the starting point for the</span>
<span class="cm"> * search. The zoneref returned is a cursor that represents the current zone</span>
<span class="cm"> * being examined. It should be advanced by one before calling</span>
<span class="cm"> * next_zones_zonelist again.</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">zoneref</span> <span class="o">*</span><span class="n">next_zones_zonelist</span><span class="p">(</span><span class="k">struct</span> <span class="n">zoneref</span> <span class="o">*</span><span class="n">z</span><span class="p">,</span>
					<span class="k">enum</span> <span class="n">zone_type</span> <span class="n">highest_zoneidx</span><span class="p">,</span>
					<span class="n">nodemask_t</span> <span class="o">*</span><span class="n">nodes</span><span class="p">,</span>
					<span class="k">struct</span> <span class="n">zone</span> <span class="o">**</span><span class="n">zone</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist</span>
<span class="cm"> * @zonelist - The zonelist to search for a suitable zone</span>
<span class="cm"> * @highest_zoneidx - The zone index of the highest zone to return</span>
<span class="cm"> * @nodes - An optional nodemask to filter the zonelist with</span>
<span class="cm"> * @zone - The first suitable zone found is returned via this parameter</span>
<span class="cm"> *</span>
<span class="cm"> * This function returns the first zone at or below a given zone index that is</span>
<span class="cm"> * within the allowed nodemask. The zoneref returned is a cursor that can be</span>
<span class="cm"> * used to iterate the zonelist with next_zones_zonelist by advancing it by</span>
<span class="cm"> * one before calling.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">zoneref</span> <span class="o">*</span><span class="nf">first_zones_zonelist</span><span class="p">(</span><span class="k">struct</span> <span class="n">zonelist</span> <span class="o">*</span><span class="n">zonelist</span><span class="p">,</span>
					<span class="k">enum</span> <span class="n">zone_type</span> <span class="n">highest_zoneidx</span><span class="p">,</span>
					<span class="n">nodemask_t</span> <span class="o">*</span><span class="n">nodes</span><span class="p">,</span>
					<span class="k">struct</span> <span class="n">zone</span> <span class="o">**</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">next_zones_zonelist</span><span class="p">(</span><span class="n">zonelist</span><span class="o">-&gt;</span><span class="n">_zonerefs</span><span class="p">,</span> <span class="n">highest_zoneidx</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span>
								<span class="n">zone</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask</span>
<span class="cm"> * @zone - The current zone in the iterator</span>
<span class="cm"> * @z - The current pointer within zonelist-&gt;zones being iterated</span>
<span class="cm"> * @zlist - The zonelist being iterated</span>
<span class="cm"> * @highidx - The zone index of the highest zone to return</span>
<span class="cm"> * @nodemask - Nodemask allowed by the allocator</span>
<span class="cm"> *</span>
<span class="cm"> * This iterator iterates though all zones at or below a given zone index and</span>
<span class="cm"> * within a given nodemask</span>
<span class="cm"> */</span>
<span class="cp">#define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \</span>
<span class="cp">	for (z = first_zones_zonelist(zlist, highidx, nodemask, &amp;zone);	\</span>
<span class="cp">		zone;							\</span>
<span class="cp">		z = next_zones_zonelist(++z, highidx, nodemask, &amp;zone))	\</span>

<span class="cm">/**</span>
<span class="cm"> * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index</span>
<span class="cm"> * @zone - The current zone in the iterator</span>
<span class="cm"> * @z - The current pointer within zonelist-&gt;zones being iterated</span>
<span class="cm"> * @zlist - The zonelist being iterated</span>
<span class="cm"> * @highidx - The zone index of the highest zone to return</span>
<span class="cm"> *</span>
<span class="cm"> * This iterator iterates though all zones at or below a given zone index.</span>
<span class="cm"> */</span>
<span class="cp">#define for_each_zone_zonelist(zone, z, zlist, highidx) \</span>
<span class="cp">	for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, NULL)</span>

<span class="cp">#ifdef CONFIG_SPARSEMEM</span>
<span class="cp">#include &lt;asm/sparsemem.h&gt;</span>
<span class="cp">#endif</span>

<span class="cp">#if !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) &amp;&amp; \</span>
<span class="cp">	!defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="nf">early_pfn_to_nid</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_FLATMEM</span>
<span class="cp">#define pfn_to_nid(pfn)		(0)</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_SPARSEMEM</span>

<span class="cm">/*</span>
<span class="cm"> * SECTION_SHIFT    		#bits space required to store a section #</span>
<span class="cm"> *</span>
<span class="cm"> * PA_SECTION_SHIFT		physical address to/from section number</span>
<span class="cm"> * PFN_SECTION_SHIFT		pfn to/from section number</span>
<span class="cm"> */</span>
<span class="cp">#define SECTIONS_SHIFT		(MAX_PHYSMEM_BITS - SECTION_SIZE_BITS)</span>

<span class="cp">#define PA_SECTION_SHIFT	(SECTION_SIZE_BITS)</span>
<span class="cp">#define PFN_SECTION_SHIFT	(SECTION_SIZE_BITS - PAGE_SHIFT)</span>

<span class="cp">#define NR_MEM_SECTIONS		(1UL &lt;&lt; SECTIONS_SHIFT)</span>

<span class="cp">#define PAGES_PER_SECTION       (1UL &lt;&lt; PFN_SECTION_SHIFT)</span>
<span class="cp">#define PAGE_SECTION_MASK	(~(PAGES_PER_SECTION-1))</span>

<span class="cp">#define SECTION_BLOCKFLAGS_BITS \</span>
<span class="cp">	((1UL &lt;&lt; (PFN_SECTION_SHIFT - pageblock_order)) * NR_PAGEBLOCK_BITS)</span>

<span class="cp">#if (MAX_ORDER - 1 + PAGE_SHIFT) &gt; SECTION_SIZE_BITS</span>
<span class="cp">#error Allocator MAX_ORDER exceeds SECTION_SIZE</span>
<span class="cp">#endif</span>

<span class="cp">#define pfn_to_section_nr(pfn) ((pfn) &gt;&gt; PFN_SECTION_SHIFT)</span>
<span class="cp">#define section_nr_to_pfn(sec) ((sec) &lt;&lt; PFN_SECTION_SHIFT)</span>

<span class="cp">#define SECTION_ALIGN_UP(pfn)	(((pfn) + PAGES_PER_SECTION - 1) &amp; PAGE_SECTION_MASK)</span>
<span class="cp">#define SECTION_ALIGN_DOWN(pfn)	((pfn) &amp; PAGE_SECTION_MASK)</span>

<span class="k">struct</span> <span class="n">page</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">page_cgroup</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">mem_section</span> <span class="p">{</span>
	<span class="cm">/*</span>
<span class="cm">	 * This is, logically, a pointer to an array of struct</span>
<span class="cm">	 * pages.  However, it is stored with some other magic.</span>
<span class="cm">	 * (see sparse.c::sparse_init_one_section())</span>
<span class="cm">	 *</span>
<span class="cm">	 * Additionally during early boot we encode node id of</span>
<span class="cm">	 * the location of the section here to guide allocation.</span>
<span class="cm">	 * (see sparse.c::memory_present())</span>
<span class="cm">	 *</span>
<span class="cm">	 * Making it a UL at least makes someone do a cast</span>
<span class="cm">	 * before using it wrong.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">section_mem_map</span><span class="p">;</span>

	<span class="cm">/* See declaration of similar field in struct zone */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="o">*</span><span class="n">pageblock_flags</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_CGROUP_MEM_RES_CTLR</span>
	<span class="cm">/*</span>
<span class="cm">	 * If !SPARSEMEM, pgdat doesn&#39;t have page_cgroup pointer. We use</span>
<span class="cm">	 * section. (see memcontrol.h/page_cgroup.h about this.)</span>
<span class="cm">	 */</span>
	<span class="k">struct</span> <span class="n">page_cgroup</span> <span class="o">*</span><span class="n">page_cgroup</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pad</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="cp">#ifdef CONFIG_SPARSEMEM_EXTREME</span>
<span class="cp">#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))</span>
<span class="cp">#else</span>
<span class="cp">#define SECTIONS_PER_ROOT	1</span>
<span class="cp">#endif</span>

<span class="cp">#define SECTION_NR_TO_ROOT(sec)	((sec) / SECTIONS_PER_ROOT)</span>
<span class="cp">#define NR_SECTION_ROOTS	DIV_ROUND_UP(NR_MEM_SECTIONS, SECTIONS_PER_ROOT)</span>
<span class="cp">#define SECTION_ROOT_MASK	(SECTIONS_PER_ROOT - 1)</span>

<span class="cp">#ifdef CONFIG_SPARSEMEM_EXTREME</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">mem_section</span> <span class="o">*</span><span class="n">mem_section</span><span class="p">[</span><span class="n">NR_SECTION_ROOTS</span><span class="p">];</span>
<span class="cp">#else</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">mem_section</span> <span class="n">mem_section</span><span class="p">[</span><span class="n">NR_SECTION_ROOTS</span><span class="p">][</span><span class="n">SECTIONS_PER_ROOT</span><span class="p">];</span>
<span class="cp">#endif</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">mem_section</span> <span class="o">*</span><span class="nf">__nr_to_section</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nr</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">mem_section</span><span class="p">[</span><span class="n">SECTION_NR_TO_ROOT</span><span class="p">(</span><span class="n">nr</span><span class="p">)])</span>
		<span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="n">mem_section</span><span class="p">[</span><span class="n">SECTION_NR_TO_ROOT</span><span class="p">(</span><span class="n">nr</span><span class="p">)][</span><span class="n">nr</span> <span class="o">&amp;</span> <span class="n">SECTION_ROOT_MASK</span><span class="p">];</span>
<span class="p">}</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">__section_nr</span><span class="p">(</span><span class="k">struct</span> <span class="n">mem_section</span><span class="o">*</span> <span class="n">ms</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">usemap_size</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * We use the lower bits of the mem_map pointer to store</span>
<span class="cm"> * a little bit of information.  There should be at least</span>
<span class="cm"> * 3 bits here due to 32-bit alignment.</span>
<span class="cm"> */</span>
<span class="cp">#define	SECTION_MARKED_PRESENT	(1UL&lt;&lt;0)</span>
<span class="cp">#define SECTION_HAS_MEM_MAP	(1UL&lt;&lt;1)</span>
<span class="cp">#define SECTION_MAP_LAST_BIT	(1UL&lt;&lt;2)</span>
<span class="cp">#define SECTION_MAP_MASK	(~(SECTION_MAP_LAST_BIT-1))</span>
<span class="cp">#define SECTION_NID_SHIFT	2</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="nf">__section_mem_map_addr</span><span class="p">(</span><span class="k">struct</span> <span class="n">mem_section</span> <span class="o">*</span><span class="n">section</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">map</span> <span class="o">=</span> <span class="n">section</span><span class="o">-&gt;</span><span class="n">section_mem_map</span><span class="p">;</span>
	<span class="n">map</span> <span class="o">&amp;=</span> <span class="n">SECTION_MAP_MASK</span><span class="p">;</span>
	<span class="k">return</span> <span class="p">(</span><span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="p">)</span><span class="n">map</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">present_section</span><span class="p">(</span><span class="k">struct</span> <span class="n">mem_section</span> <span class="o">*</span><span class="n">section</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">section</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">section</span><span class="o">-&gt;</span><span class="n">section_mem_map</span> <span class="o">&amp;</span> <span class="n">SECTION_MARKED_PRESENT</span><span class="p">));</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">present_section_nr</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nr</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">present_section</span><span class="p">(</span><span class="n">__nr_to_section</span><span class="p">(</span><span class="n">nr</span><span class="p">));</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">valid_section</span><span class="p">(</span><span class="k">struct</span> <span class="n">mem_section</span> <span class="o">*</span><span class="n">section</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">section</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">section</span><span class="o">-&gt;</span><span class="n">section_mem_map</span> <span class="o">&amp;</span> <span class="n">SECTION_HAS_MEM_MAP</span><span class="p">));</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">valid_section_nr</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nr</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">valid_section</span><span class="p">(</span><span class="n">__nr_to_section</span><span class="p">(</span><span class="n">nr</span><span class="p">));</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">mem_section</span> <span class="o">*</span><span class="nf">__pfn_to_section</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">__nr_to_section</span><span class="p">(</span><span class="n">pfn_to_section_nr</span><span class="p">(</span><span class="n">pfn</span><span class="p">));</span>
<span class="p">}</span>

<span class="cp">#ifndef CONFIG_HAVE_ARCH_PFN_VALID</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">pfn_valid</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">pfn_to_section_nr</span><span class="p">(</span><span class="n">pfn</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">NR_MEM_SECTIONS</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">valid_section</span><span class="p">(</span><span class="n">__nr_to_section</span><span class="p">(</span><span class="n">pfn_to_section_nr</span><span class="p">(</span><span class="n">pfn</span><span class="p">)));</span>
<span class="p">}</span>
<span class="cp">#endif</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">pfn_present</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">pfn_to_section_nr</span><span class="p">(</span><span class="n">pfn</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">NR_MEM_SECTIONS</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">present_section</span><span class="p">(</span><span class="n">__nr_to_section</span><span class="p">(</span><span class="n">pfn_to_section_nr</span><span class="p">(</span><span class="n">pfn</span><span class="p">)));</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * These are _only_ used during initialisation, therefore they</span>
<span class="cm"> * can use __initdata ...  They could have names to indicate</span>
<span class="cm"> * this restriction.</span>
<span class="cm"> */</span>
<span class="cp">#ifdef CONFIG_NUMA</span>
<span class="cp">#define pfn_to_nid(pfn)							\</span>
<span class="cp">({									\</span>
<span class="cp">	unsigned long __pfn_to_nid_pfn = (pfn);				\</span>
<span class="cp">	page_to_nid(pfn_to_page(__pfn_to_nid_pfn));			\</span>
<span class="cp">})</span>
<span class="cp">#else</span>
<span class="cp">#define pfn_to_nid(pfn)		(0)</span>
<span class="cp">#endif</span>

<span class="cp">#define early_pfn_valid(pfn)	pfn_valid(pfn)</span>
<span class="kt">void</span> <span class="n">sparse_init</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="cp">#define sparse_init()	do {} while (0)</span>
<span class="cp">#define sparse_index_init(_sec, _nid)  do {} while (0)</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_SPARSEMEM */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_NODES_SPAN_OTHER_NODES</span>
<span class="n">bool</span> <span class="n">early_pfn_in_nid</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nid</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="cp">#define early_pfn_in_nid(pfn, nid)	(1)</span>
<span class="cp">#endif</span>

<span class="cp">#ifndef early_pfn_valid</span>
<span class="cp">#define early_pfn_valid(pfn)	(1)</span>
<span class="cp">#endif</span>

<span class="kt">void</span> <span class="n">memory_present</span><span class="p">(</span><span class="kt">int</span> <span class="n">nid</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">start</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">end</span><span class="p">);</span>
<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">__init</span> <span class="n">node_memmap_size_bytes</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we</span>
<span class="cm"> * need to check pfn validility within that MAX_ORDER_NR_PAGES block.</span>
<span class="cm"> * pfn_valid_within() should be used in this case; we optimise this away</span>
<span class="cm"> * when we have no holes within a MAX_ORDER_NR_PAGES block.</span>
<span class="cm"> */</span>
<span class="cp">#ifdef CONFIG_HOLES_IN_ZONE</span>
<span class="cp">#define pfn_valid_within(pfn) pfn_valid(pfn)</span>
<span class="cp">#else</span>
<span class="cp">#define pfn_valid_within(pfn) (1)</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_ARCH_HAS_HOLES_MEMORYMODEL</span>
<span class="cm">/*</span>
<span class="cm"> * pfn_valid() is meant to be able to tell if a given PFN has valid memmap</span>
<span class="cm"> * associated with it or not. In FLATMEM, it is expected that holes always</span>
<span class="cm"> * have valid memmap as long as there is valid PFNs either side of the hole.</span>
<span class="cm"> * In SPARSEMEM, it is assumed that a valid section has a memmap for the</span>
<span class="cm"> * entire section.</span>
<span class="cm"> *</span>
<span class="cm"> * However, an ARM, and maybe other embedded architectures in the future</span>
<span class="cm"> * free memmap backing holes to save memory on the assumption the memmap is</span>
<span class="cm"> * never used. The page_zone linkages are then broken even though pfn_valid()</span>
<span class="cm"> * returns true. A walker of the full memmap must then do this additional</span>
<span class="cm"> * check to ensure the memmap they are looking at is sane by making sure</span>
<span class="cm"> * the zone and PFN linkages are still valid. This is expensive, but walkers</span>
<span class="cm"> * of the full memmap are extremely rare.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="n">memmap_valid_within</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">,</span>
					<span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">page</span><span class="p">,</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">memmap_valid_within</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">pfn</span><span class="p">,</span>
					<span class="k">struct</span> <span class="n">page</span> <span class="o">*</span><span class="n">page</span><span class="p">,</span> <span class="k">struct</span> <span class="n">zone</span> <span class="o">*</span><span class="n">zone</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_ARCH_HAS_HOLES_MEMORYMODEL */</span><span class="cp"></span>

<span class="cp">#endif </span><span class="cm">/* !__GENERATING_BOUNDS.H */</span><span class="cp"></span>
<span class="cp">#endif </span><span class="cm">/* !__ASSEMBLY__ */</span><span class="cp"></span>
<span class="cp">#endif </span><span class="cm">/* _LINUX_MMZONE_H */</span><span class="cp"></span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:2}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../../javascript/docco.min.js"></script>
</html>
