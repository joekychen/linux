<!DOCTYPE html>
<html><head><title>joekychen/linux » include › linux › rcupdate.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../../index.html"></a><h1>rcupdate.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cm">/*</span>
<span class="cm"> * Read-Copy Update mechanism for mutual exclusion</span>
<span class="cm"> *</span>
<span class="cm"> * This program is free software; you can redistribute it and/or modify</span>
<span class="cm"> * it under the terms of the GNU General Public License as published by</span>
<span class="cm"> * the Free Software Foundation; either version 2 of the License, or</span>
<span class="cm"> * (at your option) any later version.</span>
<span class="cm"> *</span>
<span class="cm"> * This program is distributed in the hope that it will be useful,</span>
<span class="cm"> * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="cm"> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="cm"> * GNU General Public License for more details.</span>
<span class="cm"> *</span>
<span class="cm"> * You should have received a copy of the GNU General Public License</span>
<span class="cm"> * along with this program; if not, write to the Free Software</span>
<span class="cm"> * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.</span>
<span class="cm"> *</span>
<span class="cm"> * Copyright IBM Corporation, 2001</span>
<span class="cm"> *</span>
<span class="cm"> * Author: Dipankar Sarma &lt;dipankar@in.ibm.com&gt;</span>
<span class="cm"> *</span>
<span class="cm"> * Based on the original work by Paul McKenney &lt;paulmck@us.ibm.com&gt;</span>
<span class="cm"> * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.</span>
<span class="cm"> * Papers:</span>
<span class="cm"> * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf</span>
<span class="cm"> * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)</span>
<span class="cm"> *</span>
<span class="cm"> * For detailed explanation of Read-Copy Update mechanism see -</span>
<span class="cm"> *		http://lse.sourceforge.net/locking/rcupdate.html</span>
<span class="cm"> *</span>
<span class="cm"> */</span>

<span class="cp">#ifndef __LINUX_RCUPDATE_H</span>
<span class="cp">#define __LINUX_RCUPDATE_H</span>

<span class="cp">#include &lt;linux/types.h&gt;</span>
<span class="cp">#include &lt;linux/cache.h&gt;</span>
<span class="cp">#include &lt;linux/spinlock.h&gt;</span>
<span class="cp">#include &lt;linux/threads.h&gt;</span>
<span class="cp">#include &lt;linux/cpumask.h&gt;</span>
<span class="cp">#include &lt;linux/seqlock.h&gt;</span>
<span class="cp">#include &lt;linux/lockdep.h&gt;</span>
<span class="cp">#include &lt;linux/completion.h&gt;</span>
<span class="cp">#include &lt;linux/debugobjects.h&gt;</span>
<span class="cp">#include &lt;linux/bug.h&gt;</span>
<span class="cp">#include &lt;linux/compiler.h&gt;</span>

<span class="cp">#ifdef CONFIG_RCU_TORTURE_TEST</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">rcutorture_runnable</span><span class="p">;</span> <span class="cm">/* for sysctl */</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_TORTURE_TEST */</span><span class="cp"></span>

<span class="cp">#if defined(CONFIG_TREE_RCU) || defined(CONFIG_TREE_PREEMPT_RCU)</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcutorture_record_test_transition</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcutorture_record_progress</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">vernum</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">do_trace_rcu_torture_read</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">rcutorturename</span><span class="p">,</span>
				      <span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rhp</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcutorture_record_test_transition</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcutorture_record_progress</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">vernum</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>
<span class="cp">#ifdef CONFIG_RCU_TRACE</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">do_trace_rcu_torture_read</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">rcutorturename</span><span class="p">,</span>
				      <span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rhp</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="cp">#define do_trace_rcu_torture_read(rcutorturename, rhp) do { } while (0)</span>
<span class="cp">#endif</span>
<span class="cp">#endif</span>

<span class="cp">#define UINT_CMP_GE(a, b)	(UINT_MAX / 2 &gt;= (a) - (b))</span>
<span class="cp">#define UINT_CMP_LT(a, b)	(UINT_MAX / 2 &lt; (a) - (b))</span>
<span class="cp">#define ULONG_CMP_GE(a, b)	(ULONG_MAX / 2 &gt;= (a) - (b))</span>
<span class="cp">#define ULONG_CMP_LT(a, b)	(ULONG_MAX / 2 &lt; (a) - (b))</span>

<span class="cm">/* Exported common interfaces */</span>

<span class="cp">#ifdef CONFIG_PREEMPT_RCU</span>

<span class="cm">/**</span>
<span class="cm"> * call_rcu() - Queue an RCU callback for invocation after a grace period.</span>
<span class="cm"> * @head: structure to be used for queueing the RCU updates.</span>
<span class="cm"> * @func: actual callback function to be invoked after the grace period</span>
<span class="cm"> *</span>
<span class="cm"> * The callback function will be invoked some time after a full grace</span>
<span class="cm"> * period elapses, in other words after all pre-existing RCU read-side</span>
<span class="cm"> * critical sections have completed.  However, the callback function</span>
<span class="cm"> * might well execute concurrently with RCU read-side critical sections</span>
<span class="cm"> * that started after call_rcu() was invoked.  RCU read-side critical</span>
<span class="cm"> * sections are delimited by rcu_read_lock() and rcu_read_unlock(),</span>
<span class="cm"> * and may be nested.</span>
<span class="cm"> */</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">call_rcu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
			      <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">));</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_PREEMPT_RCU */</span><span class="cp"></span>

<span class="cm">/* In classic RCU, call_rcu() is just call_rcu_sched(). */</span>
<span class="cp">#define	call_rcu	call_rcu_sched</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_PREEMPT_RCU */</span><span class="cp"></span>

<span class="cm">/**</span>
<span class="cm"> * call_rcu_bh() - Queue an RCU for invocation after a quicker grace period.</span>
<span class="cm"> * @head: structure to be used for queueing the RCU updates.</span>
<span class="cm"> * @func: actual callback function to be invoked after the grace period</span>
<span class="cm"> *</span>
<span class="cm"> * The callback function will be invoked some time after a full grace</span>
<span class="cm"> * period elapses, in other words after all currently executing RCU</span>
<span class="cm"> * read-side critical sections have completed. call_rcu_bh() assumes</span>
<span class="cm"> * that the read-side critical sections end on completion of a softirq</span>
<span class="cm"> * handler. This means that read-side critical sections in process</span>
<span class="cm"> * context must not be interrupted by softirqs. This interface is to be</span>
<span class="cm"> * used when most of the read-side critical sections are in softirq context.</span>
<span class="cm"> * RCU read-side critical sections are delimited by :</span>
<span class="cm"> *  - rcu_read_lock() and  rcu_read_unlock(), if in interrupt context.</span>
<span class="cm"> *  OR</span>
<span class="cm"> *  - rcu_read_lock_bh() and rcu_read_unlock_bh(), if in process context.</span>
<span class="cm"> *  These may be nested.</span>
<span class="cm"> */</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">call_rcu_bh</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
			<span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">));</span>

<span class="cm">/**</span>
<span class="cm"> * call_rcu_sched() - Queue an RCU for invocation after sched grace period.</span>
<span class="cm"> * @head: structure to be used for queueing the RCU updates.</span>
<span class="cm"> * @func: actual callback function to be invoked after the grace period</span>
<span class="cm"> *</span>
<span class="cm"> * The callback function will be invoked some time after a full grace</span>
<span class="cm"> * period elapses, in other words after all currently executing RCU</span>
<span class="cm"> * read-side critical sections have completed. call_rcu_sched() assumes</span>
<span class="cm"> * that the read-side critical sections end on enabling of preemption</span>
<span class="cm"> * or on voluntary preemption.</span>
<span class="cm"> * RCU read-side critical sections are delimited by :</span>
<span class="cm"> *  - rcu_read_lock_sched() and  rcu_read_unlock_sched(),</span>
<span class="cm"> *  OR</span>
<span class="cm"> *  anything that disables preemption.</span>
<span class="cm"> *  These may be nested.</span>
<span class="cm"> */</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">call_rcu_sched</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
			   <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">));</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">synchronize_sched</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_PREEMPT_RCU</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">__rcu_read_lock</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">__rcu_read_unlock</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="kt">void</span> <span class="n">synchronize_rcu</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Defined as a macro as it is a very low level header included from</span>
<span class="cm"> * areas that don&#39;t even know about current.  This gives the rcu_read_lock()</span>
<span class="cm"> * nesting depth, but makes sense only if CONFIG_PREEMPT_RCU -- in other</span>
<span class="cm"> * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_preempt_depth() (current-&gt;rcu_read_lock_nesting)</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_PREEMPT_RCU */</span><span class="cp"></span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">__rcu_read_lock</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">preempt_disable</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">__rcu_read_unlock</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">preempt_enable</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">synchronize_rcu</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">synchronize_sched</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_preempt_depth</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_PREEMPT_RCU */</span><span class="cp"></span>

<span class="cm">/* Internal to kernel */</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_sched_qs</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_bh_qs</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_preempt_note_context_switch</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_check_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">int</span> <span class="n">user</span><span class="p">);</span>
<span class="k">struct</span> <span class="n">notifier_block</span><span class="p">;</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_idle_enter</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_idle_exit</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_irq_enter</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">rcu_irq_exit</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">exit_rcu</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * RCU_NONIDLE - Indicate idle-loop code that needs RCU readers</span>
<span class="cm"> * @a: Code that RCU needs to pay attention to.</span>
<span class="cm"> *</span>
<span class="cm"> * RCU, RCU-bh, and RCU-sched read-side critical sections are forbidden</span>
<span class="cm"> * in the inner idle loop, that is, between the rcu_idle_enter() and</span>
<span class="cm"> * the rcu_idle_exit() -- RCU will happily ignore any such read-side</span>
<span class="cm"> * critical sections.  However, things like powertop need tracepoints</span>
<span class="cm"> * in the inner idle loop.</span>
<span class="cm"> *</span>
<span class="cm"> * This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())</span>
<span class="cm"> * will tell RCU that it needs to pay attending, invoke its argument</span>
<span class="cm"> * (in this example, a call to the do_something_with_RCU() function),</span>
<span class="cm"> * and then tell RCU to go back to ignoring this CPU.  It is permissible</span>
<span class="cm"> * to nest RCU_NONIDLE() wrappers, but the nesting level is currently</span>
<span class="cm"> * quite limited.  If deeper nesting is required, it will be necessary</span>
<span class="cm"> * to adjust DYNTICK_TASK_NESTING_VALUE accordingly.</span>
<span class="cm"> *</span>
<span class="cm"> * This macro may be used from process-level code only.</span>
<span class="cm"> */</span>
<span class="cp">#define RCU_NONIDLE(a) \</span>
<span class="cp">	do { \</span>
<span class="cp">		rcu_idle_exit(); \</span>
<span class="cp">		do { a; } while (0); \</span>
<span class="cp">		rcu_idle_enter(); \</span>
<span class="cp">	} while (0)</span>

<span class="cm">/*</span>
<span class="cm"> * Infrastructure to implement the synchronize_() primitives in</span>
<span class="cm"> * TREE_RCU and rcu_barrier_() primitives in TINY_RCU.</span>
<span class="cm"> */</span>

<span class="k">typedef</span> <span class="kt">void</span> <span class="n">call_rcu_func_t</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
			     <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">));</span>
<span class="kt">void</span> <span class="n">wait_rcu_gp</span><span class="p">(</span><span class="n">call_rcu_func_t</span> <span class="n">crf</span><span class="p">);</span>

<span class="cp">#if defined(CONFIG_TREE_RCU) || defined(CONFIG_TREE_PREEMPT_RCU)</span>
<span class="cp">#include &lt;linux/rcutree.h&gt;</span>
<span class="cp">#elif defined(CONFIG_TINY_RCU) || defined(CONFIG_TINY_PREEMPT_RCU)</span>
<span class="cp">#include &lt;linux/rcutiny.h&gt;</span>
<span class="cp">#else</span>
<span class="cp">#error &quot;Unknown RCU implementation specified to kernel configuration&quot;</span>
<span class="cp">#endif</span>

<span class="cm">/*</span>
<span class="cm"> * init_rcu_head_on_stack()/destroy_rcu_head_on_stack() are needed for dynamic</span>
<span class="cm"> * initialization and destruction of rcu_head on the stack. rcu_head structures</span>
<span class="cm"> * allocated dynamically in the heap or defined statically don&#39;t need any</span>
<span class="cm"> * initialization.</span>
<span class="cm"> */</span>
<span class="cp">#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_rcu_head_on_stack</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">destroy_rcu_head_on_stack</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">);</span>
<span class="cp">#else </span><span class="cm">/* !CONFIG_DEBUG_OBJECTS_RCU_HEAD */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">init_rcu_head_on_stack</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">destroy_rcu_head_on_stack</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>
<span class="cp">#endif	</span><span class="cm">/* #else !CONFIG_DEBUG_OBJECTS_RCU_HEAD */</span><span class="cp"></span>

<span class="cp">#if defined(CONFIG_HOTPLUG_CPU) &amp;&amp; defined(CONFIG_PROVE_RCU)</span>
<span class="n">bool</span> <span class="n">rcu_lockdep_current_cpu_online</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="cp">#else </span><span class="cm">/* #if defined(CONFIG_HOTPLUG_CPU) &amp;&amp; defined(CONFIG_PROVE_RCU) */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="n">bool</span> <span class="nf">rcu_lockdep_current_cpu_online</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #else #if defined(CONFIG_HOTPLUG_CPU) &amp;&amp; defined(CONFIG_PROVE_RCU) */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_DEBUG_LOCK_ALLOC</span>

<span class="cp">#ifdef CONFIG_PROVE_RCU</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">rcu_is_cpu_idle</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="cp">#else </span><span class="cm">/* !CONFIG_PROVE_RCU */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_is_cpu_idle</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* else !CONFIG_PROVE_RCU */</span><span class="cp"></span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_lock_acquire</span><span class="p">(</span><span class="k">struct</span> <span class="n">lockdep_map</span> <span class="o">*</span><span class="n">map</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">lock_acquire</span><span class="p">(</span><span class="n">map</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">_THIS_IP_</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_lock_release</span><span class="p">(</span><span class="k">struct</span> <span class="n">lockdep_map</span> <span class="o">*</span><span class="n">map</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">lock_release</span><span class="p">(</span><span class="n">map</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">_THIS_IP_</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">lockdep_map</span> <span class="n">rcu_lock_map</span><span class="p">;</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">lockdep_map</span> <span class="n">rcu_bh_lock_map</span><span class="p">;</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">lockdep_map</span> <span class="n">rcu_sched_lock_map</span><span class="p">;</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">debug_lockdep_rcu_enabled</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_read_lock_held() - might we be in RCU read-side critical section?</span>
<span class="cm"> *</span>
<span class="cm"> * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an RCU</span>
<span class="cm"> * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,</span>
<span class="cm"> * this assumes we are in an RCU read-side critical section unless it can</span>
<span class="cm"> * prove otherwise.  This is useful for debug checks in functions that</span>
<span class="cm"> * require that they be called within an RCU read-side critical section.</span>
<span class="cm"> *</span>
<span class="cm"> * Checks debug_lockdep_rcu_enabled() to prevent false positives during boot</span>
<span class="cm"> * and while lockdep is disabled.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that rcu_read_lock() and the matching rcu_read_unlock() must</span>
<span class="cm"> * occur in the same context, for example, it is illegal to invoke</span>
<span class="cm"> * rcu_read_unlock() in process context if the matching rcu_read_lock()</span>
<span class="cm"> * was invoked from within an irq handler.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that rcu_read_lock() is disallowed if the CPU is either idle or</span>
<span class="cm"> * offline from an RCU perspective, so check for those as well.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">debug_lockdep_rcu_enabled</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_is_cpu_idle</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_lockdep_current_cpu_online</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * rcu_read_lock_bh_held() is defined out of line to avoid #include-file</span>
<span class="cm"> * hell.</span>
<span class="cm"> */</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">rcu_read_lock_bh_held</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_read_lock_sched_held() - might we be in RCU-sched read-side critical section?</span>
<span class="cm"> *</span>
<span class="cm"> * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an</span>
<span class="cm"> * RCU-sched read-side critical section.  In absence of</span>
<span class="cm"> * CONFIG_DEBUG_LOCK_ALLOC, this assumes we are in an RCU-sched read-side</span>
<span class="cm"> * critical section unless it can prove otherwise.  Note that disabling</span>
<span class="cm"> * of preemption (including disabling irqs) counts as an RCU-sched</span>
<span class="cm"> * read-side critical section.  This is useful for debug checks in functions</span>
<span class="cm"> * that required that they be called within an RCU-sched read-side</span>
<span class="cm"> * critical section.</span>
<span class="cm"> *</span>
<span class="cm"> * Check debug_lockdep_rcu_enabled() to prevent false positives during boot</span>
<span class="cm"> * and while lockdep is disabled.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that if the CPU is in the idle loop from an RCU point of</span>
<span class="cm"> * view (ie: that we are in the section between rcu_idle_enter() and</span>
<span class="cm"> * rcu_idle_exit()) then rcu_read_lock_held() returns false even if the CPU</span>
<span class="cm"> * did an rcu_read_lock().  The reason for this is that RCU ignores CPUs</span>
<span class="cm"> * that are in such a section, considering these as in extended quiescent</span>
<span class="cm"> * state, so such a CPU is effectively never in an RCU read-side critical</span>
<span class="cm"> * section regardless of what RCU primitives it invokes.  This state of</span>
<span class="cm"> * affairs is required --- we need to keep an RCU-free window in idle</span>
<span class="cm"> * where the CPU may possibly enter into low power mode. This way we can</span>
<span class="cm"> * notice an extended quiescent state to other CPUs that started a grace</span>
<span class="cm"> * period. Otherwise we would delay any grace period as long as we run in</span>
<span class="cm"> * the idle task.</span>
<span class="cm"> *</span>
<span class="cm"> * Similarly, we avoid claiming an SRCU read lock held if the current</span>
<span class="cm"> * CPU is offline.</span>
<span class="cm"> */</span>
<span class="cp">#ifdef CONFIG_PREEMPT_COUNT</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_sched_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">lockdep_opinion</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">debug_lockdep_rcu_enabled</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_is_cpu_idle</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_lockdep_current_cpu_online</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">debug_locks</span><span class="p">)</span>
		<span class="n">lockdep_opinion</span> <span class="o">=</span> <span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">lockdep_opinion</span> <span class="o">||</span> <span class="n">preempt_count</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">irqs_disabled</span><span class="p">();</span>
<span class="p">}</span>
<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_PREEMPT_COUNT */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_sched_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_PREEMPT_COUNT */</span><span class="cp"></span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_DEBUG_LOCK_ALLOC */</span><span class="cp"></span>

<span class="cp"># define rcu_lock_acquire(a)		do { } while (0)</span>
<span class="cp"># define rcu_lock_release(a)		do { } while (0)</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_bh_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_PREEMPT_COUNT</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_sched_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">preempt_count</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">irqs_disabled</span><span class="p">();</span>
<span class="p">}</span>
<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_PREEMPT_COUNT */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_read_lock_sched_held</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_PREEMPT_COUNT */</span><span class="cp"></span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_PROVE_RCU</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">rcu_my_thread_group_empty</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_lockdep_assert - emit lockdep splat if specified condition not met</span>
<span class="cm"> * @c: condition to check</span>
<span class="cm"> * @s: informative message</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_lockdep_assert(c, s)					\</span>
<span class="cp">	do {								\</span>
<span class="cp">		static bool __section(.data.unlikely) __warned;		\</span>
<span class="cp">		if (debug_lockdep_rcu_enabled() &amp;&amp; !__warned &amp;&amp; !(c)) {	\</span>
<span class="cp">			__warned = true;				\</span>
<span class="cp">			lockdep_rcu_suspicious(__FILE__, __LINE__, s);	\</span>
<span class="cp">		}							\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#if defined(CONFIG_PROVE_RCU) &amp;&amp; !defined(CONFIG_PREEMPT_RCU)</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_preempt_sleep_check</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal context switch in RCU read-side &quot;</span>
			   <span class="s">&quot;critical section&quot;</span><span class="p">);</span>
<span class="p">}</span>
<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_PROVE_RCU */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_preempt_sleep_check</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_PROVE_RCU */</span><span class="cp"></span>

<span class="cp">#define rcu_sleep_check()						\</span>
<span class="cp">	do {								\</span>
<span class="cp">		rcu_preempt_sleep_check();				\</span>
<span class="cp">		rcu_lockdep_assert(!lock_is_held(&amp;rcu_bh_lock_map),	\</span>
<span class="cp">				   &quot;Illegal context switch in RCU-bh&quot;	\</span>
<span class="cp">				   &quot; read-side critical section&quot;);	\</span>
<span class="cp">		rcu_lockdep_assert(!lock_is_held(&amp;rcu_sched_lock_map),	\</span>
<span class="cp">				   &quot;Illegal context switch in RCU-sched&quot;\</span>
<span class="cp">				   &quot; read-side critical section&quot;);	\</span>
<span class="cp">	} while (0)</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_PROVE_RCU */</span><span class="cp"></span>

<span class="cp">#define rcu_lockdep_assert(c, s) do { } while (0)</span>
<span class="cp">#define rcu_sleep_check() do { } while (0)</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_PROVE_RCU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Helper functions for rcu_dereference_check(), rcu_dereference_protected()</span>
<span class="cm"> * and rcu_assign_pointer().  Some of these could be folded into their</span>
<span class="cm"> * callers, but they are left separate in order to ease introduction of</span>
<span class="cm"> * multiple flavors of pointers to match the multiple flavors of RCU</span>
<span class="cm"> * (e.g., __rcu_bh, * __rcu_sched, and __srcu), should this make sense in</span>
<span class="cm"> * the future.</span>
<span class="cm"> */</span>

<span class="cp">#ifdef __CHECKER__</span>
<span class="cp">#define rcu_dereference_sparse(p, space) \</span>
<span class="cp">	((void)(((typeof(*p) space *)p) == p))</span>
<span class="cp">#else </span><span class="cm">/* #ifdef __CHECKER__ */</span><span class="cp"></span>
<span class="cp">#define rcu_dereference_sparse(p, space)</span>
<span class="cp">#endif </span><span class="cm">/* #else #ifdef __CHECKER__ */</span><span class="cp"></span>

<span class="cp">#define __rcu_access_pointer(p, space) \</span>
<span class="cp">	({ \</span>
<span class="cp">		typeof(*p) *_________p1 = (typeof(*p)*__force )ACCESS_ONCE(p); \</span>
<span class="cp">		rcu_dereference_sparse(p, space); \</span>
<span class="cp">		((typeof(*p) __force __kernel *)(_________p1)); \</span>
<span class="cp">	})</span>
<span class="cp">#define __rcu_dereference_check(p, c, space) \</span>
<span class="cp">	({ \</span>
<span class="cp">		typeof(*p) *_________p1 = (typeof(*p)*__force )ACCESS_ONCE(p); \</span>
<span class="cp">		rcu_lockdep_assert(c, &quot;suspicious rcu_dereference_check()&quot; \</span>
<span class="cp">				      &quot; usage&quot;); \</span>
<span class="cp">		rcu_dereference_sparse(p, space); \</span>
<span class="cp">		smp_read_barrier_depends(); \</span>
<span class="cp">		((typeof(*p) __force __kernel *)(_________p1)); \</span>
<span class="cp">	})</span>
<span class="cp">#define __rcu_dereference_protected(p, c, space) \</span>
<span class="cp">	({ \</span>
<span class="cp">		rcu_lockdep_assert(c, &quot;suspicious rcu_dereference_protected()&quot; \</span>
<span class="cp">				      &quot; usage&quot;); \</span>
<span class="cp">		rcu_dereference_sparse(p, space); \</span>
<span class="cp">		((typeof(*p) __force __kernel *)(p)); \</span>
<span class="cp">	})</span>

<span class="cp">#define __rcu_access_index(p, space) \</span>
<span class="cp">	({ \</span>
<span class="cp">		typeof(p) _________p1 = ACCESS_ONCE(p); \</span>
<span class="cp">		rcu_dereference_sparse(p, space); \</span>
<span class="cp">		(_________p1); \</span>
<span class="cp">	})</span>
<span class="cp">#define __rcu_dereference_index_check(p, c) \</span>
<span class="cp">	({ \</span>
<span class="cp">		typeof(p) _________p1 = ACCESS_ONCE(p); \</span>
<span class="cp">		rcu_lockdep_assert(c, \</span>
<span class="cp">				   &quot;suspicious rcu_dereference_index_check()&quot; \</span>
<span class="cp">				   &quot; usage&quot;); \</span>
<span class="cp">		smp_read_barrier_depends(); \</span>
<span class="cp">		(_________p1); \</span>
<span class="cp">	})</span>
<span class="cp">#define __rcu_assign_pointer(p, v, space) \</span>
<span class="cp">	({ \</span>
<span class="cp">		smp_wmb(); \</span>
<span class="cp">		(p) = (typeof(*v) __force space *)(v); \</span>
<span class="cp">	})</span>


<span class="cm">/**</span>
<span class="cm"> * rcu_access_pointer() - fetch RCU pointer with no dereferencing</span>
<span class="cm"> * @p: The pointer to read</span>
<span class="cm"> *</span>
<span class="cm"> * Return the value of the specified RCU-protected pointer, but omit the</span>
<span class="cm"> * smp_read_barrier_depends() and keep the ACCESS_ONCE().  This is useful</span>
<span class="cm"> * when the value of this pointer is accessed, but the pointer is not</span>
<span class="cm"> * dereferenced, for example, when testing an RCU-protected pointer against</span>
<span class="cm"> * NULL.  Although rcu_access_pointer() may also be used in cases where</span>
<span class="cm"> * update-side locks prevent the value of the pointer from changing, you</span>
<span class="cm"> * should instead use rcu_dereference_protected() for this use case.</span>
<span class="cm"> *</span>
<span class="cm"> * It is also permissible to use rcu_access_pointer() when read-side</span>
<span class="cm"> * access to the pointer was removed at least one grace period ago, as</span>
<span class="cm"> * is the case in the context of the RCU callback that is freeing up</span>
<span class="cm"> * the data, or after a synchronize_rcu() returns.  This can be useful</span>
<span class="cm"> * when tearing down multi-linked structures after a grace period</span>
<span class="cm"> * has elapsed.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_access_pointer(p) __rcu_access_pointer((p), __rcu)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_check() - rcu_dereference with debug checking</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> * @c: The conditions under which the dereference will take place</span>
<span class="cm"> *</span>
<span class="cm"> * Do an rcu_dereference(), but check that the conditions under which the</span>
<span class="cm"> * dereference will take place are correct.  Typically the conditions</span>
<span class="cm"> * indicate the various locking conditions that should be held at that</span>
<span class="cm"> * point.  The check should return true if the conditions are satisfied.</span>
<span class="cm"> * An implicit check for being in an RCU read-side critical section</span>
<span class="cm"> * (rcu_read_lock()) is included.</span>
<span class="cm"> *</span>
<span class="cm"> * For example:</span>
<span class="cm"> *</span>
<span class="cm"> *	bar = rcu_dereference_check(foo-&gt;bar, lockdep_is_held(&amp;foo-&gt;lock));</span>
<span class="cm"> *</span>
<span class="cm"> * could be used to indicate to lockdep that foo-&gt;bar may only be dereferenced</span>
<span class="cm"> * if either rcu_read_lock() is held, or that the lock required to replace</span>
<span class="cm"> * the bar struct at foo-&gt;bar is held.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that the list of conditions may also include indications of when a lock</span>
<span class="cm"> * need not be held, for example during initialisation or destruction of the</span>
<span class="cm"> * target struct:</span>
<span class="cm"> *</span>
<span class="cm"> *	bar = rcu_dereference_check(foo-&gt;bar, lockdep_is_held(&amp;foo-&gt;lock) ||</span>
<span class="cm"> *					      atomic_read(&amp;foo-&gt;usage) == 0);</span>
<span class="cm"> *</span>
<span class="cm"> * Inserts memory barriers on architectures that require them</span>
<span class="cm"> * (currently only the Alpha), prevents the compiler from refetching</span>
<span class="cm"> * (and from merging fetches), and, more importantly, documents exactly</span>
<span class="cm"> * which pointers are protected by RCU and checks that the pointer is</span>
<span class="cm"> * annotated as __rcu.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_check(p, c) \</span>
<span class="cp">	__rcu_dereference_check((p), rcu_read_lock_held() || (c), __rcu)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> * @c: The conditions under which the dereference will take place</span>
<span class="cm"> *</span>
<span class="cm"> * This is the RCU-bh counterpart to rcu_dereference_check().</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_bh_check(p, c) \</span>
<span class="cp">	__rcu_dereference_check((p), rcu_read_lock_bh_held() || (c), __rcu)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> * @c: The conditions under which the dereference will take place</span>
<span class="cm"> *</span>
<span class="cm"> * This is the RCU-sched counterpart to rcu_dereference_check().</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_sched_check(p, c) \</span>
<span class="cp">	__rcu_dereference_check((p), rcu_read_lock_sched_held() || (c), \</span>
<span class="cp">				__rcu)</span>

<span class="cp">#define rcu_dereference_raw(p) rcu_dereference_check(p, 1) </span><span class="cm">/*@@@ needed? @@@*/</span><span class="cp"></span>

<span class="cm">/**</span>
<span class="cm"> * rcu_access_index() - fetch RCU index with no dereferencing</span>
<span class="cm"> * @p: The index to read</span>
<span class="cm"> *</span>
<span class="cm"> * Return the value of the specified RCU-protected index, but omit the</span>
<span class="cm"> * smp_read_barrier_depends() and keep the ACCESS_ONCE().  This is useful</span>
<span class="cm"> * when the value of this index is accessed, but the index is not</span>
<span class="cm"> * dereferenced, for example, when testing an RCU-protected index against</span>
<span class="cm"> * -1.  Although rcu_access_index() may also be used in cases where</span>
<span class="cm"> * update-side locks prevent the value of the index from changing, you</span>
<span class="cm"> * should instead use rcu_dereference_index_protected() for this use case.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_access_index(p) __rcu_access_index((p), __rcu)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_index_check() - rcu_dereference for indices with debug checking</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> * @c: The conditions under which the dereference will take place</span>
<span class="cm"> *</span>
<span class="cm"> * Similar to rcu_dereference_check(), but omits the sparse checking.</span>
<span class="cm"> * This allows rcu_dereference_index_check() to be used on integers,</span>
<span class="cm"> * which can then be used as array indices.  Attempting to use</span>
<span class="cm"> * rcu_dereference_check() on an integer will give compiler warnings</span>
<span class="cm"> * because the sparse address-space mechanism relies on dereferencing</span>
<span class="cm"> * the RCU-protected pointer.  Dereferencing integers is not something</span>
<span class="cm"> * that even gcc will put up with.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that this function does not implicitly check for RCU read-side</span>
<span class="cm"> * critical sections.  If this function gains lots of uses, it might</span>
<span class="cm"> * make sense to provide versions for each flavor of RCU, but it does</span>
<span class="cm"> * not make sense as of early 2010.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_index_check(p, c) \</span>
<span class="cp">	__rcu_dereference_index_check((p), (c))</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_protected() - fetch RCU pointer when updates prevented</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> * @c: The conditions under which the dereference will take place</span>
<span class="cm"> *</span>
<span class="cm"> * Return the value of the specified RCU-protected pointer, but omit</span>
<span class="cm"> * both the smp_read_barrier_depends() and the ACCESS_ONCE().  This</span>
<span class="cm"> * is useful in cases where update-side locks prevent the value of the</span>
<span class="cm"> * pointer from changing.  Please note that this primitive does -not-</span>
<span class="cm"> * prevent the compiler from repeating this reference or combining it</span>
<span class="cm"> * with other references, so it should not be used without protection</span>
<span class="cm"> * of appropriate locks.</span>
<span class="cm"> *</span>
<span class="cm"> * This function is only for update-side use.  Using this function</span>
<span class="cm"> * when protected only by rcu_read_lock() will result in infrequent</span>
<span class="cm"> * but very ugly failures.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_protected(p, c) \</span>
<span class="cp">	__rcu_dereference_protected((p), (c), __rcu)</span>


<span class="cm">/**</span>
<span class="cm"> * rcu_dereference() - fetch RCU-protected pointer for dereferencing</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> *</span>
<span class="cm"> * This is a simple wrapper around rcu_dereference_check().</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference(p) rcu_dereference_check(p, 0)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> *</span>
<span class="cm"> * Makes rcu_dereference_check() do the dirty work.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_bh(p) rcu_dereference_bh_check(p, 0)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing</span>
<span class="cm"> * @p: The pointer to read, prior to dereferencing</span>
<span class="cm"> *</span>
<span class="cm"> * Makes rcu_dereference_check() do the dirty work.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_dereference_sched(p) rcu_dereference_sched_check(p, 0)</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_read_lock() - mark the beginning of an RCU read-side critical section</span>
<span class="cm"> *</span>
<span class="cm"> * When synchronize_rcu() is invoked on one CPU while other CPUs</span>
<span class="cm"> * are within RCU read-side critical sections, then the</span>
<span class="cm"> * synchronize_rcu() is guaranteed to block until after all the other</span>
<span class="cm"> * CPUs exit their critical sections.  Similarly, if call_rcu() is invoked</span>
<span class="cm"> * on one CPU while other CPUs are within RCU read-side critical</span>
<span class="cm"> * sections, invocation of the corresponding RCU callback is deferred</span>
<span class="cm"> * until after the all the other CPUs exit their critical sections.</span>
<span class="cm"> *</span>
<span class="cm"> * Note, however, that RCU callbacks are permitted to run concurrently</span>
<span class="cm"> * with new RCU read-side critical sections.  One way that this can happen</span>
<span class="cm"> * is via the following sequence of events: (1) CPU 0 enters an RCU</span>
<span class="cm"> * read-side critical section, (2) CPU 1 invokes call_rcu() to register</span>
<span class="cm"> * an RCU callback, (3) CPU 0 exits the RCU read-side critical section,</span>
<span class="cm"> * (4) CPU 2 enters a RCU read-side critical section, (5) the RCU</span>
<span class="cm"> * callback is invoked.  This is legal, because the RCU read-side critical</span>
<span class="cm"> * section that was running concurrently with the call_rcu() (and which</span>
<span class="cm"> * therefore might be referencing something that the corresponding RCU</span>
<span class="cm"> * callback would free up) has completed before the corresponding</span>
<span class="cm"> * RCU callback is invoked.</span>
<span class="cm"> *</span>
<span class="cm"> * RCU read-side critical sections may be nested.  Any deferred actions</span>
<span class="cm"> * will be deferred until the outermost RCU read-side critical section</span>
<span class="cm"> * completes.</span>
<span class="cm"> *</span>
<span class="cm"> * You can avoid reading and understanding the next paragraph by</span>
<span class="cm"> * following this rule: don&#39;t put anything in an rcu_read_lock() RCU</span>
<span class="cm"> * read-side critical section that would block in a !PREEMPT kernel.</span>
<span class="cm"> * But if you want the full story, read on!</span>
<span class="cm"> *</span>
<span class="cm"> * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU), it</span>
<span class="cm"> * is illegal to block while in an RCU read-side critical section.  In</span>
<span class="cm"> * preemptible RCU implementations (TREE_PREEMPT_RCU and TINY_PREEMPT_RCU)</span>
<span class="cm"> * in CONFIG_PREEMPT kernel builds, RCU read-side critical sections may</span>
<span class="cm"> * be preempted, but explicit blocking is illegal.  Finally, in preemptible</span>
<span class="cm"> * RCU implementations in real-time (CONFIG_PREEMPT_RT) kernel builds,</span>
<span class="cm"> * RCU read-side critical sections may be preempted and they may also</span>
<span class="cm"> * block, but only when acquiring spinlocks that are subject to priority</span>
<span class="cm"> * inheritance.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_read_lock</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">__rcu_read_lock</span><span class="p">();</span>
	<span class="n">__acquire</span><span class="p">(</span><span class="n">RCU</span><span class="p">);</span>
	<span class="n">rcu_lock_acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">);</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_is_cpu_idle</span><span class="p">(),</span>
			   <span class="s">&quot;rcu_read_lock() used illegally while idle&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * So where is rcu_write_lock()?  It does not exist, as there is no</span>
<span class="cm"> * way for writers to lock out RCU readers.  This is a feature, not</span>
<span class="cm"> * a bug -- this property is what provides RCU&#39;s performance benefits.</span>
<span class="cm"> * Of course, writers must coordinate with each other.  The normal</span>
<span class="cm"> * spinlock primitives work well for this, but any other technique may be</span>
<span class="cm"> * used as well.  RCU does not care how the writers keep out of each</span>
<span class="cm"> * others&#39; way, as long as they do so.</span>
<span class="cm"> */</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_read_unlock() - marks the end of an RCU read-side critical section.</span>
<span class="cm"> *</span>
<span class="cm"> * See rcu_read_lock() for more information.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_read_unlock</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_is_cpu_idle</span><span class="p">(),</span>
			   <span class="s">&quot;rcu_read_unlock() used illegally while idle&quot;</span><span class="p">);</span>
	<span class="n">rcu_lock_release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">);</span>
	<span class="n">__release</span><span class="p">(</span><span class="n">RCU</span><span class="p">);</span>
	<span class="n">__rcu_read_unlock</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_read_lock_bh() - mark the beginning of an RCU-bh critical section</span>
<span class="cm"> *</span>
<span class="cm"> * This is equivalent of rcu_read_lock(), but to be used when updates</span>
<span class="cm"> * are being done using call_rcu_bh() or synchronize_rcu_bh(). Since</span>
<span class="cm"> * both call_rcu_bh() and synchronize_rcu_bh() consider completion of a</span>
<span class="cm"> * softirq handler to be a quiescent state, a process in RCU read-side</span>
<span class="cm"> * critical section must be protected by disabling softirqs. Read-side</span>
<span class="cm"> * critical sections in interrupt context can use just rcu_read_lock(),</span>
<span class="cm"> * though this should at least be commented to avoid confusing people</span>
<span class="cm"> * reading the code.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()</span>
<span class="cm"> * must occur in the same context, for example, it is illegal to invoke</span>
<span class="cm"> * rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()</span>
<span class="cm"> * was invoked from some other task.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_read_lock_bh</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">local_bh_disable</span><span class="p">();</span>
	<span class="n">__acquire</span><span class="p">(</span><span class="n">RCU_BH</span><span class="p">);</span>
	<span class="n">rcu_lock_acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_lock_map</span><span class="p">);</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_is_cpu_idle</span><span class="p">(),</span>
			   <span class="s">&quot;rcu_read_lock_bh() used illegally while idle&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * rcu_read_unlock_bh - marks the end of a softirq-only RCU critical section</span>
<span class="cm"> *</span>
<span class="cm"> * See rcu_read_lock_bh() for more information.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_read_unlock_bh</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_is_cpu_idle</span><span class="p">(),</span>
			   <span class="s">&quot;rcu_read_unlock_bh() used illegally while idle&quot;</span><span class="p">);</span>
	<span class="n">rcu_lock_release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_lock_map</span><span class="p">);</span>
	<span class="n">__release</span><span class="p">(</span><span class="n">RCU_BH</span><span class="p">);</span>
	<span class="n">local_bh_enable</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_read_lock_sched() - mark the beginning of a RCU-sched critical section</span>
<span class="cm"> *</span>
<span class="cm"> * This is equivalent of rcu_read_lock(), but to be used when updates</span>
<span class="cm"> * are being done using call_rcu_sched() or synchronize_rcu_sched().</span>
<span class="cm"> * Read-side critical sections can also be introduced by anything that</span>
<span class="cm"> * disables preemption, including local_irq_disable() and friends.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()</span>
<span class="cm"> * must occur in the same context, for example, it is illegal to invoke</span>
<span class="cm"> * rcu_read_unlock_sched() from process context if the matching</span>
<span class="cm"> * rcu_read_lock_sched() was invoked from an NMI handler.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_read_lock_sched</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">preempt_disable</span><span class="p">();</span>
	<span class="n">__acquire</span><span class="p">(</span><span class="n">RCU_SCHED</span><span class="p">);</span>
	<span class="n">rcu_lock_acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">);</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_is_cpu_idle</span><span class="p">(),</span>
			   <span class="s">&quot;rcu_read_lock_sched() used illegally while idle&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="n">notrace</span> <span class="kt">void</span> <span class="nf">rcu_read_lock_sched_notrace</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">preempt_disable_notrace</span><span class="p">();</span>
	<span class="n">__acquire</span><span class="p">(</span><span class="n">RCU_SCHED</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * rcu_read_unlock_sched - marks the end of a RCU-classic critical section</span>
<span class="cm"> *</span>
<span class="cm"> * See rcu_read_lock_sched for more information.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">rcu_read_unlock_sched</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_is_cpu_idle</span><span class="p">(),</span>
			   <span class="s">&quot;rcu_read_unlock_sched() used illegally while idle&quot;</span><span class="p">);</span>
	<span class="n">rcu_lock_release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">);</span>
	<span class="n">__release</span><span class="p">(</span><span class="n">RCU_SCHED</span><span class="p">);</span>
	<span class="n">preempt_enable</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="n">notrace</span> <span class="kt">void</span> <span class="nf">rcu_read_unlock_sched_notrace</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">__release</span><span class="p">(</span><span class="n">RCU_SCHED</span><span class="p">);</span>
	<span class="n">preempt_enable_notrace</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_assign_pointer() - assign to RCU-protected pointer</span>
<span class="cm"> * @p: pointer to assign to</span>
<span class="cm"> * @v: value to assign (publish)</span>
<span class="cm"> *</span>
<span class="cm"> * Assigns the specified value to the specified RCU-protected</span>
<span class="cm"> * pointer, ensuring that any concurrent RCU readers will see</span>
<span class="cm"> * any prior initialization.  Returns the value assigned.</span>
<span class="cm"> *</span>
<span class="cm"> * Inserts memory barriers on architectures that require them</span>
<span class="cm"> * (which is most of them), and also prevents the compiler from</span>
<span class="cm"> * reordering the code that initializes the structure after the pointer</span>
<span class="cm"> * assignment.  More importantly, this call documents which pointers</span>
<span class="cm"> * will be dereferenced by RCU read-side code.</span>
<span class="cm"> *</span>
<span class="cm"> * In some special cases, you may use RCU_INIT_POINTER() instead</span>
<span class="cm"> * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due</span>
<span class="cm"> * to the fact that it does not constrain either the CPU or the compiler.</span>
<span class="cm"> * That said, using RCU_INIT_POINTER() when you should have used</span>
<span class="cm"> * rcu_assign_pointer() is a very bad thing that results in</span>
<span class="cm"> * impossible-to-diagnose memory corruption.  So please be careful.</span>
<span class="cm"> * See the RCU_INIT_POINTER() comment header for details.</span>
<span class="cm"> */</span>
<span class="cp">#define rcu_assign_pointer(p, v) \</span>
<span class="cp">	__rcu_assign_pointer((p), (v), __rcu)</span>

<span class="cm">/**</span>
<span class="cm"> * RCU_INIT_POINTER() - initialize an RCU protected pointer</span>
<span class="cm"> *</span>
<span class="cm"> * Initialize an RCU-protected pointer in special cases where readers</span>
<span class="cm"> * do not need ordering constraints on the CPU or the compiler.  These</span>
<span class="cm"> * special cases are:</span>
<span class="cm"> *</span>
<span class="cm"> * 1.	This use of RCU_INIT_POINTER() is NULLing out the pointer -or-</span>
<span class="cm"> * 2.	The caller has taken whatever steps are required to prevent</span>
<span class="cm"> *	RCU readers from concurrently accessing this pointer -or-</span>
<span class="cm"> * 3.	The referenced data structure has already been exposed to</span>
<span class="cm"> *	readers either at compile time or via rcu_assign_pointer() -and-</span>
<span class="cm"> *	a.	You have not made -any- reader-visible changes to</span>
<span class="cm"> *		this structure since then -or-</span>
<span class="cm"> *	b.	It is OK for readers accessing this structure from its</span>
<span class="cm"> *		new location to see the old state of the structure.  (For</span>
<span class="cm"> *		example, the changes were to statistical counters or to</span>
<span class="cm"> *		other state where exact synchronization is not required.)</span>
<span class="cm"> *</span>
<span class="cm"> * Failure to follow these rules governing use of RCU_INIT_POINTER() will</span>
<span class="cm"> * result in impossible-to-diagnose memory corruption.  As in the structures</span>
<span class="cm"> * will look OK in crash dumps, but any concurrent RCU readers might</span>
<span class="cm"> * see pre-initialized values of the referenced data structure.  So</span>
<span class="cm"> * please be very careful how you use RCU_INIT_POINTER()!!!</span>
<span class="cm"> *</span>
<span class="cm"> * If you are creating an RCU-protected linked structure that is accessed</span>
<span class="cm"> * by a single external-to-structure RCU-protected pointer, then you may</span>
<span class="cm"> * use RCU_INIT_POINTER() to initialize the internal RCU-protected</span>
<span class="cm"> * pointers, but you must use rcu_assign_pointer() to initialize the</span>
<span class="cm"> * external-to-structure pointer -after- you have completely initialized</span>
<span class="cm"> * the reader-accessible portions of the linked structure.</span>
<span class="cm"> */</span>
<span class="cp">#define RCU_INIT_POINTER(p, v) \</span>
<span class="cp">		p = (typeof(*v) __force __rcu *)(v)</span>

<span class="k">static</span> <span class="n">__always_inline</span> <span class="n">bool</span> <span class="nf">__is_kfree_rcu_offset</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">offset</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="mi">4096</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="n">__always_inline</span>
<span class="kt">void</span> <span class="nf">__kfree_rcu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">offset</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">typedef</span> <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">rcu_callback</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="p">);</span>

	<span class="n">BUILD_BUG_ON</span><span class="p">(</span><span class="o">!</span><span class="n">__builtin_constant_p</span><span class="p">(</span><span class="n">offset</span><span class="p">));</span>

	<span class="cm">/* See the kfree_rcu() header comment. */</span>
	<span class="n">BUILD_BUG_ON</span><span class="p">(</span><span class="o">!</span><span class="n">__is_kfree_rcu_offset</span><span class="p">(</span><span class="n">offset</span><span class="p">));</span>

	<span class="n">kfree_call_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="p">(</span><span class="n">rcu_callback</span><span class="p">)</span><span class="n">offset</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Does the specified offset indicate that the corresponding rcu_head</span>
<span class="cm"> * structure can be handled by kfree_rcu()?</span>
<span class="cm"> */</span>
<span class="cp">#define __is_kfree_rcu_offset(offset) ((offset) &lt; 4096)</span>

<span class="cm">/*</span>
<span class="cm"> * Helper macro for kfree_rcu() to prevent argument-expansion eyestrain.</span>
<span class="cm"> */</span>
<span class="cp">#define __kfree_rcu(head, offset) \</span>
<span class="cp">	do { \</span>
<span class="cp">		BUILD_BUG_ON(!__is_kfree_rcu_offset(offset)); \</span>
<span class="cp">		call_rcu(head, (void (*)(struct rcu_head *))(unsigned long)(offset)); \</span>
<span class="cp">	} while (0)</span>

<span class="cm">/**</span>
<span class="cm"> * kfree_rcu() - kfree an object after a grace period.</span>
<span class="cm"> * @ptr:	pointer to kfree</span>
<span class="cm"> * @rcu_head:	the name of the struct rcu_head within the type of @ptr.</span>
<span class="cm"> *</span>
<span class="cm"> * Many rcu callbacks functions just call kfree() on the base structure.</span>
<span class="cm"> * These functions are trivial, but their size adds up, and furthermore</span>
<span class="cm"> * when they are used in a kernel module, that module must invoke the</span>
<span class="cm"> * high-latency rcu_barrier() function at module-unload time.</span>
<span class="cm"> *</span>
<span class="cm"> * The kfree_rcu() function handles this issue.  Rather than encoding a</span>
<span class="cm"> * function address in the embedded rcu_head structure, kfree_rcu() instead</span>
<span class="cm"> * encodes the offset of the rcu_head structure within the base structure.</span>
<span class="cm"> * Because the functions are not allowed in the low-order 4096 bytes of</span>
<span class="cm"> * kernel virtual memory, offsets up to 4095 bytes can be accommodated.</span>
<span class="cm"> * If the offset is larger than 4095 bytes, a compile-time error will</span>
<span class="cm"> * be generated in __kfree_rcu().  If this error is triggered, you can</span>
<span class="cm"> * either fall back to use of call_rcu() or rearrange the structure to</span>
<span class="cm"> * position the rcu_head structure into the first 4096 bytes.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that the allowable offset might decrease in the future, for example,</span>
<span class="cm"> * to allow something like kmem_cache_free_rcu().</span>
<span class="cm"> *</span>
<span class="cm"> * The BUILD_BUG_ON check must not involve any function calls, hence the</span>
<span class="cm"> * checks are done in macros here.</span>
<span class="cm"> */</span>
<span class="cp">#define kfree_rcu(ptr, rcu_head)					\</span>
<span class="cp">	__kfree_rcu(&amp;((ptr)-&gt;rcu_head), offsetof(typeof(*(ptr)), rcu_head))</span>

<span class="cp">#endif </span><span class="cm">/* __LINUX_RCUPDATE_H */</span><span class="cp"></span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:2}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../../javascript/docco.min.js"></script>
</html>
