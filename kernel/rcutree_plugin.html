<!DOCTYPE html>
<html><head><title>joekychen/linux » kernel › rcutree_plugin.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../index.html"></a><h1>rcutree_plugin.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cm">/*</span>
<span class="cm"> * Read-Copy Update mechanism for mutual exclusion (tree-based version)</span>
<span class="cm"> * Internal non-public definitions that provide either classic</span>
<span class="cm"> * or preemptible semantics.</span>
<span class="cm"> *</span>
<span class="cm"> * This program is free software; you can redistribute it and/or modify</span>
<span class="cm"> * it under the terms of the GNU General Public License as published by</span>
<span class="cm"> * the Free Software Foundation; either version 2 of the License, or</span>
<span class="cm"> * (at your option) any later version.</span>
<span class="cm"> *</span>
<span class="cm"> * This program is distributed in the hope that it will be useful,</span>
<span class="cm"> * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="cm"> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="cm"> * GNU General Public License for more details.</span>
<span class="cm"> *</span>
<span class="cm"> * You should have received a copy of the GNU General Public License</span>
<span class="cm"> * along with this program; if not, write to the Free Software</span>
<span class="cm"> * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.</span>
<span class="cm"> *</span>
<span class="cm"> * Copyright Red Hat, 2009</span>
<span class="cm"> * Copyright IBM Corporation, 2009</span>
<span class="cm"> *</span>
<span class="cm"> * Author: Ingo Molnar &lt;mingo@elte.hu&gt;</span>
<span class="cm"> *	   Paul E. McKenney &lt;paulmck@linux.vnet.ibm.com&gt;</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;linux/delay.h&gt;</span>

<span class="cp">#define RCU_KTHREAD_PRIO 1</span>

<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
<span class="cp">#define RCU_BOOST_PRIO CONFIG_RCU_BOOST_PRIO</span>
<span class="cp">#else</span>
<span class="cp">#define RCU_BOOST_PRIO RCU_KTHREAD_PRIO</span>
<span class="cp">#endif</span>

<span class="cm">/*</span>
<span class="cm"> * Check the RCU kernel configuration parameters and print informative</span>
<span class="cm"> * messages about anything out of the ordinary.  If you like #ifdef, you</span>
<span class="cm"> * will love this function.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_bootup_announce_oddness</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_RCU_TRACE</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">RCU debugfs-based tracing is enabled.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#if (defined(CONFIG_64BIT) &amp;&amp; CONFIG_RCU_FANOUT != 64) || (!defined(CONFIG_64BIT) &amp;&amp; CONFIG_RCU_FANOUT != 32)</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">CONFIG_RCU_FANOUT set to non-default value of %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
	       <span class="n">CONFIG_RCU_FANOUT</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_RCU_FANOUT_EXACT</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">Hierarchical RCU autobalancing is disabled.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_RCU_FAST_NO_HZ</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span>
	       <span class="s">&quot;</span><span class="se">\t</span><span class="s">RCU dyntick-idle grace-period acceleration is enabled.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_PROVE_RCU</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">RCU lockdep checking is enabled.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_RCU_TORTURE_TEST_RUNNABLE</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">RCU torture testing starts during boot.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#if defined(CONFIG_TREE_PREEMPT_RCU) &amp;&amp; !defined(CONFIG_RCU_CPU_STALL_VERBOSE)</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">Dump stacks of tasks blocking RCU-preempt GP.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#if defined(CONFIG_RCU_CPU_STALL_INFO)</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">Additional per-CPU info printed with stalls.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="cp">#if NUM_RCU_LVL_4 != 0</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">Experimental four-level hierarchy is enabled.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_TREE_PREEMPT_RCU</span>

<span class="k">struct</span> <span class="n">rcu_state</span> <span class="n">rcu_preempt_state</span> <span class="o">=</span> <span class="n">RCU_STATE_INITIALIZER</span><span class="p">(</span><span class="n">rcu_preempt</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span><span class="p">,</span> <span class="n">rcu_preempt_data</span><span class="p">);</span>
<span class="k">static</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rcu_state</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">;</span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">rcu_read_unlock_special</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">);</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">rcu_preempted_readers_exp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Tell them what RCU they are running.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_bootup_announce</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;Preemptible hierarchical RCU implementation.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
	<span class="n">rcu_bootup_announce_oddness</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Return the number of RCU-preempt batches processed thus far</span>
<span class="cm"> * for debug and statistics.</span>
<span class="cm"> */</span>
<span class="kt">long</span> <span class="nf">rcu_batches_completed_preempt</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rcu_preempt_state</span><span class="p">.</span><span class="n">completed</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_batches_completed_preempt</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Return the number of RCU batches processed thus far for debug &amp; stats.</span>
<span class="cm"> */</span>
<span class="kt">long</span> <span class="nf">rcu_batches_completed</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rcu_batches_completed_preempt</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_batches_completed</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Force a quiescent state for preemptible RCU.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_force_quiescent_state</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">force_quiescent_state</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_force_quiescent_state</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Record a preemptible-RCU quiescent state for the specified CPU.  Note</span>
<span class="cm"> * that this just means that the task currently running on the CPU is</span>
<span class="cm"> * not in a quiescent state.  There might be any number of tasks blocked</span>
<span class="cm"> * while in an RCU read-side critical section.</span>
<span class="cm"> *</span>
<span class="cm"> * Unlike the other rcu_*_qs() functions, callers to this function</span>
<span class="cm"> * must disable irqs in order to protect the assignment to</span>
<span class="cm"> * -&gt;rcu_read_unlock_special.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_qs</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce_gpnum</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
	<span class="n">barrier</span><span class="p">();</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="s">&quot;rcu_preempt&quot;</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;cpuqs&quot;</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">current</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span> <span class="o">&amp;=</span> <span class="o">~</span><span class="n">RCU_READ_UNLOCK_NEED_QS</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * We have entered the scheduler, and the current task might soon be</span>
<span class="cm"> * context-switched away from.  If this task is in an RCU read-side</span>
<span class="cm"> * critical section, we will no longer be able to rely on the CPU to</span>
<span class="cm"> * record that fact, so we enqueue the task on the blkd_tasks list.</span>
<span class="cm"> * The task will dequeue itself when it exits the outermost enclosing</span>
<span class="cm"> * RCU read-side critical section.  Therefore, the current grace period</span>
<span class="cm"> * cannot be permitted to complete until the blkd_tasks list entries</span>
<span class="cm"> * predating the current grace period drain, in other words, until</span>
<span class="cm"> * rnp-&gt;gp_tasks becomes NULL.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must disable preemption.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_preempt_note_context_switch</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span> <span class="o">=</span> <span class="n">current</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
	    <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span> <span class="o">&amp;</span> <span class="n">RCU_READ_UNLOCK_BLOCKED</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>

		<span class="cm">/* Possibly blocking in an RCU read-side critical section. */</span>
		<span class="n">rdp</span> <span class="o">=</span> <span class="n">__this_cpu_ptr</span><span class="p">(</span><span class="n">rcu_preempt_state</span><span class="p">.</span><span class="n">rda</span><span class="p">);</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span> <span class="o">|=</span> <span class="n">RCU_READ_UNLOCK_BLOCKED</span><span class="p">;</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_blocked_node</span> <span class="o">=</span> <span class="n">rnp</span><span class="p">;</span>

		<span class="cm">/*</span>
<span class="cm">		 * If this CPU has already checked in, then this task</span>
<span class="cm">		 * will hold up the next grace period rather than the</span>
<span class="cm">		 * current grace period.  Queue the task accordingly.</span>
<span class="cm">		 * If the task is queued for the current grace period</span>
<span class="cm">		 * (i.e., this CPU has not yet passed through a quiescent</span>
<span class="cm">		 * state for the current grace period), then as long</span>
<span class="cm">		 * as that task remains queued, the current grace period</span>
<span class="cm">		 * cannot end.  Note that there is some uncertainty as</span>
<span class="cm">		 * to exactly when the current grace period started.</span>
<span class="cm">		 * We take a conservative approach, which can result</span>
<span class="cm">		 * in unnecessarily waiting on tasks that started very</span>
<span class="cm">		 * slightly after the current grace period began.  C&#39;est</span>
<span class="cm">		 * la vie!!!</span>
<span class="cm">		 *</span>
<span class="cm">		 * But first, note that the current CPU must still be</span>
<span class="cm">		 * on line!</span>
<span class="cm">		 */</span>
		<span class="n">WARN_ON_ONCE</span><span class="p">((</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span> <span class="o">&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="o">!</span><span class="n">list_empty</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">));</span>
		<span class="k">if</span> <span class="p">((</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">list_add</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="o">-&gt;</span><span class="n">prev</span><span class="p">);</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="n">list_add</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">);</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">)</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">trace_rcu_preempt_task</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span>
				       <span class="n">t</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">,</span>
				       <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">)</span>
				       <span class="o">?</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span>
				       <span class="o">:</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
		   <span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span><span class="p">)</span> <span class="p">{</span>

		<span class="cm">/*</span>
<span class="cm">		 * Complete exit from RCU read-side critical section on</span>
<span class="cm">		 * behalf of preempted instance of __rcu_read_unlock().</span>
<span class="cm">		 */</span>
		<span class="n">rcu_read_unlock_special</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Either we were not in an RCU read-side critical section to</span>
<span class="cm">	 * begin with, or we have now recorded that critical section</span>
<span class="cm">	 * globally.  Either way, we can now note a quiescent state</span>
<span class="cm">	 * for this CPU.  Again, if we were in an RCU read-side critical</span>
<span class="cm">	 * section, and if that critical section was blocking the current</span>
<span class="cm">	 * grace period, then the fact that the task has been enqueued</span>
<span class="cm">	 * means that we continue to block the current grace period.</span>
<span class="cm">	 */</span>
	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rcu_preempt_qs</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">());</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Tree-preemptible RCU implementation for rcu_read_lock().</span>
<span class="cm"> * Just increment -&gt;rcu_read_lock_nesting, shared state will be updated</span>
<span class="cm"> * if we block.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">__rcu_read_lock</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">current</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span><span class="o">++</span><span class="p">;</span>
	<span class="n">barrier</span><span class="p">();</span>  <span class="cm">/* needed if we ever invoke rcu_read_lock in rcutree.c */</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">__rcu_read_lock</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Check for preempted RCU readers blocking the current grace period</span>
<span class="cm"> * for the specified rcu_node structure.  If the caller needs a reliable</span>
<span class="cm"> * answer, it must hold the rcu_node&#39;s -&gt;lock.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Record a quiescent state for all tasks that were previously queued</span>
<span class="cm"> * on the specified rcu_node structure and that were blocking the current</span>
<span class="cm"> * RCU grace period.  The caller must hold the specified rnp-&gt;lock with</span>
<span class="cm"> * irqs disabled, and this lock is released upon return, but irqs remain</span>
<span class="cm"> * disabled.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_report_unblock_qs_rnp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp_p</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>  <span class="cm">/* Still need more quiescent states! */</span>
	<span class="p">}</span>

	<span class="n">rnp_p</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp_p</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="cm">/*</span>
<span class="cm">		 * Either there is only one rcu_node in the tree,</span>
<span class="cm">		 * or tasks were kicked up to root rcu_node due to</span>
<span class="cm">		 * CPUs going offline.</span>
<span class="cm">		 */</span>
		<span class="n">rcu_report_qs_rsp</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Report up the rest of the hierarchy. */</span>
	<span class="n">mask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* irqs remain disabled. */</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp_p</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* irqs already disabled. */</span>
	<span class="n">rcu_report_qs_rnp</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="n">rnp_p</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Advance a -&gt;blkd_tasks-list pointer to the next entry, instead</span>
<span class="cm"> * returning NULL if at the end of the list.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="k">struct</span> <span class="n">list_head</span> <span class="o">*</span><span class="nf">rcu_next_node_entry</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">,</span>
					     <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="o">*</span><span class="n">np</span><span class="p">;</span>

	<span class="n">np</span> <span class="o">=</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">.</span><span class="n">next</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">np</span> <span class="o">==</span> <span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">)</span>
		<span class="n">np</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">np</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Handle special cases during rcu_read_unlock(), such as needing to</span>
<span class="cm"> * notify RCU core processing or task having blocked during the RCU</span>
<span class="cm"> * read-side critical section.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="n">noinline</span> <span class="kt">void</span> <span class="nf">rcu_read_unlock_special</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">empty</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">empty_exp</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">empty_exp_now</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="o">*</span><span class="n">np</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
	<span class="k">struct</span> <span class="n">rt_mutex</span> <span class="o">*</span><span class="n">rbmp</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">special</span><span class="p">;</span>

	<span class="cm">/* NMI handlers cannot block and cannot safely manipulate state. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">in_nmi</span><span class="p">())</span>
		<span class="k">return</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * If RCU core is waiting for this CPU to exit critical section,</span>
<span class="cm">	 * let it know that we have done so.</span>
<span class="cm">	 */</span>
	<span class="n">special</span> <span class="o">=</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">special</span> <span class="o">&amp;</span> <span class="n">RCU_READ_UNLOCK_NEED_QS</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rcu_preempt_qs</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">());</span>
	<span class="p">}</span>

	<span class="cm">/* Hardware IRQ handlers cannot block. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">in_irq</span><span class="p">()</span> <span class="o">||</span> <span class="n">in_serving_softirq</span><span class="p">())</span> <span class="p">{</span>
		<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Clean up if blocked during RCU read-side critical section. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">special</span> <span class="o">&amp;</span> <span class="n">RCU_READ_UNLOCK_BLOCKED</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span> <span class="o">&amp;=</span> <span class="o">~</span><span class="n">RCU_READ_UNLOCK_BLOCKED</span><span class="p">;</span>

		<span class="cm">/*</span>
<span class="cm">		 * Remove this task from the list it blocked on.  The</span>
<span class="cm">		 * task can migrate while we acquire the lock, but at</span>
<span class="cm">		 * most one time.  So at most two passes through loop.</span>
<span class="cm">		 */</span>
		<span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
			<span class="n">rnp</span> <span class="o">=</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_blocked_node</span><span class="p">;</span>
			<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs already disabled. */</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">==</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_blocked_node</span><span class="p">)</span>
				<span class="k">break</span><span class="p">;</span>
			<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled. */</span>
		<span class="p">}</span>
		<span class="n">empty</span> <span class="o">=</span> <span class="o">!</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="n">empty_exp</span> <span class="o">=</span> <span class="o">!</span><span class="n">rcu_preempted_readers_exp</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* ensure expedited fastpath sees end of RCU c-s. */</span>
		<span class="n">np</span> <span class="o">=</span> <span class="n">rcu_next_node_entry</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">rnp</span><span class="p">);</span>
		<span class="n">list_del_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">);</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_blocked_node</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
		<span class="n">trace_rcu_unlock_preempted_task</span><span class="p">(</span><span class="s">&quot;rcu_preempt&quot;</span><span class="p">,</span>
						<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">)</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">=</span> <span class="n">np</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span><span class="p">)</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">=</span> <span class="n">np</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span><span class="p">)</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">=</span> <span class="n">np</span><span class="p">;</span>
		<span class="cm">/* Snapshot/clear -&gt;rcu_boost_mutex with rcu_node lock held. */</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_boost_mutex</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">rbmp</span> <span class="o">=</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_boost_mutex</span><span class="p">;</span>
			<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_boost_mutex</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
		<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

		<span class="cm">/*</span>
<span class="cm">		 * If this was the last task on the current list, and if</span>
<span class="cm">		 * we aren&#39;t waiting on any CPUs, report the quiescent state.</span>
<span class="cm">		 * Note that rcu_report_unblock_qs_rnp() releases rnp-&gt;lock,</span>
<span class="cm">		 * so we must take a snapshot of the expedited state.</span>
<span class="cm">		 */</span>
		<span class="n">empty_exp_now</span> <span class="o">=</span> <span class="o">!</span><span class="n">rcu_preempted_readers_exp</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">empty</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">trace_rcu_quiescent_state_report</span><span class="p">(</span><span class="s">&quot;preempt_rcu&quot;</span><span class="p">,</span>
							 <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span>
							 <span class="mi">0</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">,</span>
							 <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">,</span>
							 <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">,</span>
							 <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">,</span>
							 <span class="o">!!</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">);</span>
			<span class="n">rcu_report_unblock_qs_rnp</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="p">}</span> <span class="k">else</span>
			<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
		<span class="cm">/* Unboost if we were boosted. */</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rbmp</span><span class="p">)</span>
			<span class="n">rt_mutex_unlock</span><span class="p">(</span><span class="n">rbmp</span><span class="p">);</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

		<span class="cm">/*</span>
<span class="cm">		 * If this was the last task on the expedited lists,</span>
<span class="cm">		 * then we need to report up the rcu_node hierarchy.</span>
<span class="cm">		 */</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">empty_exp</span> <span class="o">&amp;&amp;</span> <span class="n">empty_exp_now</span><span class="p">)</span>
			<span class="n">rcu_report_exp_rnp</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Tree-preemptible RCU implementation for rcu_read_unlock().</span>
<span class="cm"> * Decrement -&gt;rcu_read_lock_nesting.  If the result is zero (outermost</span>
<span class="cm"> * rcu_read_unlock()) and -&gt;rcu_read_unlock_special is non-zero, then</span>
<span class="cm"> * invoke rcu_read_unlock_special() to clean up after a context switch</span>
<span class="cm"> * in an RCU read-side critical section and other special cases.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">__rcu_read_unlock</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span> <span class="o">=</span> <span class="n">current</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="o">--</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span><span class="p">;</span>
	<span class="k">else</span> <span class="p">{</span>
		<span class="n">barrier</span><span class="p">();</span>  <span class="cm">/* critical section before exit code. */</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">=</span> <span class="n">INT_MIN</span><span class="p">;</span>
		<span class="n">barrier</span><span class="p">();</span>  <span class="cm">/* assign before -&gt;rcu_read_unlock_special load */</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span><span class="p">)))</span>
			<span class="n">rcu_read_unlock_special</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
		<span class="n">barrier</span><span class="p">();</span>  <span class="cm">/* -&gt;rcu_read_unlock_special load before assign */</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>
<span class="cp">#ifdef CONFIG_PROVE_LOCKING</span>
	<span class="p">{</span>
		<span class="kt">int</span> <span class="n">rrln</span> <span class="o">=</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span><span class="p">);</span>

		<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rrln</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">rrln</span> <span class="o">&gt;</span> <span class="n">INT_MIN</span> <span class="o">/</span> <span class="mi">2</span><span class="p">);</span>
	<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_PROVE_LOCKING */</span><span class="cp"></span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">__rcu_read_unlock</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_RCU_CPU_STALL_VERBOSE</span>

<span class="cm">/*</span>
<span class="cm"> * Dump detailed information for all tasks blocking the current RCU</span>
<span class="cm"> * grace period on the specified rcu_node structure.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_detail_task_stall_rnp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">t</span> <span class="o">=</span> <span class="n">list_entry</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">,</span>
		       <span class="k">struct</span> <span class="n">task_struct</span><span class="p">,</span> <span class="n">rcu_node_entry</span><span class="p">);</span>
	<span class="n">list_for_each_entry_continue</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">,</span> <span class="n">rcu_node_entry</span><span class="p">)</span>
		<span class="n">sched_show_task</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Dump detailed information for all tasks blocking the current RCU</span>
<span class="cm"> * grace period.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_detail_task_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="n">rcu_print_detail_task_stall_rnp</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
	<span class="n">rcu_for_each_leaf_node</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span>
		<span class="n">rcu_print_detail_task_stall_rnp</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_CPU_STALL_VERBOSE */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_detail_task_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_CPU_STALL_VERBOSE */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_RCU_CPU_STALL_INFO</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_task_stall_begin</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_ERR</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">Tasks blocked on level-%d rcu_node (CPUs %d-%d):&quot;</span><span class="p">,</span>
	       <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_task_stall_end</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_CPU_STALL_INFO */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_task_stall_begin</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_task_stall_end</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_CPU_STALL_INFO */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Scan the current list of tasks blocked within RCU read-side critical</span>
<span class="cm"> * sections, printing out the tid of each.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_print_task_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">ndetected</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rcu_print_task_stall_begin</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
	<span class="n">t</span> <span class="o">=</span> <span class="n">list_entry</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">,</span>
		       <span class="k">struct</span> <span class="n">task_struct</span><span class="p">,</span> <span class="n">rcu_node_entry</span><span class="p">);</span>
	<span class="n">list_for_each_entry_continue</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">,</span> <span class="n">rcu_node_entry</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot; P%d&quot;</span><span class="p">,</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">);</span>
		<span class="n">ndetected</span><span class="o">++</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">rcu_print_task_stall_end</span><span class="p">();</span>
	<span class="k">return</span> <span class="n">ndetected</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Suppress preemptible RCU&#39;s CPU stall warnings by pushing the</span>
<span class="cm"> * time of the next stall-warning message comfortably far into the</span>
<span class="cm"> * future.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_stall_reset</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_preempt_state</span><span class="p">.</span><span class="n">jiffies_stall</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">ULONG_MAX</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check that the list of blocked tasks for the newly completed grace</span>
<span class="cm"> * period is in fact empty.  It is a serious bug to complete a grace</span>
<span class="cm"> * period that still has RCU readers blocked!  This function must be</span>
<span class="cm"> * invoked -before- updating this rnp&#39;s -&gt;gpnum, and the rnp&#39;s -&gt;lock</span>
<span class="cm"> * must be held by the caller.</span>
<span class="cm"> *</span>
<span class="cm"> * Also, if there are blocked tasks on the list, they automatically</span>
<span class="cm"> * block the newly created grace period, so set up -&gt;gp_tasks accordingly.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_check_blocked_tasks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">));</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">list_empty</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">))</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">.</span><span class="n">next</span><span class="p">;</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/*</span>
<span class="cm"> * Handle tasklist migration for case in which all CPUs covered by the</span>
<span class="cm"> * specified rcu_node have gone offline.  Move them up to the root</span>
<span class="cm"> * rcu_node.  The reason for not just moving them to the immediate</span>
<span class="cm"> * parent is to remove the need for rcu_read_unlock_special() to</span>
<span class="cm"> * make more than two attempts to acquire the target rcu_node&#39;s lock.</span>
<span class="cm"> * Returns true if there were tasks blocking the current RCU grace</span>
<span class="cm"> * period.</span>
<span class="cm"> *</span>
<span class="cm"> * Returns 1 if there was previously a task blocking the current grace</span>
<span class="cm"> * period on the specified rcu_node structure.</span>
<span class="cm"> *</span>
<span class="cm"> * The caller must hold rnp-&gt;lock with irqs disabled.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_offline_tasks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
				     <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span>
				     <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="o">*</span><span class="n">lp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="o">*</span><span class="n">lp_root</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">retval</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp_root</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">==</span> <span class="n">rnp_root</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">WARN_ONCE</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s">&quot;Last CPU thought to be offlined?&quot;</span><span class="p">);</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>  <span class="cm">/* Shouldn&#39;t happen: at least one CPU online. */</span>
	<span class="p">}</span>

	<span class="cm">/* If we are on an internal node, complain bitterly. */</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rnp</span> <span class="o">!=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Move tasks up to root rcu_node.  Don&#39;t try to get fancy for</span>
<span class="cm">	 * this corner-case operation -- just put this node&#39;s tasks</span>
<span class="cm">	 * at the head of the root node&#39;s list, and update the root node&#39;s</span>
<span class="cm">	 * -&gt;gp_tasks and -&gt;exp_tasks pointers to those of this node&#39;s,</span>
<span class="cm">	 * if non-NULL.  This might result in waiting for more tasks than</span>
<span class="cm">	 * absolutely necessary, but this is a good performance/complexity</span>
<span class="cm">	 * tradeoff.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">retval</span> <span class="o">|=</span> <span class="n">RCU_OFL_TASKS_NORM_GP</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_preempted_readers_exp</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span>
		<span class="n">retval</span> <span class="o">|=</span> <span class="n">RCU_OFL_TASKS_EXP_GP</span><span class="p">;</span>
	<span class="n">lp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">;</span>
	<span class="n">lp_root</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">;</span>
	<span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">list_empty</span><span class="p">(</span><span class="n">lp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">t</span> <span class="o">=</span> <span class="n">list_entry</span><span class="p">(</span><span class="n">lp</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">,</span> <span class="n">typeof</span><span class="p">(</span><span class="o">*</span><span class="n">t</span><span class="p">),</span> <span class="n">rcu_node_entry</span><span class="p">);</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled */</span>
		<span class="n">list_del</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">);</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_blocked_node</span> <span class="o">=</span> <span class="n">rnp_root</span><span class="p">;</span>
		<span class="n">list_add</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span><span class="p">,</span> <span class="n">lp_root</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">)</span>
			<span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span><span class="p">)</span>
			<span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_node_entry</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span><span class="p">)</span>
			<span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs still disabled */</span>
	<span class="p">}</span>

<span class="cp">#ifdef CONFIG_RCU_BOOST</span>
	<span class="cm">/* In case root is being boosted and leaf is not. */</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span>
	    <span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">!=</span> <span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">)</span>
		<span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">=</span> <span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">;</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs still disabled */</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

	<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">retval</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Do CPU-offline processing for preemptible RCU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_cleanup_dead_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_cleanup_dead_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check for a quiescent state from the current CPU.  When a task blocks,</span>
<span class="cm"> * the task is recorded in the corresponding CPU&#39;s rcu_node structure,</span>
<span class="cm"> * which is checked elsewhere.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must disable hard irqs.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_check_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span> <span class="o">=</span> <span class="n">current</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rcu_preempt_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_lock_nesting</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
	    <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">qs_pending</span><span class="p">)</span>
		<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_read_unlock_special</span> <span class="o">|=</span> <span class="n">RCU_READ_UNLOCK_NEED_QS</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Process callbacks for preemptible RCU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_process_callbacks</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">__rcu_process_callbacks</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">));</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_RCU_BOOST</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_do_callbacks</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_do_batch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">));</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Queue a preemptible-RCU callback for invocation after a grace period.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">call_rcu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">))</span>
<span class="p">{</span>
	<span class="n">__call_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">call_rcu</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Queue an RCU callback for lazy invocation after a grace period.</span>
<span class="cm"> * This will likely be later named something like &quot;call_rcu_lazy()&quot;,</span>
<span class="cm"> * but this change will require some way of tagging the lazy RCU</span>
<span class="cm"> * callbacks in the list of pending callbacks.  Until then, this</span>
<span class="cm"> * function may only be called from __kfree_rcu().</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">kfree_call_rcu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
		    <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">))</span>
<span class="p">{</span>
	<span class="n">__call_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">kfree_call_rcu</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * synchronize_rcu - wait until a grace period has elapsed.</span>
<span class="cm"> *</span>
<span class="cm"> * Control will return to the caller some time after a full grace</span>
<span class="cm"> * period has elapsed, in other words after all currently executing RCU</span>
<span class="cm"> * read-side critical sections have completed.  Note, however, that</span>
<span class="cm"> * upon return from synchronize_rcu(), the caller might well be executing</span>
<span class="cm"> * concurrently with new RCU read-side critical sections that began while</span>
<span class="cm"> * synchronize_rcu() was waiting.  RCU read-side critical sections are</span>
<span class="cm"> * delimited by rcu_read_lock() and rcu_read_unlock(), and may be nested.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">synchronize_rcu</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_lock_map</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
			   <span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
			   <span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal synchronize_rcu() in RCU read-side critical section&quot;</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_scheduler_active</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">wait_rcu_gp</span><span class="p">(</span><span class="n">call_rcu</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">synchronize_rcu</span><span class="p">);</span>

<span class="k">static</span> <span class="n">DECLARE_WAIT_QUEUE_HEAD</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_wq</span><span class="p">);</span>
<span class="k">static</span> <span class="kt">long</span> <span class="n">sync_rcu_preempt_exp_count</span><span class="p">;</span>
<span class="k">static</span> <span class="n">DEFINE_MUTEX</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_mutex</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Return non-zero if there are any tasks in RCU read-side critical</span>
<span class="cm"> * sections blocking the current preemptible-RCU expedited grace period.</span>
<span class="cm"> * If there is no preemptible-RCU expedited grace period currently in</span>
<span class="cm"> * progress, returns zero unconditionally.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempted_readers_exp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * return non-zero if there is no RCU expedited grace period in progress</span>
<span class="cm"> * for the specified rcu_node structure, in other words, if all CPUs and</span>
<span class="cm"> * tasks covered by the specified rcu_node structure have done their bit</span>
<span class="cm"> * for the current expedited grace period.  Works only for preemptible</span>
<span class="cm"> * RCU -- other RCU implementation use other means.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must hold sync_rcu_preempt_exp_mutex.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">sync_rcu_preempt_exp_done</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="o">!</span><span class="n">rcu_preempted_readers_exp</span><span class="p">(</span><span class="n">rnp</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
	       <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">expmask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Report the exit from RCU read-side critical section for the last task</span>
<span class="cm"> * that queued itself during or before the current expedited preemptible-RCU</span>
<span class="cm"> * grace period.  This event is reported either to the rcu_node structure on</span>
<span class="cm"> * which the task was queued or to one of that rcu_node structure&#39;s ancestors,</span>
<span class="cm"> * recursively up the tree.  (Calm down, calm down, we do the recursion</span>
<span class="cm"> * iteratively!)</span>
<span class="cm"> *</span>
<span class="cm"> * Most callers will set the &quot;wake&quot; flag, but the task initiating the</span>
<span class="cm"> * expedited grace period need not wake itself.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must hold sync_rcu_preempt_exp_mutex.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_report_exp_rnp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span>
			       <span class="n">bool</span> <span class="n">wake</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>

	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">sync_rcu_preempt_exp_done</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">wake</span><span class="p">)</span>
				<span class="n">wake_up</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_rcu_preempt_exp_wq</span><span class="p">);</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled */</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">;</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled */</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">expmask</span> <span class="o">&amp;=</span> <span class="o">~</span><span class="n">mask</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Snapshot the tasks blocking the newly started preemptible-RCU expedited</span>
<span class="cm"> * grace period for the specified rcu_node structure.  If there are no such</span>
<span class="cm"> * tasks, report it up the rcu_node hierarchy.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must hold sync_rcu_preempt_exp_mutex and rsp-&gt;onofflock.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">sync_rcu_preempt_exp_init</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">must_wait</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">list_empty</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">))</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">else</span> <span class="p">{</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">.</span><span class="n">next</span><span class="p">;</span>
		<span class="n">rcu_initiate_boost</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>  <span class="cm">/* releases rnp-&gt;lock */</span>
		<span class="n">must_wait</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">must_wait</span><span class="p">)</span>
		<span class="n">rcu_report_exp_rnp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="nb">false</span><span class="p">);</span> <span class="cm">/* Don&#39;t wake self. */</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * synchronize_rcu_expedited - Brute-force RCU grace period</span>
<span class="cm"> *</span>
<span class="cm"> * Wait for an RCU-preempt grace period, but expedite it.  The basic</span>
<span class="cm"> * idea is to invoke synchronize_sched_expedited() to push all the tasks to</span>
<span class="cm"> * the -&gt;blkd_tasks lists and wait for this list to drain.  This consumes</span>
<span class="cm"> * significant time on all CPUs and is unfriendly to real-time workloads,</span>
<span class="cm"> * so is thus not recommended for any sort of common-case code.</span>
<span class="cm"> * In fact, if you are using synchronize_rcu_expedited() in a loop,</span>
<span class="cm"> * please restructure your code to batch your updates, and then Use a</span>
<span class="cm"> * single synchronize_rcu() instead.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that it is illegal to call this function while holding any lock</span>
<span class="cm"> * that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal</span>
<span class="cm"> * to call this function from a CPU-hotplug notifier.  Failing to observe</span>
<span class="cm"> * these restriction will result in deadlock.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">synchronize_rcu_expedited</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">;</span>
	<span class="kt">long</span> <span class="n">snap</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">trycount</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* Caller&#39;s modifications seen first by other CPUs. */</span>
	<span class="n">snap</span> <span class="o">=</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_count</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* Above access cannot bleed into critical section. */</span>

	<span class="cm">/*</span>
<span class="cm">	 * Acquire lock, falling back to synchronize_rcu() if too many</span>
<span class="cm">	 * lock-acquisition failures.  Of course, if someone does the</span>
<span class="cm">	 * expedited grace period for us, just leave.</span>
<span class="cm">	 */</span>
	<span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">mutex_trylock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_rcu_preempt_exp_mutex</span><span class="p">))</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">trycount</span><span class="o">++</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">)</span>
			<span class="n">udelay</span><span class="p">(</span><span class="n">trycount</span> <span class="o">*</span> <span class="n">num_online_cpus</span><span class="p">());</span>
		<span class="k">else</span> <span class="p">{</span>
			<span class="n">synchronize_rcu</span><span class="p">();</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">((</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_count</span><span class="p">)</span> <span class="o">-</span> <span class="n">snap</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
			<span class="k">goto</span> <span class="n">mb_ret</span><span class="p">;</span> <span class="cm">/* Others did our work for us. */</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">((</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_count</span><span class="p">)</span> <span class="o">-</span> <span class="n">snap</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
		<span class="k">goto</span> <span class="n">unlock_mb_ret</span><span class="p">;</span> <span class="cm">/* Others did our work for us. */</span>

	<span class="cm">/* force all RCU readers onto -&gt;blkd_tasks lists. */</span>
	<span class="n">synchronize_sched_expedited</span><span class="p">();</span>

	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="cm">/* Initialize -&gt;expmask for all non-leaf rcu_node structures. */</span>
	<span class="n">rcu_for_each_nonleaf_node_breadth_first</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled. */</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">expmask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span><span class="p">;</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled. */</span>
	<span class="p">}</span>

	<span class="cm">/* Snapshot current state of -&gt;blkd_tasks lists. */</span>
	<span class="n">rcu_for_each_leaf_node</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span>
		<span class="n">sync_rcu_preempt_exp_init</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">NUM_RCU_NODES</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
		<span class="n">sync_rcu_preempt_exp_init</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">));</span>

	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="cm">/* Wait for snapshotted -&gt;blkd_tasks lists to drain. */</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="n">wait_event</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_wq</span><span class="p">,</span>
		   <span class="n">sync_rcu_preempt_exp_done</span><span class="p">(</span><span class="n">rnp</span><span class="p">));</span>

	<span class="cm">/* Clean up and exit. */</span>
	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* ensure expedited GP seen before counter increment. */</span>
	<span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">sync_rcu_preempt_exp_count</span><span class="p">)</span><span class="o">++</span><span class="p">;</span>
<span class="nl">unlock_mb_ret:</span>
	<span class="n">mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_rcu_preempt_exp_mutex</span><span class="p">);</span>
<span class="nl">mb_ret:</span>
	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* ensure subsequent action seen after grace period. */</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">synchronize_rcu_expedited</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if there is any immediate preemptible-RCU-related work</span>
<span class="cm"> * to be done.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_pending</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">__rcu_pending</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span>
			     <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">));</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Does preemptible RCU have callbacks on this CPU?</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_cpu_has_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="o">!!</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">nxtlist</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_barrier - Wait until all in-flight call_rcu() callbacks complete.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_barrier</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">_rcu_barrier</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="n">call_rcu</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_barrier</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Initialize preemptible RCU&#39;s per-CPU data.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__cpuinit</span> <span class="nf">rcu_preempt_init_percpu_data</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_init_percpu_data</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Move preemptible RCU&#39;s callbacks from dying CPU to other online CPU</span>
<span class="cm"> * and record a quiescent state.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_cleanup_dying_cpu</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_cleanup_dying_cpu</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Initialize preemptible RCU&#39;s state structures.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">__rcu_init_preempt</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_init_one</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_preempt_data</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_TREE_PREEMPT_RCU */</span><span class="cp"></span>

<span class="k">static</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rcu_state</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">;</span>

<span class="cm">/*</span>
<span class="cm"> * Tell them what RCU they are running.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_bootup_announce</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_INFO</span> <span class="s">&quot;Hierarchical RCU implementation.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
	<span class="n">rcu_bootup_announce_oddness</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Return the number of RCU batches processed thus far for debug &amp; stats.</span>
<span class="cm"> */</span>
<span class="kt">long</span> <span class="nf">rcu_batches_completed</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rcu_batches_completed_sched</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_batches_completed</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Force a quiescent state for RCU, which, because there is no preemptible</span>
<span class="cm"> * RCU, becomes the same as rcu-sched.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_force_quiescent_state</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_sched_force_quiescent_state</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_force_quiescent_state</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, there are never any preempted</span>
<span class="cm"> * RCU readers.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/* Because preemptible RCU does not exist, no quieting of tasks. */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_report_unblock_qs_rnp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, we never have to check for</span>
<span class="cm"> * tasks blocked within RCU read-side critical sections.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_print_detail_task_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, we never have to check for</span>
<span class="cm"> * tasks blocked within RCU read-side critical sections.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_print_task_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, there is no need to suppress</span>
<span class="cm"> * its CPU stall warnings.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_stall_reset</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because there is no preemptible RCU, there can be no readers blocked,</span>
<span class="cm"> * so there is no need to check for blocked tasks.  So check only for</span>
<span class="cm"> * bogus qsmask values.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_check_blocked_tasks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it never needs to migrate</span>
<span class="cm"> * tasks that were blocked within RCU read-side critical sections, and</span>
<span class="cm"> * such non-existent tasks cannot possibly have been blocking the current</span>
<span class="cm"> * grace period.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_offline_tasks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
				     <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span>
				     <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it never needs CPU-offline</span>
<span class="cm"> * processing.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_cleanup_dead_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it never has any callbacks</span>
<span class="cm"> * to check.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_check_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it never has any callbacks</span>
<span class="cm"> * to process.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_process_callbacks</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Queue an RCU callback for lazy invocation after a grace period.</span>
<span class="cm"> * This will likely be later named something like &quot;call_rcu_lazy()&quot;,</span>
<span class="cm"> * but this change will require some way of tagging the lazy RCU</span>
<span class="cm"> * callbacks in the list of pending callbacks.  Until then, this</span>
<span class="cm"> * function may only be called from __kfree_rcu().</span>
<span class="cm"> *</span>
<span class="cm"> * Because there is no preemptible RCU, we use RCU-sched instead.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">kfree_call_rcu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
		    <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">))</span>
<span class="p">{</span>
	<span class="n">__call_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">kfree_call_rcu</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Wait for an rcu-preempt grace period, but make it happen quickly.</span>
<span class="cm"> * But because preemptible RCU does not exist, map to rcu-sched.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">synchronize_rcu_expedited</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">synchronize_sched_expedited</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">synchronize_rcu_expedited</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, there is never any need to</span>
<span class="cm"> * report on tasks preempted in RCU read-side critical sections during</span>
<span class="cm"> * expedited RCU grace periods.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_report_exp_rnp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span>
			       <span class="n">bool</span> <span class="n">wake</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it never has any work to do.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_pending</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it never has callbacks</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_preempt_cpu_has_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, rcu_barrier() is just</span>
<span class="cm"> * another name for rcu_barrier_sched().</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_barrier</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_barrier_sched</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_barrier</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, there is no per-CPU</span>
<span class="cm"> * data to initialize.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__cpuinit</span> <span class="nf">rcu_preempt_init_percpu_data</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because there is no preemptible RCU, there is no cleanup to do.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_cleanup_dying_cpu</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because preemptible RCU does not exist, it need not be initialized.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">__rcu_init_preempt</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_TREE_PREEMPT_RCU */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_RCU_BOOST</span>

<span class="cp">#include &quot;rtmutex_common.h&quot;</span>

<span class="cp">#ifdef CONFIG_RCU_TRACE</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_initiate_boost_trace</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">list_empty</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">))</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_blkd_tasks</span><span class="o">++</span><span class="p">;</span>
	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_exp_gp_tasks</span><span class="o">++</span><span class="p">;</span>
	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_boost_tasks</span><span class="o">++</span><span class="p">;</span>
	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_notblocked</span><span class="o">++</span><span class="p">;</span>
	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span>
		 <span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">jiffies</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_time</span><span class="p">))</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_notyet</span><span class="o">++</span><span class="p">;</span>
	<span class="k">else</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_nos</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_TRACE */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_initiate_boost_trace</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_TRACE */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Carry out RCU priority boosting on the task indicated by -&gt;exp_tasks</span>
<span class="cm"> * or -&gt;boost_tasks, advancing the pointer to the next task in the</span>
<span class="cm"> * -&gt;blkd_tasks list.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that irqs must be enabled: boosting the task can block.</span>
<span class="cm"> * Returns 1 if there are more tasks needing to be boosted.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_boost</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rt_mutex</span> <span class="n">mtx</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="o">*</span><span class="n">tb</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>  <span class="cm">/* Nothing left to boost. */</span>

	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Recheck under the lock: all tasks in need of boosting</span>
<span class="cm">	 * might exit their RCU read-side critical sections on their own.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Preferentially boost tasks blocking expedited grace periods.</span>
<span class="cm">	 * This cannot starve the normal grace periods because a second</span>
<span class="cm">	 * expedited grace period must boost all blocked tasks, including</span>
<span class="cm">	 * those blocking the pre-existing normal grace period.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">tb</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span><span class="p">;</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_exp_boosts</span><span class="o">++</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="n">tb</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span><span class="p">;</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_normal_boosts</span><span class="o">++</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_tasks_boosted</span><span class="o">++</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * We boost task t by manufacturing an rt_mutex that appears to</span>
<span class="cm">	 * be held by task t.  We leave a pointer to that rt_mutex where</span>
<span class="cm">	 * task t can find it, and task t will release the mutex when it</span>
<span class="cm">	 * exits its outermost RCU read-side critical section.  Then</span>
<span class="cm">	 * simply acquiring this artificial rt_mutex will boost task</span>
<span class="cm">	 * t&#39;s priority.  (Thanks to tglx for suggesting this approach!)</span>
<span class="cm">	 *</span>
<span class="cm">	 * Note that task t must acquire rnp-&gt;lock to remove itself from</span>
<span class="cm">	 * the -&gt;blkd_tasks list, which it will do from exit() if from</span>
<span class="cm">	 * nowhere else.  We therefore are guaranteed that task t will</span>
<span class="cm">	 * stay around at least until we drop rnp-&gt;lock.  Note that</span>
<span class="cm">	 * rnp-&gt;lock also resolves races between our priority boosting</span>
<span class="cm">	 * and task t&#39;s exiting its outermost RCU read-side critical</span>
<span class="cm">	 * section.</span>
<span class="cm">	 */</span>
	<span class="n">t</span> <span class="o">=</span> <span class="n">container_of</span><span class="p">(</span><span class="n">tb</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span><span class="p">,</span> <span class="n">rcu_node_entry</span><span class="p">);</span>
	<span class="n">rt_mutex_init_proxy_locked</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mtx</span><span class="p">,</span> <span class="n">t</span><span class="p">);</span>
	<span class="n">t</span><span class="o">-&gt;</span><span class="n">rcu_boost_mutex</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">mtx</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">rt_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mtx</span><span class="p">);</span>  <span class="cm">/* Side effect: boosts task t&#39;s priority. */</span>
	<span class="n">rt_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mtx</span><span class="p">);</span>  <span class="cm">/* Keep lockdep happy. */</span>

	<span class="k">return</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">||</span>
	       <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Timer handler to initiate waking up of boost kthreads that</span>
<span class="cm"> * have yielded the CPU due to excessive numbers of tasks to</span>
<span class="cm"> * boost.  We wake up the per-rcu_node kthread, which in turn</span>
<span class="cm"> * will wake up the booster kthread.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_boost_kthread_timer</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">invoke_rcu_node_kthread</span><span class="p">((</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="p">)</span><span class="n">arg</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Priority-boosting kthread.  One per leaf rcu_node and one for the</span>
<span class="cm"> * root rcu_node.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_boost_kthread</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="p">)</span><span class="n">arg</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">spincnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">more2boost</span><span class="p">;</span>

	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start boost kthread@init&quot;</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_status</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_WAITING</span><span class="p">;</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End boost kthread@rcu_wait&quot;</span><span class="p">);</span>
		<span class="n">rcu_wait</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">||</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span><span class="p">);</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start boost kthread@rcu_wait&quot;</span><span class="p">);</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_status</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_RUNNING</span><span class="p">;</span>
		<span class="n">more2boost</span> <span class="o">=</span> <span class="n">rcu_boost</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">more2boost</span><span class="p">)</span>
			<span class="n">spincnt</span><span class="o">++</span><span class="p">;</span>
		<span class="k">else</span>
			<span class="n">spincnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">spincnt</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End boost kthread@rcu_yield&quot;</span><span class="p">);</span>
			<span class="n">rcu_yield</span><span class="p">(</span><span class="n">rcu_boost_kthread_timer</span><span class="p">,</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">rnp</span><span class="p">);</span>
			<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start boost kthread@rcu_yield&quot;</span><span class="p">);</span>
			<span class="n">spincnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="cm">/* NOTREACHED */</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End boost kthread@notreached&quot;</span><span class="p">);</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if it is time to start boosting RCU readers that are</span>
<span class="cm"> * blocking the current grace period, and, if so, tell the per-rcu_node</span>
<span class="cm"> * kthread to start boosting them.  If there is an expedited grace</span>
<span class="cm"> * period in progress, it is always time to boost.</span>
<span class="cm"> *</span>
<span class="cm"> * The caller must hold rnp-&gt;lock, which this function releases,</span>
<span class="cm"> * but irqs remain disabled.  The -&gt;boost_kthread_task is immortal,</span>
<span class="cm"> * so we don&#39;t need to worry about it going away.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_initiate_boost</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">n_balk_exp_gp_tasks</span><span class="o">++</span><span class="p">;</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">||</span>
	    <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span>
	     <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span>
	     <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
	     <span class="n">ULONG_CMP_GE</span><span class="p">(</span><span class="n">jiffies</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_time</span><span class="p">)))</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">exp_tasks</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_tasks</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">;</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">t</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_task</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
			<span class="n">wake_up_process</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="n">rcu_initiate_boost_trace</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Wake up the per-CPU kthread to invoke RCU callbacks.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">invoke_rcu_callbacks_kthread</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">__this_cpu_write</span><span class="p">(</span><span class="n">rcu_cpu_has_work</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">__this_cpu_read</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span>
	    <span class="n">current</span> <span class="o">!=</span> <span class="n">__this_cpu_read</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">))</span>
		<span class="n">wake_up_process</span><span class="p">(</span><span class="n">__this_cpu_read</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">));</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Is the current CPU running the RCU-callbacks kthread?</span>
<span class="cm"> * Caller must have preemption disabled.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="n">bool</span> <span class="nf">rcu_is_callbacks_kthread</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">)</span> <span class="o">==</span> <span class="n">current</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Set the affinity of the boost kthread.  The CPU-hotplug locks are</span>
<span class="cm"> * held, so no one should be messing with the existence of the boost</span>
<span class="cm"> * kthread.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_boost_kthread_setaffinity</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span>
					  <span class="n">cpumask_var_t</span> <span class="n">cm</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="n">t</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_task</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="n">set_cpus_allowed_ptr</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_task</span><span class="p">,</span> <span class="n">cm</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#define RCU_BOOST_DELAY_JIFFIES DIV_ROUND_UP(CONFIG_RCU_BOOST_DELAY * HZ, 1000)</span>

<span class="cm">/*</span>
<span class="cm"> * Do priority-boost accounting for the start of a new grace period.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_boost_start_gp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_time</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">RCU_BOOST_DELAY_JIFFIES</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Create an RCU-boost kthread for the specified node if one does not</span>
<span class="cm"> * already exist.  We only create this kthread for preemptible RCU.</span>
<span class="cm"> * Returns zero if all is well, a negated errno otherwise.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">__cpuinit</span> <span class="nf">rcu_spawn_one_boost_kthread</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
						 <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span>
						 <span class="kt">int</span> <span class="n">rnp_index</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">sched_param</span> <span class="n">sp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span> <span class="o">!=</span> <span class="n">rsp</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">boost</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_task</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">t</span> <span class="o">=</span> <span class="n">kthread_create</span><span class="p">(</span><span class="n">rcu_boost_kthread</span><span class="p">,</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="n">rnp</span><span class="p">,</span>
			   <span class="s">&quot;rcub/%d&quot;</span><span class="p">,</span> <span class="n">rnp_index</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">IS_ERR</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">PTR_ERR</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">boost_kthread_task</span> <span class="o">=</span> <span class="n">t</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="n">RCU_BOOST_PRIO</span><span class="p">;</span>
	<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">SCHED_FIFO</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
	<span class="n">wake_up_process</span><span class="p">(</span><span class="n">t</span><span class="p">);</span> <span class="cm">/* get to TASK_INTERRUPTIBLE quickly. */</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/*</span>
<span class="cm"> * Stop the RCU&#39;s per-CPU kthread when its CPU goes offline,.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_stop_cpu_kthread</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="cm">/* Stop the CPU&#39;s kthread. */</span>
	<span class="n">t</span> <span class="o">=</span> <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
		<span class="n">kthread_stop</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_kthread_do_work</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_do_batch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">));</span>
	<span class="n">rcu_do_batch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">));</span>
	<span class="n">rcu_preempt_do_callbacks</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Wake up the specified per-rcu_node-structure kthread.</span>
<span class="cm"> * Because the per-rcu_node kthreads are immortal, we don&#39;t need</span>
<span class="cm"> * to do anything to keep them alive.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">invoke_rcu_node_kthread</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="n">t</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_task</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="n">wake_up_process</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Set the specified CPU&#39;s kthread to run RT or not, as specified by</span>
<span class="cm"> * the to_rt argument.  The CPU-hotplug locks are held, so the task</span>
<span class="cm"> * is not going away.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cpu_kthread_setrt</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">int</span> <span class="n">to_rt</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">policy</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">sched_param</span> <span class="n">sp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="n">t</span> <span class="o">=</span> <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">to_rt</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">policy</span> <span class="o">=</span> <span class="n">SCHED_FIFO</span><span class="p">;</span>
		<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_PRIO</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="n">policy</span> <span class="o">=</span> <span class="n">SCHED_NORMAL</span><span class="p">;</span>
		<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Timer handler to initiate the waking up of per-CPU kthreads that</span>
<span class="cm"> * have yielded the CPU due to excess numbers of RCU callbacks.</span>
<span class="cm"> * We wake up the per-rcu_node kthread, which in turn will wake up</span>
<span class="cm"> * the booster kthread.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cpu_kthread_timer</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rcu_state</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">arg</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>

	<span class="n">atomic_or</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">wakemask</span><span class="p">);</span>
	<span class="n">invoke_rcu_node_kthread</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Drop to non-real-time priority and yield, but only after posting a</span>
<span class="cm"> * timer that will cause us to regain our real-time priority if we</span>
<span class="cm"> * remain preempted.  Either way, we restore our real-time priority</span>
<span class="cm"> * before returning.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_yield</span><span class="p">(</span><span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">f</span><span class="p">)(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">),</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">sched_param</span> <span class="n">sp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">timer_list</span> <span class="n">yield_timer</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">prio</span> <span class="o">=</span> <span class="n">current</span><span class="o">-&gt;</span><span class="n">rt_priority</span><span class="p">;</span>

	<span class="n">setup_timer_on_stack</span><span class="p">(</span><span class="o">&amp;</span><span class="n">yield_timer</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">arg</span><span class="p">);</span>
	<span class="n">mod_timer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">yield_timer</span><span class="p">,</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="mi">2</span><span class="p">);</span>
	<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">SCHED_NORMAL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
	<span class="n">set_user_nice</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="mi">19</span><span class="p">);</span>
	<span class="n">schedule</span><span class="p">();</span>
	<span class="n">set_user_nice</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="n">prio</span><span class="p">;</span>
	<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">SCHED_FIFO</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
	<span class="n">del_timer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">yield_timer</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Handle cases where the rcu_cpu_kthread() ends up on the wrong CPU.</span>
<span class="cm"> * This can happen while the corresponding CPU is either coming online</span>
<span class="cm"> * or going offline.  We cannot wait until the CPU is fully online</span>
<span class="cm"> * before starting the kthread, because the various notifier functions</span>
<span class="cm"> * can wait for RCU grace periods.  So we park rcu_cpu_kthread() until</span>
<span class="cm"> * the corresponding CPU is online.</span>
<span class="cm"> *</span>
<span class="cm"> * Return 1 if the kthread needs to stop, 0 otherwise.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must disable bh.  This function can momentarily enable it.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_cpu_kthread_should_stop</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">while</span> <span class="p">(</span><span class="n">cpu_is_offline</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span> <span class="o">||</span>
	       <span class="o">!</span><span class="n">cpumask_equal</span><span class="p">(</span><span class="o">&amp;</span><span class="n">current</span><span class="o">-&gt;</span><span class="n">cpus_allowed</span><span class="p">,</span> <span class="n">cpumask_of</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="o">||</span>
	       <span class="n">smp_processor_id</span><span class="p">()</span> <span class="o">!=</span> <span class="n">cpu</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">kthread_should_stop</span><span class="p">())</span>
			<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
		<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_status</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_OFFCPU</span><span class="p">;</span>
		<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_cpu</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="n">raw_smp_processor_id</span><span class="p">();</span>
		<span class="n">local_bh_enable</span><span class="p">();</span>
		<span class="n">schedule_timeout_uninterruptible</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">cpumask_equal</span><span class="p">(</span><span class="o">&amp;</span><span class="n">current</span><span class="o">-&gt;</span><span class="n">cpus_allowed</span><span class="p">,</span> <span class="n">cpumask_of</span><span class="p">(</span><span class="n">cpu</span><span class="p">)))</span>
			<span class="n">set_cpus_allowed_ptr</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">cpumask_of</span><span class="p">(</span><span class="n">cpu</span><span class="p">));</span>
		<span class="n">local_bh_disable</span><span class="p">();</span>
	<span class="p">}</span>
	<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_cpu</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Per-CPU kernel thread that invokes RCU callbacks.  This replaces the</span>
<span class="cm"> * RCU softirq used in flavors and configurations of RCU that do not</span>
<span class="cm"> * support RCU priority boosting.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_cpu_kthread</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)(</span><span class="kt">long</span><span class="p">)</span><span class="n">arg</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">spincnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">statusp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_status</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="kt">char</span> <span class="n">work</span><span class="p">;</span>
	<span class="kt">char</span> <span class="o">*</span><span class="n">workp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_has_work</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start CPU kthread@init&quot;</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">statusp</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_WAITING</span><span class="p">;</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End CPU kthread@rcu_wait&quot;</span><span class="p">);</span>
		<span class="n">rcu_wait</span><span class="p">(</span><span class="o">*</span><span class="n">workp</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">kthread_should_stop</span><span class="p">());</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start CPU kthread@rcu_wait&quot;</span><span class="p">);</span>
		<span class="n">local_bh_disable</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rcu_cpu_kthread_should_stop</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">local_bh_enable</span><span class="p">();</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="o">*</span><span class="n">statusp</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_RUNNING</span><span class="p">;</span>
		<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_loops</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span><span class="o">++</span><span class="p">;</span>
		<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
		<span class="n">work</span> <span class="o">=</span> <span class="o">*</span><span class="n">workp</span><span class="p">;</span>
		<span class="o">*</span><span class="n">workp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">work</span><span class="p">)</span>
			<span class="n">rcu_kthread_do_work</span><span class="p">();</span>
		<span class="n">local_bh_enable</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">*</span><span class="n">workp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">spincnt</span><span class="o">++</span><span class="p">;</span>
		<span class="k">else</span>
			<span class="n">spincnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">spincnt</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">)</span> <span class="p">{</span>
			<span class="o">*</span><span class="n">statusp</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_YIELDING</span><span class="p">;</span>
			<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End CPU kthread@rcu_yield&quot;</span><span class="p">);</span>
			<span class="n">rcu_yield</span><span class="p">(</span><span class="n">rcu_cpu_kthread_timer</span><span class="p">,</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">cpu</span><span class="p">);</span>
			<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start CPU kthread@rcu_yield&quot;</span><span class="p">);</span>
			<span class="n">spincnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="o">*</span><span class="n">statusp</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_STOPPED</span><span class="p">;</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End CPU kthread@term&quot;</span><span class="p">);</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Spawn a per-CPU kthread, setting up affinity and priority.</span>
<span class="cm"> * Because the CPU hotplug lock is held, no other CPU will be attempting</span>
<span class="cm"> * to manipulate rcu_cpu_kthread_task.  There might be another CPU</span>
<span class="cm"> * attempting to access it during boot, but the locking in kthread_bind()</span>
<span class="cm"> * will enforce sufficient ordering.</span>
<span class="cm"> *</span>
<span class="cm"> * Please note that we cannot simply refuse to wake up the per-CPU</span>
<span class="cm"> * kthread because kthreads are created in TASK_UNINTERRUPTIBLE state,</span>
<span class="cm"> * which can result in softlockup complaints if the task ends up being</span>
<span class="cm"> * idle for more than a couple of minutes.</span>
<span class="cm"> *</span>
<span class="cm"> * However, please note also that we cannot bind the per-CPU kthread to its</span>
<span class="cm"> * CPU until that CPU is fully online.  We also cannot wait until the</span>
<span class="cm"> * CPU is fully online before we create its per-CPU kthread, as this would</span>
<span class="cm"> * deadlock the system when CPU notifiers tried waiting for grace</span>
<span class="cm"> * periods.  So we bind the per-CPU kthread to its CPU only if the CPU</span>
<span class="cm"> * is online.  If its CPU is not yet fully online, then the code in</span>
<span class="cm"> * rcu_cpu_kthread() will wait until it is fully online, and then do</span>
<span class="cm"> * the binding.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">__cpuinit</span> <span class="nf">rcu_spawn_one_cpu_kthread</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">sched_param</span> <span class="n">sp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_scheduler_fully_active</span> <span class="o">||</span>
	    <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">t</span> <span class="o">=</span> <span class="n">kthread_create_on_node</span><span class="p">(</span><span class="n">rcu_cpu_kthread</span><span class="p">,</span>
				   <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)(</span><span class="kt">long</span><span class="p">)</span><span class="n">cpu</span><span class="p">,</span>
				   <span class="n">cpu_to_node</span><span class="p">(</span><span class="n">cpu</span><span class="p">),</span>
				   <span class="s">&quot;rcuc/%d&quot;</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">IS_ERR</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">PTR_ERR</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_online</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span>
		<span class="n">kthread_bind</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_cpu</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">);</span>
	<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_PRIO</span><span class="p">;</span>
	<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">SCHED_FIFO</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
	<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="n">t</span><span class="p">;</span>
	<span class="n">wake_up_process</span><span class="p">(</span><span class="n">t</span><span class="p">);</span> <span class="cm">/* Get to TASK_INTERRUPTIBLE quickly. */</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Per-rcu_node kthread, which is in charge of waking up the per-CPU</span>
<span class="cm"> * kthreads when needed.  We ignore requests to wake up kthreads</span>
<span class="cm"> * for offline CPUs, which is OK because force_quiescent_state()</span>
<span class="cm"> * takes care of this case.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_node_kthread</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="p">)</span><span class="n">arg</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">sched_param</span> <span class="n">sp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_status</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_WAITING</span><span class="p">;</span>
		<span class="n">rcu_wait</span><span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">wakemask</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_status</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_RUNNING</span><span class="p">;</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">atomic_xchg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">wakemask</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">rcu_initiate_boost</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span> <span class="cm">/* releases rnp-&gt;lock. */</span>
		<span class="k">for</span> <span class="p">(</span><span class="n">cpu</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">;</span> <span class="n">cpu</span> <span class="o">&lt;=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">;</span> <span class="n">cpu</span><span class="o">++</span><span class="p">,</span> <span class="n">mask</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">if</span> <span class="p">((</span><span class="n">mask</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
				<span class="k">continue</span><span class="p">;</span>
			<span class="n">preempt_disable</span><span class="p">();</span>
			<span class="n">t</span> <span class="o">=</span> <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_kthread_task</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
			<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">cpu_online</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span> <span class="o">||</span> <span class="n">t</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
				<span class="n">preempt_enable</span><span class="p">();</span>
				<span class="k">continue</span><span class="p">;</span>
			<span class="p">}</span>
			<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_has_work</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
			<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_PRIO</span><span class="p">;</span>
			<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">SCHED_FIFO</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
			<span class="n">preempt_enable</span><span class="p">();</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="cm">/* NOTREACHED */</span>
	<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_status</span> <span class="o">=</span> <span class="n">RCU_KTHREAD_STOPPED</span><span class="p">;</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Set the per-rcu_node kthread&#39;s affinity to cover all CPUs that are</span>
<span class="cm"> * served by the rcu_node in question.  The CPU hotplug lock is still</span>
<span class="cm"> * held, so the value of rnp-&gt;qsmaskinit will be stable.</span>
<span class="cm"> *</span>
<span class="cm"> * We don&#39;t include outgoingcpu in the affinity set, use -1 if there is</span>
<span class="cm"> * no outgoing CPU.  If there are no CPUs left in the affinity set,</span>
<span class="cm"> * this function allows the kthread to execute on any CPU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_node_kthread_setaffinity</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">outgoingcpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">cpumask_var_t</span> <span class="n">cm</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_task</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">alloc_cpumask_var</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cm</span><span class="p">,</span> <span class="n">GFP_KERNEL</span><span class="p">))</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">cpumask_clear</span><span class="p">(</span><span class="n">cm</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">cpu</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">;</span> <span class="n">cpu</span> <span class="o">&lt;=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">;</span> <span class="n">cpu</span><span class="o">++</span><span class="p">,</span> <span class="n">mask</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">if</span> <span class="p">((</span><span class="n">mask</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">cpu</span> <span class="o">!=</span> <span class="n">outgoingcpu</span><span class="p">)</span>
			<span class="n">cpumask_set_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">cm</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpumask_weight</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">cpumask_setall</span><span class="p">(</span><span class="n">cm</span><span class="p">);</span>
		<span class="k">for</span> <span class="p">(</span><span class="n">cpu</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">;</span> <span class="n">cpu</span> <span class="o">&lt;=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">;</span> <span class="n">cpu</span><span class="o">++</span><span class="p">)</span>
			<span class="n">cpumask_clear_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">cm</span><span class="p">);</span>
		<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">cpumask_weight</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="n">set_cpus_allowed_ptr</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_task</span><span class="p">,</span> <span class="n">cm</span><span class="p">);</span>
	<span class="n">rcu_boost_kthread_setaffinity</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">cm</span><span class="p">);</span>
	<span class="n">free_cpumask_var</span><span class="p">(</span><span class="n">cm</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Spawn a per-rcu_node kthread, setting priority and affinity.</span>
<span class="cm"> * Called during boot before online/offline can happen, or, if</span>
<span class="cm"> * during runtime, with the main CPU-hotplug locks held.  So only</span>
<span class="cm"> * one of these can be executing at a time.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">__cpuinit</span> <span class="nf">rcu_spawn_one_node_kthread</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
						<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">rnp_index</span> <span class="o">=</span> <span class="n">rnp</span> <span class="o">-</span> <span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
	<span class="k">struct</span> <span class="n">sched_param</span> <span class="n">sp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">t</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_scheduler_fully_active</span> <span class="o">||</span>
	    <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_task</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">t</span> <span class="o">=</span> <span class="n">kthread_create</span><span class="p">(</span><span class="n">rcu_node_kthread</span><span class="p">,</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="n">rnp</span><span class="p">,</span>
				   <span class="s">&quot;rcun/%d&quot;</span><span class="p">,</span> <span class="n">rnp_index</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">IS_ERR</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
			<span class="k">return</span> <span class="n">PTR_ERR</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_task</span> <span class="o">=</span> <span class="n">t</span><span class="p">;</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">sp</span><span class="p">.</span><span class="n">sched_priority</span> <span class="o">=</span> <span class="mi">99</span><span class="p">;</span>
		<span class="n">sched_setscheduler_nocheck</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">SCHED_FIFO</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sp</span><span class="p">);</span>
		<span class="n">wake_up_process</span><span class="p">(</span><span class="n">t</span><span class="p">);</span> <span class="cm">/* get to TASK_INTERRUPTIBLE quickly. */</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">rcu_spawn_one_boost_kthread</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rnp_index</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Spawn all kthreads -- called as soon as the scheduler is running.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">__init</span> <span class="nf">rcu_spawn_kthreads</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="n">rcu_scheduler_fully_active</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">for_each_possible_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_cpu_has_work</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">cpu_online</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span>
			<span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">rcu_spawn_one_cpu_kthread</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rcu_state</span><span class="p">);</span>
	<span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">rcu_spawn_one_node_kthread</span><span class="p">(</span><span class="n">rcu_state</span><span class="p">,</span> <span class="n">rnp</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">NUM_RCU_NODES</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rcu_for_each_leaf_node</span><span class="p">(</span><span class="n">rcu_state</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span>
			<span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">rcu_spawn_one_node_kthread</span><span class="p">(</span><span class="n">rcu_state</span><span class="p">,</span> <span class="n">rnp</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">early_initcall</span><span class="p">(</span><span class="n">rcu_spawn_kthreads</span><span class="p">);</span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">__cpuinit</span> <span class="nf">rcu_prepare_kthreads</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rcu_state</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>

	<span class="cm">/* Fire up the incoming CPU&#39;s kthread and leaf rcu_node kthread. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_scheduler_fully_active</span><span class="p">)</span> <span class="p">{</span>
		<span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">rcu_spawn_one_cpu_kthread</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">node_kthread_task</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
			<span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">rcu_spawn_one_node_kthread</span><span class="p">(</span><span class="n">rcu_state</span><span class="p">,</span> <span class="n">rnp</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_initiate_boost</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">invoke_rcu_callbacks_kthread</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="n">bool</span> <span class="nf">rcu_is_callbacks_kthread</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_preempt_boost_start_gp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_stop_cpu_kthread</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_node_kthread_setaffinity</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">outgoingcpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cpu_kthread_setrt</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">int</span> <span class="n">to_rt</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">int</span> <span class="n">__init</span> <span class="nf">rcu_scheduler_really_started</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_scheduler_fully_active</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">early_initcall</span><span class="p">(</span><span class="n">rcu_scheduler_really_started</span><span class="p">);</span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">__cpuinit</span> <span class="nf">rcu_prepare_kthreads</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

<span class="cp">#if !defined(CONFIG_RCU_FAST_NO_HZ)</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if any future RCU-related work will need to be done</span>
<span class="cm"> * by the current CPU, even if none need be done immediately, returning</span>
<span class="cm"> * 1 if so.  This function is part of the RCU implementation; it is -not-</span>
<span class="cm"> * an exported member of the RCU API.</span>
<span class="cm"> *</span>
<span class="cm"> * Because we not have RCU_FAST_NO_HZ, just check whether this CPU needs</span>
<span class="cm"> * any flavor of RCU.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="nf">rcu_needs_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="o">*</span><span class="n">delta_jiffies</span><span class="p">)</span>
<span class="p">{</span>
	<span class="o">*</span><span class="n">delta_jiffies</span> <span class="o">=</span> <span class="n">ULONG_MAX</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">rcu_cpu_has_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because we do not have RCU_FAST_NO_HZ, don&#39;t bother initializing for it.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_prepare_for_idle_init</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Because we do not have RCU_FAST_NO_HZ, don&#39;t bother cleaning up</span>
<span class="cm"> * after it.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cleanup_after_idle</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Do the idle-entry grace-period work, which, because CONFIG_RCU_FAST_NO_HZ=n,</span>
<span class="cm"> * is nothing.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_prepare_for_idle</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Don&#39;t bother keeping a running count of the number of RCU callbacks</span>
<span class="cm"> * posted because CONFIG_RCU_FAST_NO_HZ=n.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_idle_count_callbacks_posted</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #if !defined(CONFIG_RCU_FAST_NO_HZ) */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * This code is invoked when a CPU goes idle, at which point we want</span>
<span class="cm"> * to have the CPU do everything required for RCU so that it can enter</span>
<span class="cm"> * the energy-efficient dyntick-idle mode.  This is handled by a</span>
<span class="cm"> * state machine implemented by rcu_prepare_for_idle() below.</span>
<span class="cm"> *</span>
<span class="cm"> * The following three proprocessor symbols control this state machine:</span>
<span class="cm"> *</span>
<span class="cm"> * RCU_IDLE_FLUSHES gives the maximum number of times that we will attempt</span>
<span class="cm"> *	to satisfy RCU.  Beyond this point, it is better to incur a periodic</span>
<span class="cm"> *	scheduling-clock interrupt than to loop through the state machine</span>
<span class="cm"> *	at full power.</span>
<span class="cm"> * RCU_IDLE_OPT_FLUSHES gives the number of RCU_IDLE_FLUSHES that are</span>
<span class="cm"> *	optional if RCU does not need anything immediately from this</span>
<span class="cm"> *	CPU, even if this CPU still has RCU callbacks queued.  The first</span>
<span class="cm"> *	times through the state machine are mandatory: we need to give</span>
<span class="cm"> *	the state machine a chance to communicate a quiescent state</span>
<span class="cm"> *	to the RCU core.</span>
<span class="cm"> * RCU_IDLE_GP_DELAY gives the number of jiffies that a CPU is permitted</span>
<span class="cm"> *	to sleep in dyntick-idle mode with RCU callbacks pending.  This</span>
<span class="cm"> *	is sized to be roughly one RCU grace period.  Those energy-efficiency</span>
<span class="cm"> *	benchmarkers who might otherwise be tempted to set this to a large</span>
<span class="cm"> *	number, be warned: Setting RCU_IDLE_GP_DELAY too high can hang your</span>
<span class="cm"> *	system.  And if you are -that- concerned about energy efficiency,</span>
<span class="cm"> *	just power the system down and be done with it!</span>
<span class="cm"> * RCU_IDLE_LAZY_GP_DELAY gives the number of jiffies that a CPU is</span>
<span class="cm"> *	permitted to sleep in dyntick-idle mode with only lazy RCU</span>
<span class="cm"> *	callbacks pending.  Setting this too high can OOM your system.</span>
<span class="cm"> *</span>
<span class="cm"> * The values below work well in practice.  If future workloads require</span>
<span class="cm"> * adjustment, they can be converted into kernel config parameters, though</span>
<span class="cm"> * making the state machine smarter might be a better option.</span>
<span class="cm"> */</span>
<span class="cp">#define RCU_IDLE_FLUSHES 5		</span><span class="cm">/* Number of dyntick-idle tries. */</span><span class="cp"></span>
<span class="cp">#define RCU_IDLE_OPT_FLUSHES 3		</span><span class="cm">/* Optional dyntick-idle tries. */</span><span class="cp"></span>
<span class="cp">#define RCU_IDLE_GP_DELAY 6		</span><span class="cm">/* Roughly one grace period. */</span><span class="cp"></span>
<span class="cp">#define RCU_IDLE_LAZY_GP_DELAY (6 * HZ)	</span><span class="cm">/* Roughly six seconds. */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Does the specified flavor of RCU have non-lazy callbacks pending on</span>
<span class="cm"> * the specified CPU?  Both RCU flavor and CPU are specified by the</span>
<span class="cm"> * rcu_data structure.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="n">bool</span> <span class="nf">__rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">!=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_TREE_PREEMPT_RCU</span>

<span class="cm">/*</span>
<span class="cm"> * Are there non-lazy RCU-preempt callbacks?  (There cannot be if there</span>
<span class="cm"> * is no RCU-preempt in the kernel.)</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="n">bool</span> <span class="nf">rcu_preempt_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="k">return</span> <span class="n">__rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="n">rdp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_TREE_PREEMPT_RCU */</span><span class="cp"></span>

<span class="k">static</span> <span class="n">bool</span> <span class="nf">rcu_preempt_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* else #ifdef CONFIG_TREE_PREEMPT_RCU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Does any flavor of RCU have non-lazy callbacks on the specified CPU?</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="n">bool</span> <span class="nf">rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">__rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">))</span> <span class="o">||</span>
	       <span class="n">__rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">))</span> <span class="o">||</span>
	       <span class="n">rcu_preempt_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Allow the CPU to enter dyntick-idle mode if either: (1) There are no</span>
<span class="cm"> * callbacks on this CPU, (2) this CPU has not yet attempted to enter</span>
<span class="cm"> * dyntick-idle mode, or (3) this CPU is in the process of attempting to</span>
<span class="cm"> * enter dyntick-idle mode.  Otherwise, if we have recently tried and failed</span>
<span class="cm"> * to enter dyntick-idle mode, we refuse to try to enter it.  After all,</span>
<span class="cm"> * it is better to incur scheduling-clock interrupts than to spin</span>
<span class="cm"> * continuously for the same time duration!</span>
<span class="cm"> *</span>
<span class="cm"> * The delta_jiffies argument is used to store the time when RCU is</span>
<span class="cm"> * going to need the CPU again if it still has callbacks.  The reason</span>
<span class="cm"> * for this is that rcu_prepare_for_idle() might need to post a timer,</span>
<span class="cm"> * but if so, it will do so after tick_nohz_stop_sched_tick() has set</span>
<span class="cm"> * the wakeup time for this CPU.  This means that RCU&#39;s timer can be</span>
<span class="cm"> * delayed until the wakeup time, which defeats the purpose of posting</span>
<span class="cm"> * a timer.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="nf">rcu_needs_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="o">*</span><span class="n">delta_jiffies</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="cm">/* Flag a new idle sojourn to the idle-entry state machine. */</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_first_pass</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="cm">/* If no callbacks, RCU doesn&#39;t need the CPU. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_cpu_has_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">delta_jiffies</span> <span class="o">=</span> <span class="n">ULONG_MAX</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">==</span> <span class="n">jiffies</span><span class="p">)</span> <span class="p">{</span>
		<span class="cm">/* RCU recently tried and failed, so don&#39;t try again. */</span>
		<span class="o">*</span><span class="n">delta_jiffies</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="cm">/* Set up for the possibility that RCU will post a timer. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span>
		<span class="o">*</span><span class="n">delta_jiffies</span> <span class="o">=</span> <span class="n">RCU_IDLE_GP_DELAY</span><span class="p">;</span>
	<span class="k">else</span>
		<span class="o">*</span><span class="n">delta_jiffies</span> <span class="o">=</span> <span class="n">RCU_IDLE_LAZY_GP_DELAY</span><span class="p">;</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Handler for smp_call_function_single().  The only point of this</span>
<span class="cm"> * handler is to wake the CPU up, so the handler does only tracing.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_idle_demigrate</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">unused</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Demigrate&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Timer handler used to force CPU to start pushing its remaining RCU</span>
<span class="cm"> * callbacks in the case where it entered dyntick-idle mode with callbacks</span>
<span class="cm"> * pending.  The hander doesn&#39;t really need to do anything because the</span>
<span class="cm"> * real work is done upon re-entry to idle, or by the next scheduling-clock</span>
<span class="cm"> * interrupt should idle not be re-entered.</span>
<span class="cm"> *</span>
<span class="cm"> * One special case: the timer gets migrated without awakening the CPU</span>
<span class="cm"> * on which the timer was scheduled on.  In this case, we must wake up</span>
<span class="cm"> * that CPU.  We do so with smp_call_function_single().</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_idle_gp_timer_func</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">cpu_in</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">cpu_in</span><span class="p">;</span>

	<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Timer&quot;</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu</span> <span class="o">!=</span> <span class="n">smp_processor_id</span><span class="p">())</span>
		<span class="n">smp_call_function_single</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">rcu_idle_demigrate</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="k">else</span>
		<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="cm">/* Getting here can hang the system... */</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Initialize the timer used to pull CPUs out of dyntick-idle mode.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_prepare_for_idle_init</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">setup_timer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer</span><span class="p">,</span> <span class="n">rcu_idle_gp_timer_func</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer_expires</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_first_pass</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Clean up for exit from idle.  Because we are exiting from idle, there</span>
<span class="cm"> * is no longer any point to -&gt;idle_gp_timer, so cancel it.  This will</span>
<span class="cm"> * do nothing if this timer is not active, so just cancel it unconditionally.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cleanup_after_idle</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="n">del_timer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer</span><span class="p">);</span>
	<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Cleanup after idle&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if any RCU-related work can be done by the current CPU,</span>
<span class="cm"> * and if so, schedule a softirq to get it done.  This function is part</span>
<span class="cm"> * of the RCU implementation; it is -not- an exported member of the RCU API.</span>
<span class="cm"> *</span>
<span class="cm"> * The idea is for the current CPU to clear out all work required by the</span>
<span class="cm"> * RCU core for the current grace period, so that this CPU can be permitted</span>
<span class="cm"> * to enter dyntick-idle mode.  In some cases, it will need to be awakened</span>
<span class="cm"> * at the end of the grace period by whatever CPU ends the grace period.</span>
<span class="cm"> * This allows CPUs to go dyntick-idle more quickly, and to reduce the</span>
<span class="cm"> * number of wakeups by a modest integer factor.</span>
<span class="cm"> *</span>
<span class="cm"> * Because it is not legal to invoke rcu_process_callbacks() with irqs</span>
<span class="cm"> * disabled, we do one pass of force_quiescent_state(), then do a</span>
<span class="cm"> * invoke_rcu_core() to cause rcu_process_callbacks() to be invoked</span>
<span class="cm"> * later.  The -&gt;dyntick_drain field controls the sequencing.</span>
<span class="cm"> *</span>
<span class="cm"> * The caller must have disabled interrupts.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_prepare_for_idle</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">timer_list</span> <span class="o">*</span><span class="n">tp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * If this is an idle re-entry, for example, due to use of</span>
<span class="cm">	 * RCU_NONIDLE() or the new idle-loop tracing API within the idle</span>
<span class="cm">	 * loop, then don&#39;t take any state-machine actions, unless the</span>
<span class="cm">	 * momentary exit from idle queued additional non-lazy callbacks.</span>
<span class="cm">	 * Instead, repost the -&gt;idle_gp_timer if this CPU has callbacks</span>
<span class="cm">	 * pending.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_first_pass</span> <span class="o">&amp;&amp;</span>
	    <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">nonlazy_posted</span> <span class="o">==</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">nonlazy_posted_snap</span><span class="p">))</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rcu_cpu_has_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">tp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer</span><span class="p">;</span>
			<span class="n">mod_timer_pinned</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer_expires</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_first_pass</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">nonlazy_posted_snap</span> <span class="o">=</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">nonlazy_posted</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * If there are no callbacks on this CPU, enter dyntick-idle mode.</span>
<span class="cm">	 * Also reset state to avoid prejudicing later attempts.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_cpu_has_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;No callbacks&quot;</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * If in holdoff mode, just return.  We will presumably have</span>
<span class="cm">	 * refrained from disabling the scheduling-clock tick.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">==</span> <span class="n">jiffies</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;In holdoff&quot;</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Check and update the -&gt;dyntick_drain sequencing. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="cm">/* First time through, initialize the counter. */</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span> <span class="o">=</span> <span class="n">RCU_IDLE_FLUSHES</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span> <span class="o">&lt;=</span> <span class="n">RCU_IDLE_OPT_FLUSHES</span> <span class="o">&amp;&amp;</span>
		   <span class="o">!</span><span class="n">rcu_pending</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
		   <span class="o">!</span><span class="n">local_softirq_pending</span><span class="p">())</span> <span class="p">{</span>
		<span class="cm">/* Can we go dyntick-idle despite still having callbacks? */</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">=</span> <span class="n">jiffies</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rcu_cpu_has_nonlazy_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Dyntick with callbacks&quot;</span><span class="p">);</span>
			<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer_expires</span> <span class="o">=</span>
					   <span class="n">jiffies</span> <span class="o">+</span> <span class="n">RCU_IDLE_GP_DELAY</span><span class="p">;</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer_expires</span> <span class="o">=</span>
					   <span class="n">jiffies</span> <span class="o">+</span> <span class="n">RCU_IDLE_LAZY_GP_DELAY</span><span class="p">;</span>
			<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Dyntick with lazy callbacks&quot;</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="n">tp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer</span><span class="p">;</span>
		<span class="n">mod_timer_pinned</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer_expires</span><span class="p">);</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">nonlazy_posted_snap</span> <span class="o">=</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">nonlazy_posted</span><span class="p">;</span>
		<span class="k">return</span><span class="p">;</span> <span class="cm">/* Nothing more to do immediately. */</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="o">--</span><span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="cm">/* We have hit the limit, so time to give up. */</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">=</span> <span class="n">jiffies</span><span class="p">;</span>
		<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Begin holdoff&quot;</span><span class="p">);</span>
		<span class="n">invoke_rcu_core</span><span class="p">();</span>  <span class="cm">/* Force the CPU out of dyntick-idle. */</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Do one step of pushing the remaining RCU callbacks through</span>
<span class="cm">	 * the RCU core state machine.</span>
<span class="cm">	 */</span>
<span class="cp">#ifdef CONFIG_TREE_PREEMPT_RCU</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">nxtlist</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rcu_preempt_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="n">force_quiescent_state</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_preempt_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_TREE_PREEMPT_RCU */</span><span class="cp"></span>
	<span class="k">if</span> <span class="p">(</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">nxtlist</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rcu_sched_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="n">force_quiescent_state</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">nxtlist</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rcu_bh_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="n">force_quiescent_state</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * If RCU callbacks are still pending, RCU still needs this CPU.</span>
<span class="cm">	 * So try forcing the callbacks through the grace period.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_cpu_has_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;More callbacks&quot;</span><span class="p">);</span>
		<span class="n">invoke_rcu_core</span><span class="p">();</span>
	<span class="p">}</span> <span class="k">else</span>
		<span class="n">trace_rcu_prep_idle</span><span class="p">(</span><span class="s">&quot;Callbacks drained&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Keep a running count of the number of non-lazy callbacks posted</span>
<span class="cm"> * on this CPU.  This running counter (which is never decremented) allows</span>
<span class="cm"> * rcu_prepare_for_idle() to detect when something out of the idle loop</span>
<span class="cm"> * posts a callback, even if an equal number of callbacks are invoked.</span>
<span class="cm"> * Of course, callbacks should only be posted from within a trace event</span>
<span class="cm"> * designed to be called from idle or from within RCU_NONIDLE().</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_idle_count_callbacks_posted</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">__this_cpu_add</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">.</span><span class="n">nonlazy_posted</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #if !defined(CONFIG_RCU_FAST_NO_HZ) */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_RCU_CPU_STALL_INFO</span>

<span class="cp">#ifdef CONFIG_RCU_FAST_NO_HZ</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_fast_no_hz</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">cp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">timer_list</span> <span class="o">*</span><span class="n">tltp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">idle_gp_timer</span><span class="p">;</span>

	<span class="n">sprintf</span><span class="p">(</span><span class="n">cp</span><span class="p">,</span> <span class="s">&quot;drain=%d %c timer=%lu&quot;</span><span class="p">,</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_drain</span><span class="p">,</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dyntick_holdoff</span> <span class="o">==</span> <span class="n">jiffies</span> <span class="o">?</span> <span class="sc">&#39;H&#39;</span> <span class="o">:</span> <span class="sc">&#39;.&#39;</span><span class="p">,</span>
		<span class="n">timer_pending</span><span class="p">(</span><span class="n">tltp</span><span class="p">)</span> <span class="o">?</span> <span class="n">tltp</span><span class="o">-&gt;</span><span class="n">expires</span> <span class="o">-</span> <span class="n">jiffies</span> <span class="o">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_FAST_NO_HZ */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_fast_no_hz</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">cp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_FAST_NO_HZ */</span><span class="cp"></span>

<span class="cm">/* Initiate the stall-info list. */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_info_begin</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Print out diagnostic information for the specified stalled CPU.</span>
<span class="cm"> *</span>
<span class="cm"> * If the specified CPU is aware of the current RCU grace period</span>
<span class="cm"> * (flavor specified by rsp), then print the number of scheduling</span>
<span class="cm"> * clock interrupts the CPU has taken during the time that it has</span>
<span class="cm"> * been aware.  Otherwise, print the number of RCU grace periods</span>
<span class="cm"> * that this CPU is ignorant of, for example, &quot;1&quot; if the CPU was</span>
<span class="cm"> * aware of the previous grace period.</span>
<span class="cm"> *</span>
<span class="cm"> * Also print out idle and (if CONFIG_RCU_FAST_NO_HZ) idle-entry info.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_info</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">char</span> <span class="n">fast_no_hz</span><span class="p">[</span><span class="mi">72</span><span class="p">];</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">;</span>
	<span class="kt">char</span> <span class="o">*</span><span class="n">ticks_title</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">ticks_value</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">ticks_title</span> <span class="o">=</span> <span class="s">&quot;ticks this GP&quot;</span><span class="p">;</span>
		<span class="n">ticks_value</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">ticks_this_gp</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="n">ticks_title</span> <span class="o">=</span> <span class="s">&quot;GPs behind&quot;</span><span class="p">;</span>
		<span class="n">ticks_value</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">-</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">print_cpu_stall_fast_no_hz</span><span class="p">(</span><span class="n">fast_no_hz</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_ERR</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">%d: (%lu %s) idle=%03x/%llx/%d %s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
	       <span class="n">cpu</span><span class="p">,</span> <span class="n">ticks_value</span><span class="p">,</span> <span class="n">ticks_title</span><span class="p">,</span>
	       <span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xfff</span><span class="p">,</span>
	       <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nmi_nesting</span><span class="p">,</span>
	       <span class="n">fast_no_hz</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/* Terminate the stall-info list. */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_info_end</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_ERR</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/* Zero -&gt;ticks_this_gp for all flavors of RCU. */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">zero_cpu_stall_ticks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">ticks_this_gp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/* Increment -&gt;ticks_this_gp for all flavors of RCU. */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">increment_cpu_stall_ticks</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">).</span><span class="n">ticks_this_gp</span><span class="o">++</span><span class="p">;</span>
	<span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">).</span><span class="n">ticks_this_gp</span><span class="o">++</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_TREE_PREEMPT_RCU</span>
	<span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_preempt_data</span><span class="p">).</span><span class="n">ticks_this_gp</span><span class="o">++</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_TREE_PREEMPT_RCU */</span><span class="cp"></span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_CPU_STALL_INFO */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_info_begin</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot; {&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_info</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot; %d&quot;</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall_info_end</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot;} &quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">zero_cpu_stall_ticks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">increment_cpu_stall_ticks</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_CPU_STALL_INFO */</span><span class="cp"></span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:1}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../javascript/docco.min.js"></script>
</html>
