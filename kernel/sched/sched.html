<!DOCTYPE html>
<html><head><title>joekychen/linux » kernel › sched › sched.h

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../../index.html"></a><h1>sched.h</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cp">#include &lt;linux/sched.h&gt;</span>
<span class="cp">#include &lt;linux/mutex.h&gt;</span>
<span class="cp">#include &lt;linux/spinlock.h&gt;</span>
<span class="cp">#include &lt;linux/stop_machine.h&gt;</span>

<span class="cp">#include &quot;cpupri.h&quot;</span>

<span class="k">extern</span> <span class="n">__read_mostly</span> <span class="kt">int</span> <span class="n">scheduler_running</span><span class="p">;</span>

<span class="cm">/*</span>
<span class="cm"> * Convert user-nice values [ -20 ... 0 ... 19 ]</span>
<span class="cm"> * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],</span>
<span class="cm"> * and back.</span>
<span class="cm"> */</span>
<span class="cp">#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)</span>
<span class="cp">#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)</span>
<span class="cp">#define TASK_NICE(p)		PRIO_TO_NICE((p)-&gt;static_prio)</span>

<span class="cm">/*</span>
<span class="cm"> * &#39;User priority&#39; is the nice value converted to something we</span>
<span class="cm"> * can work with better when scaling various scheduler parameters,</span>
<span class="cm"> * it&#39;s a [ 0 ... 39 ] range.</span>
<span class="cm"> */</span>
<span class="cp">#define USER_PRIO(p)		((p)-MAX_RT_PRIO)</span>
<span class="cp">#define TASK_USER_PRIO(p)	USER_PRIO((p)-&gt;static_prio)</span>
<span class="cp">#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))</span>

<span class="cm">/*</span>
<span class="cm"> * Helpers for converting nanosecond timing to jiffy resolution</span>
<span class="cm"> */</span>
<span class="cp">#define NS_TO_JIFFIES(TIME)	((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))</span>

<span class="cp">#define NICE_0_LOAD		SCHED_LOAD_SCALE</span>
<span class="cp">#define NICE_0_SHIFT		SCHED_LOAD_SHIFT</span>

<span class="cm">/*</span>
<span class="cm"> * These are the &#39;tuning knobs&#39; of the scheduler:</span>
<span class="cm"> */</span>

<span class="cm">/*</span>
<span class="cm"> * single value that denotes runtime == period, ie unlimited time.</span>
<span class="cm"> */</span>
<span class="cp">#define RUNTIME_INF	((u64)~0ULL)</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rt_policy</span><span class="p">(</span><span class="kt">int</span> <span class="n">policy</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">policy</span> <span class="o">==</span> <span class="n">SCHED_FIFO</span> <span class="o">||</span> <span class="n">policy</span> <span class="o">==</span> <span class="n">SCHED_RR</span><span class="p">)</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">task_has_rt_policy</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rt_policy</span><span class="p">(</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">policy</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * This is the priority-queue data structure of the RT scheduling class:</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">rt_prio_array</span> <span class="p">{</span>
	<span class="n">DECLARE_BITMAP</span><span class="p">(</span><span class="n">bitmap</span><span class="p">,</span> <span class="n">MAX_RT_PRIO</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span> <span class="cm">/* include 1 bit for delimiter */</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">queue</span><span class="p">[</span><span class="n">MAX_RT_PRIO</span><span class="p">];</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">rt_bandwidth</span> <span class="p">{</span>
	<span class="cm">/* nests inside the rq lock: */</span>
	<span class="n">raw_spinlock_t</span>		<span class="n">rt_runtime_lock</span><span class="p">;</span>
	<span class="n">ktime_t</span>			<span class="n">rt_period</span><span class="p">;</span>
	<span class="n">u64</span>			<span class="n">rt_runtime</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">hrtimer</span>		<span class="n">rt_period_timer</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">mutex</span> <span class="n">sched_domains_mutex</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_CGROUP_SCHED</span>

<span class="cp">#include &lt;linux/cgroup.h&gt;</span>

<span class="k">struct</span> <span class="n">cfs_rq</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">rt_rq</span><span class="p">;</span>

<span class="k">static</span> <span class="n">LIST_HEAD</span><span class="p">(</span><span class="n">task_groups</span><span class="p">);</span>

<span class="k">struct</span> <span class="n">cfs_bandwidth</span> <span class="p">{</span>
<span class="cp">#ifdef CONFIG_CFS_BANDWIDTH</span>
	<span class="n">raw_spinlock_t</span> <span class="n">lock</span><span class="p">;</span>
	<span class="n">ktime_t</span> <span class="n">period</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">quota</span><span class="p">,</span> <span class="n">runtime</span><span class="p">;</span>
	<span class="n">s64</span> <span class="n">hierarchal_quota</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">runtime_expires</span><span class="p">;</span>

	<span class="kt">int</span> <span class="n">idle</span><span class="p">,</span> <span class="n">timer_active</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">hrtimer</span> <span class="n">period_timer</span><span class="p">,</span> <span class="n">slack_timer</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">throttled_cfs_rq</span><span class="p">;</span>

	<span class="cm">/* statistics */</span>
	<span class="kt">int</span> <span class="n">nr_periods</span><span class="p">,</span> <span class="n">nr_throttled</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">throttled_time</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="cm">/* task group related information */</span>
<span class="k">struct</span> <span class="n">task_group</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">cgroup_subsys_state</span> <span class="n">css</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_FAIR_GROUP_SCHED</span>
	<span class="cm">/* schedulable entities of this group on each cpu */</span>
	<span class="k">struct</span> <span class="n">sched_entity</span> <span class="o">**</span><span class="n">se</span><span class="p">;</span>
	<span class="cm">/* runqueue &quot;owned&quot; by this group on each cpu */</span>
	<span class="k">struct</span> <span class="n">cfs_rq</span> <span class="o">**</span><span class="n">cfs_rq</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">shares</span><span class="p">;</span>

	<span class="n">atomic_t</span> <span class="n">load_weight</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_RT_GROUP_SCHED</span>
	<span class="k">struct</span> <span class="n">sched_rt_entity</span> <span class="o">**</span><span class="n">rt_se</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rt_rq</span> <span class="o">**</span><span class="n">rt_rq</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">rt_bandwidth</span> <span class="n">rt_bandwidth</span><span class="p">;</span>
<span class="cp">#endif</span>

	<span class="k">struct</span> <span class="n">rcu_head</span> <span class="n">rcu</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">list</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">parent</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">siblings</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">children</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_SCHED_AUTOGROUP</span>
	<span class="k">struct</span> <span class="n">autogroup</span> <span class="o">*</span><span class="n">autogroup</span><span class="p">;</span>
<span class="cp">#endif</span>

	<span class="k">struct</span> <span class="n">cfs_bandwidth</span> <span class="n">cfs_bandwidth</span><span class="p">;</span>
<span class="p">};</span>

<span class="cp">#ifdef CONFIG_FAIR_GROUP_SCHED</span>
<span class="cp">#define ROOT_TASK_GROUP_LOAD	NICE_0_LOAD</span>

<span class="cm">/*</span>
<span class="cm"> * A weight of 0 or 1 can cause arithmetics problems.</span>
<span class="cm"> * A weight of a cfs_rq is the sum of weights of which entities</span>
<span class="cm"> * are queued on this cfs_rq, so a weight of a entity should not be</span>
<span class="cm"> * too large, so as the shares value of a task group.</span>
<span class="cm"> * (The default weight is 1024 - so there&#39;s no practical</span>
<span class="cm"> *  limitation from this.)</span>
<span class="cm"> */</span>
<span class="cp">#define MIN_SHARES	(1UL &lt;&lt;  1)</span>
<span class="cp">#define MAX_SHARES	(1UL &lt;&lt; 18)</span>
<span class="cp">#endif</span>

<span class="cm">/* Default task group.</span>
<span class="cm"> *	Every task in system belong to this group at bootup.</span>
<span class="cm"> */</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">task_group</span> <span class="n">root_task_group</span><span class="p">;</span>

<span class="k">typedef</span> <span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">tg_visitor</span><span class="p">)(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">walk_tg_tree_from</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">from</span><span class="p">,</span>
			     <span class="n">tg_visitor</span> <span class="n">down</span><span class="p">,</span> <span class="n">tg_visitor</span> <span class="n">up</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">data</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Iterate the full tree, calling @down when first entering a node and @up when</span>
<span class="cm"> * leaving it for the final time.</span>
<span class="cm"> *</span>
<span class="cm"> * Caller must hold rcu_lock or sufficient equivalent.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">walk_tg_tree</span><span class="p">(</span><span class="n">tg_visitor</span> <span class="n">down</span><span class="p">,</span> <span class="n">tg_visitor</span> <span class="n">up</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">data</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">walk_tg_tree_from</span><span class="p">(</span><span class="o">&amp;</span><span class="n">root_task_group</span><span class="p">,</span> <span class="n">down</span><span class="p">,</span> <span class="n">up</span><span class="p">,</span> <span class="n">data</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">tg_nop</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">data</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">free_fair_sched_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">alloc_fair_sched_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">parent</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">unregister_fair_sched_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_tg_cfs_entry</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="k">struct</span> <span class="n">cfs_rq</span> <span class="o">*</span><span class="n">cfs_rq</span><span class="p">,</span>
			<span class="k">struct</span> <span class="n">sched_entity</span> <span class="o">*</span><span class="n">se</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span>
			<span class="k">struct</span> <span class="n">sched_entity</span> <span class="o">*</span><span class="n">parent</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_cfs_bandwidth</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_bandwidth</span> <span class="o">*</span><span class="n">cfs_b</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">sched_group_set_shares</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">shares</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">__refill_cfs_bandwidth_runtime</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_bandwidth</span> <span class="o">*</span><span class="n">cfs_b</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">__start_cfs_bandwidth</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_bandwidth</span> <span class="o">*</span><span class="n">cfs_b</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">unthrottle_cfs_rq</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_rq</span> <span class="o">*</span><span class="n">cfs_rq</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">free_rt_sched_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">alloc_rt_sched_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">parent</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_tg_rt_entry</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rt_rq</span> <span class="o">*</span><span class="n">rt_rq</span><span class="p">,</span>
		<span class="k">struct</span> <span class="n">sched_rt_entity</span> <span class="o">*</span><span class="n">rt_se</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span>
		<span class="k">struct</span> <span class="n">sched_rt_entity</span> <span class="o">*</span><span class="n">parent</span><span class="p">);</span>

<span class="cp">#else </span><span class="cm">/* CONFIG_CGROUP_SCHED */</span><span class="cp"></span>

<span class="k">struct</span> <span class="n">cfs_bandwidth</span> <span class="p">{</span> <span class="p">};</span>

<span class="cp">#endif	</span><span class="cm">/* CONFIG_CGROUP_SCHED */</span><span class="cp"></span>

<span class="cm">/* CFS-related fields in a runqueue */</span>
<span class="k">struct</span> <span class="n">cfs_rq</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">load_weight</span> <span class="n">load</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">nr_running</span><span class="p">,</span> <span class="n">h_nr_running</span><span class="p">;</span>

	<span class="n">u64</span> <span class="n">exec_clock</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">min_vruntime</span><span class="p">;</span>
<span class="cp">#ifndef CONFIG_64BIT</span>
	<span class="n">u64</span> <span class="n">min_vruntime_copy</span><span class="p">;</span>
<span class="cp">#endif</span>

	<span class="k">struct</span> <span class="n">rb_root</span> <span class="n">tasks_timeline</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rb_node</span> <span class="o">*</span><span class="n">rb_leftmost</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * &#39;curr&#39; points to currently running entity on this cfs_rq.</span>
<span class="cm">	 * It is set to NULL otherwise (i.e when none are currently running).</span>
<span class="cm">	 */</span>
	<span class="k">struct</span> <span class="n">sched_entity</span> <span class="o">*</span><span class="n">curr</span><span class="p">,</span> <span class="o">*</span><span class="n">next</span><span class="p">,</span> <span class="o">*</span><span class="n">last</span><span class="p">,</span> <span class="o">*</span><span class="n">skip</span><span class="p">;</span>

<span class="cp">#ifdef	CONFIG_SCHED_DEBUG</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">nr_spread_over</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_FAIR_GROUP_SCHED</span>
	<span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">;</span>	<span class="cm">/* cpu runqueue to which this cfs_rq is attached */</span>

	<span class="cm">/*</span>
<span class="cm">	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in</span>
<span class="cm">	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities</span>
<span class="cm">	 * (like users, containers etc.)</span>
<span class="cm">	 *</span>
<span class="cm">	 * leaf_cfs_rq_list ties together list of leaf cfs_rq&#39;s in a cpu. This</span>
<span class="cm">	 * list is used during load balance.</span>
<span class="cm">	 */</span>
	<span class="kt">int</span> <span class="n">on_list</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">leaf_cfs_rq_list</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">;</span>	<span class="cm">/* group that &quot;owns&quot; this runqueue */</span>

<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="cm">/*</span>
<span class="cm">	 *   h_load = weight * f(tg)</span>
<span class="cm">	 *</span>
<span class="cm">	 * Where f(tg) is the recursive weight fraction assigned to</span>
<span class="cm">	 * this group.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">h_load</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * Maintaining per-cpu shares distribution for group scheduling</span>
<span class="cm">	 *</span>
<span class="cm">	 * load_stamp is the last time we updated the load average</span>
<span class="cm">	 * load_last is the last time we updated the load average and saw load</span>
<span class="cm">	 * load_unacc_exec_time is currently unaccounted execution time</span>
<span class="cm">	 */</span>
	<span class="n">u64</span> <span class="n">load_avg</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">load_period</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">load_stamp</span><span class="p">,</span> <span class="n">load_last</span><span class="p">,</span> <span class="n">load_unacc_exec_time</span><span class="p">;</span>

	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">load_contribution</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_SMP */</span><span class="cp"></span>
<span class="cp">#ifdef CONFIG_CFS_BANDWIDTH</span>
	<span class="kt">int</span> <span class="n">runtime_enabled</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">runtime_expires</span><span class="p">;</span>
	<span class="n">s64</span> <span class="n">runtime_remaining</span><span class="p">;</span>

	<span class="n">u64</span> <span class="n">throttled_timestamp</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">throttled</span><span class="p">,</span> <span class="n">throttle_count</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">throttled_list</span><span class="p">;</span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_CFS_BANDWIDTH */</span><span class="cp"></span>
<span class="cp">#endif </span><span class="cm">/* CONFIG_FAIR_GROUP_SCHED */</span><span class="cp"></span>
<span class="p">};</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rt_bandwidth_enabled</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">sysctl_sched_rt_runtime</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/* Real-Time classes&#39; related field in a runqueue: */</span>
<span class="k">struct</span> <span class="n">rt_rq</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">rt_prio_array</span> <span class="n">active</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">rt_nr_running</span><span class="p">;</span>
<span class="cp">#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED</span>
	<span class="k">struct</span> <span class="p">{</span>
		<span class="kt">int</span> <span class="n">curr</span><span class="p">;</span> <span class="cm">/* highest queued rt task prio */</span>
<span class="cp">#ifdef CONFIG_SMP</span>
		<span class="kt">int</span> <span class="n">next</span><span class="p">;</span> <span class="cm">/* next highest */</span>
<span class="cp">#endif</span>
	<span class="p">}</span> <span class="n">highest_prio</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">rt_nr_migratory</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">rt_nr_total</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">overloaded</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">plist_head</span> <span class="n">pushable_tasks</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="kt">int</span> <span class="n">rt_throttled</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">rt_time</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">rt_runtime</span><span class="p">;</span>
	<span class="cm">/* Nests inside the rq lock: */</span>
	<span class="n">raw_spinlock_t</span> <span class="n">rt_runtime_lock</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_RT_GROUP_SCHED</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">rt_nr_boosted</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">leaf_rt_rq_list</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="cp">#ifdef CONFIG_SMP</span>

<span class="cm">/*</span>
<span class="cm"> * We add the notion of a root-domain which will be used to define per-domain</span>
<span class="cm"> * variables. Each exclusive cpuset essentially defines an island domain by</span>
<span class="cm"> * fully partitioning the member cpus from any other cpuset. Whenever a new</span>
<span class="cm"> * exclusive cpuset is created, we also create and attach a new root-domain</span>
<span class="cm"> * object.</span>
<span class="cm"> *</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">root_domain</span> <span class="p">{</span>
	<span class="n">atomic_t</span> <span class="n">refcount</span><span class="p">;</span>
	<span class="n">atomic_t</span> <span class="n">rto_count</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_head</span> <span class="n">rcu</span><span class="p">;</span>
	<span class="n">cpumask_var_t</span> <span class="n">span</span><span class="p">;</span>
	<span class="n">cpumask_var_t</span> <span class="n">online</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * The &quot;RT overload&quot; flag: it gets set if a CPU has more than</span>
<span class="cm">	 * one runnable RT task.</span>
<span class="cm">	 */</span>
	<span class="n">cpumask_var_t</span> <span class="n">rto_mask</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">cpupri</span> <span class="n">cpupri</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">root_domain</span> <span class="n">def_root_domain</span><span class="p">;</span>

<span class="cp">#endif </span><span class="cm">/* CONFIG_SMP */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * This is the main, per-CPU runqueue data structure.</span>
<span class="cm"> *</span>
<span class="cm"> * Locking rule: those places that want to lock multiple runqueues</span>
<span class="cm"> * (such as the load balancing or the thread migration code), lock</span>
<span class="cm"> * acquire operations must be ordered by ascending &amp;runqueue.</span>
<span class="cm"> */</span>
<span class="k">struct</span> <span class="n">rq</span> <span class="p">{</span>
	<span class="cm">/* runqueue lock: */</span>
	<span class="n">raw_spinlock_t</span> <span class="n">lock</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * nr_running and cpu_load should be in the same cacheline because</span>
<span class="cm">	 * remote CPUs use both these fields when doing load calculation.</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">nr_running</span><span class="p">;</span>
	<span class="cp">#define CPU_LOAD_IDX_MAX 5</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">cpu_load</span><span class="p">[</span><span class="n">CPU_LOAD_IDX_MAX</span><span class="p">];</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">last_load_update_tick</span><span class="p">;</span>
<span class="cp">#ifdef CONFIG_NO_HZ</span>
	<span class="n">u64</span> <span class="n">nohz_stamp</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nohz_flags</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="kt">int</span> <span class="n">skip_clock_update</span><span class="p">;</span>

	<span class="cm">/* capture load from *all* tasks on this cpu: */</span>
	<span class="k">struct</span> <span class="n">load_weight</span> <span class="n">load</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nr_load_updates</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">nr_switches</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">cfs_rq</span> <span class="n">cfs</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rt_rq</span> <span class="n">rt</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_FAIR_GROUP_SCHED</span>
	<span class="cm">/* list of leaf cfs_rq on this cpu: */</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">leaf_cfs_rq_list</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_RT_GROUP_SCHED</span>
	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">leaf_rt_rq_list</span><span class="p">;</span>
<span class="cp">#endif</span>

	<span class="cm">/*</span>
<span class="cm">	 * This is part of a global counter where only the total sum</span>
<span class="cm">	 * over all CPUs matters. A task can increase this counter on</span>
<span class="cm">	 * one CPU and if it got migrated afterwards it may decrease</span>
<span class="cm">	 * it on another CPU. Always updated under the runqueue lock:</span>
<span class="cm">	 */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nr_uninterruptible</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">curr</span><span class="p">,</span> <span class="o">*</span><span class="n">idle</span><span class="p">,</span> <span class="o">*</span><span class="n">stop</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">next_balance</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">mm_struct</span> <span class="o">*</span><span class="n">prev_mm</span><span class="p">;</span>

	<span class="n">u64</span> <span class="n">clock</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">clock_task</span><span class="p">;</span>

	<span class="n">atomic_t</span> <span class="n">nr_iowait</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="k">struct</span> <span class="n">root_domain</span> <span class="o">*</span><span class="n">rd</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">sched_domain</span> <span class="o">*</span><span class="n">sd</span><span class="p">;</span>

	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">cpu_power</span><span class="p">;</span>

	<span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">idle_balance</span><span class="p">;</span>
	<span class="cm">/* For active balancing */</span>
	<span class="kt">int</span> <span class="n">post_schedule</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">active_balance</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">push_cpu</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">cpu_stop_work</span> <span class="n">active_balance_work</span><span class="p">;</span>
	<span class="cm">/* cpu of this runqueue: */</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">online</span><span class="p">;</span>

	<span class="k">struct</span> <span class="n">list_head</span> <span class="n">cfs_tasks</span><span class="p">;</span>

	<span class="n">u64</span> <span class="n">rt_avg</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">age_stamp</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">idle_stamp</span><span class="p">;</span>
	<span class="n">u64</span> <span class="n">avg_idle</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_IRQ_TIME_ACCOUNTING</span>
	<span class="n">u64</span> <span class="n">prev_irq_time</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_PARAVIRT</span>
	<span class="n">u64</span> <span class="n">prev_steal_time</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING</span>
	<span class="n">u64</span> <span class="n">prev_steal_time_rq</span><span class="p">;</span>
<span class="cp">#endif</span>

	<span class="cm">/* calc_load related fields */</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">calc_load_update</span><span class="p">;</span>
	<span class="kt">long</span> <span class="n">calc_load_active</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_SCHED_HRTICK</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="kt">int</span> <span class="n">hrtick_csd_pending</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">call_single_data</span> <span class="n">hrtick_csd</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="k">struct</span> <span class="n">hrtimer</span> <span class="n">hrtick_timer</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_SCHEDSTATS</span>
	<span class="cm">/* latency stats */</span>
	<span class="k">struct</span> <span class="n">sched_info</span> <span class="n">rq_sched_info</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="n">rq_cpu_time</span><span class="p">;</span>
	<span class="cm">/* could above be rq-&gt;cfs_rq.exec_clock + rq-&gt;rt_rq.rt_runtime ? */</span>

	<span class="cm">/* sys_sched_yield() stats */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">yld_count</span><span class="p">;</span>

	<span class="cm">/* schedule() stats */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sched_count</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sched_goidle</span><span class="p">;</span>

	<span class="cm">/* try_to_wake_up() stats */</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">ttwu_count</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">ttwu_local</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="k">struct</span> <span class="n">llist_head</span> <span class="n">wake_list</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">};</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">cpu_of</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="k">return</span> <span class="n">rq</span><span class="o">-&gt;</span><span class="n">cpu</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="n">DECLARE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span><span class="p">,</span> <span class="n">runqueues</span><span class="p">);</span>

<span class="cp">#define cpu_rq(cpu)		(&amp;per_cpu(runqueues, (cpu)))</span>
<span class="cp">#define this_rq()		(&amp;__get_cpu_var(runqueues))</span>
<span class="cp">#define task_rq(p)		cpu_rq(task_cpu(p))</span>
<span class="cp">#define cpu_curr(cpu)		(cpu_rq(cpu)-&gt;curr)</span>
<span class="cp">#define raw_rq()		(&amp;__raw_get_cpu_var(runqueues))</span>

<span class="cp">#ifdef CONFIG_SMP</span>

<span class="cp">#define rcu_dereference_check_sched_domain(p) \</span>
<span class="cp">	rcu_dereference_check((p), \</span>
<span class="cp">			      lockdep_is_held(&amp;sched_domains_mutex))</span>

<span class="cm">/*</span>
<span class="cm"> * The domain tree (rq-&gt;sd) is protected by RCU&#39;s quiescent state transition.</span>
<span class="cm"> * See detach_destroy_domains: synchronize_sched for details.</span>
<span class="cm"> *</span>
<span class="cm"> * The domain tree of any CPU may only be accessed from within</span>
<span class="cm"> * preempt-disabled sections.</span>
<span class="cm"> */</span>
<span class="cp">#define for_each_domain(cpu, __sd) \</span>
<span class="cp">	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)-&gt;sd); \</span>
<span class="cp">			__sd; __sd = __sd-&gt;parent)</span>

<span class="cp">#define for_each_lower_domain(sd) for (; sd; sd = sd-&gt;child)</span>

<span class="cm">/**</span>
<span class="cm"> * highest_flag_domain - Return highest sched_domain containing flag.</span>
<span class="cm"> * @cpu:	The cpu whose highest level of sched domain is to</span>
<span class="cm"> *		be returned.</span>
<span class="cm"> * @flag:	The flag to check for the highest sched_domain</span>
<span class="cm"> *		for the given cpu.</span>
<span class="cm"> *</span>
<span class="cm"> * Returns the highest sched_domain of a cpu which contains the given flag.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">sched_domain</span> <span class="o">*</span><span class="nf">highest_flag_domain</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">int</span> <span class="n">flag</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">sched_domain</span> <span class="o">*</span><span class="n">sd</span><span class="p">,</span> <span class="o">*</span><span class="n">hsd</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>

	<span class="n">for_each_domain</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">sd</span><span class="o">-&gt;</span><span class="n">flags</span> <span class="o">&amp;</span> <span class="n">flag</span><span class="p">))</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="n">hsd</span> <span class="o">=</span> <span class="n">sd</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="k">return</span> <span class="n">hsd</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">DECLARE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">sched_domain</span> <span class="o">*</span><span class="p">,</span> <span class="n">sd_llc</span><span class="p">);</span>
<span class="n">DECLARE_PER_CPU</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span> <span class="n">sd_llc_id</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">int</span> <span class="n">group_balance_cpu</span><span class="p">(</span><span class="k">struct</span> <span class="n">sched_group</span> <span class="o">*</span><span class="n">sg</span><span class="p">);</span>

<span class="cp">#endif </span><span class="cm">/* CONFIG_SMP */</span><span class="cp"></span>

<span class="cp">#include &quot;stats.h&quot;</span>
<span class="cp">#include &quot;auto_group.h&quot;</span>

<span class="cp">#ifdef CONFIG_CGROUP_SCHED</span>

<span class="cm">/*</span>
<span class="cm"> * Return the group to which this tasks belongs.</span>
<span class="cm"> *</span>
<span class="cm"> * We use task_subsys_state_check() and extend the RCU verification with</span>
<span class="cm"> * pi-&gt;lock and rq-&gt;lock because cpu_cgroup_attach() holds those locks for each</span>
<span class="cm"> * task it moves into the cgroup. Therefore by holding either of those locks,</span>
<span class="cm"> * we pin the task to the current cgroup.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="nf">task_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">cgroup_subsys_state</span> <span class="o">*</span><span class="n">css</span><span class="p">;</span>

	<span class="n">css</span> <span class="o">=</span> <span class="n">task_subsys_state_check</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">cpu_cgroup_subsys_id</span><span class="p">,</span>
			<span class="n">lockdep_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">pi_lock</span><span class="p">)</span> <span class="o">||</span>
			<span class="n">lockdep_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">task_rq</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">));</span>
	<span class="n">tg</span> <span class="o">=</span> <span class="n">container_of</span><span class="p">(</span><span class="n">css</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_group</span><span class="p">,</span> <span class="n">css</span><span class="p">);</span>

	<span class="k">return</span> <span class="n">autogroup_task_group</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tg</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/* Change a task&#39;s cfs_rq and parent entity if it moves across CPUs/groups */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">set_task_rq</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#if defined(CONFIG_FAIR_GROUP_SCHED) || defined(CONFIG_RT_GROUP_SCHED)</span>
	<span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="n">tg</span> <span class="o">=</span> <span class="n">task_group</span><span class="p">(</span><span class="n">p</span><span class="p">);</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_FAIR_GROUP_SCHED</span>
	<span class="n">p</span><span class="o">-&gt;</span><span class="n">se</span><span class="p">.</span><span class="n">cfs_rq</span> <span class="o">=</span> <span class="n">tg</span><span class="o">-&gt;</span><span class="n">cfs_rq</span><span class="p">[</span><span class="n">cpu</span><span class="p">];</span>
	<span class="n">p</span><span class="o">-&gt;</span><span class="n">se</span><span class="p">.</span><span class="n">parent</span> <span class="o">=</span> <span class="n">tg</span><span class="o">-&gt;</span><span class="n">se</span><span class="p">[</span><span class="n">cpu</span><span class="p">];</span>
<span class="cp">#endif</span>

<span class="cp">#ifdef CONFIG_RT_GROUP_SCHED</span>
	<span class="n">p</span><span class="o">-&gt;</span><span class="n">rt</span><span class="p">.</span><span class="n">rt_rq</span>  <span class="o">=</span> <span class="n">tg</span><span class="o">-&gt;</span><span class="n">rt_rq</span><span class="p">[</span><span class="n">cpu</span><span class="p">];</span>
	<span class="n">p</span><span class="o">-&gt;</span><span class="n">rt</span><span class="p">.</span><span class="n">parent</span> <span class="o">=</span> <span class="n">tg</span><span class="o">-&gt;</span><span class="n">rt_se</span><span class="p">[</span><span class="n">cpu</span><span class="p">];</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* CONFIG_CGROUP_SCHED */</span><span class="cp"></span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">set_task_rq</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span> <span class="p">{</span> <span class="p">}</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">task_group</span> <span class="o">*</span><span class="nf">task_group</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* CONFIG_CGROUP_SCHED */</span><span class="cp"></span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">__set_task_cpu</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">set_task_rq</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="cm">/*</span>
<span class="cm">	 * After -&gt;cpu is set up to a new value, task_rq_lock(p, ...) can be</span>
<span class="cm">	 * successfuly executed on another CPU. We must ensure that updates of</span>
<span class="cm">	 * per-task data have been completed by this moment.</span>
<span class="cm">	 */</span>
	<span class="n">smp_wmb</span><span class="p">();</span>
	<span class="n">task_thread_info</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">cpu</span> <span class="o">=</span> <span class="n">cpu</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Tunables that become constants when CONFIG_SCHED_DEBUG is off:</span>
<span class="cm"> */</span>
<span class="cp">#ifdef CONFIG_SCHED_DEBUG</span>
<span class="cp"># include &lt;linux/static_key.h&gt;</span>
<span class="cp"># define const_debug __read_mostly</span>
<span class="cp">#else</span>
<span class="cp"># define const_debug const</span>
<span class="cp">#endif</span>

<span class="k">extern</span> <span class="n">const_debug</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sysctl_sched_features</span><span class="p">;</span>

<span class="cp">#define SCHED_FEAT(name, enabled)	\</span>
<span class="cp">	__SCHED_FEAT_##name ,</span>

<span class="k">enum</span> <span class="p">{</span>
<span class="cp">#include &quot;features.h&quot;</span>
	<span class="n">__SCHED_FEAT_NR</span><span class="p">,</span>
<span class="p">};</span>

<span class="cp">#undef SCHED_FEAT</span>

<span class="cp">#if defined(CONFIG_SCHED_DEBUG) &amp;&amp; defined(HAVE_JUMP_LABEL)</span>
<span class="k">static</span> <span class="n">__always_inline</span> <span class="n">bool</span> <span class="nf">static_branch__true</span><span class="p">(</span><span class="k">struct</span> <span class="n">static_key</span> <span class="o">*</span><span class="n">key</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">static_key_true</span><span class="p">(</span><span class="n">key</span><span class="p">);</span> <span class="cm">/* Not out of line branch. */</span>
<span class="p">}</span>

<span class="k">static</span> <span class="n">__always_inline</span> <span class="n">bool</span> <span class="nf">static_branch__false</span><span class="p">(</span><span class="k">struct</span> <span class="n">static_key</span> <span class="o">*</span><span class="n">key</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">static_key_false</span><span class="p">(</span><span class="n">key</span><span class="p">);</span> <span class="cm">/* Out of line branch. */</span>
<span class="p">}</span>

<span class="cp">#define SCHED_FEAT(name, enabled)					\</span>
<span class="cp">static __always_inline bool static_branch_##name(struct static_key *key) \</span>
<span class="cp">{									\</span>
<span class="cp">	return static_branch__##enabled(key);				\</span>
<span class="cp">}</span>

<span class="cp">#include &quot;features.h&quot;</span>

<span class="cp">#undef SCHED_FEAT</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">static_key</span> <span class="n">sched_feat_keys</span><span class="p">[</span><span class="n">__SCHED_FEAT_NR</span><span class="p">];</span>
<span class="cp">#define sched_feat(x) (static_branch_##x(&amp;sched_feat_keys[__SCHED_FEAT_##x]))</span>
<span class="cp">#else </span><span class="cm">/* !(SCHED_DEBUG &amp;&amp; HAVE_JUMP_LABEL) */</span><span class="cp"></span>
<span class="cp">#define sched_feat(x) (sysctl_sched_features &amp; (1UL &lt;&lt; __SCHED_FEAT_##x))</span>
<span class="cp">#endif </span><span class="cm">/* SCHED_DEBUG &amp;&amp; HAVE_JUMP_LABEL */</span><span class="cp"></span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">u64</span> <span class="nf">global_rt_period</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">u64</span><span class="p">)</span><span class="n">sysctl_sched_rt_period</span> <span class="o">*</span> <span class="n">NSEC_PER_USEC</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">u64</span> <span class="nf">global_rt_runtime</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">sysctl_sched_rt_runtime</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">RUNTIME_INF</span><span class="p">;</span>

	<span class="k">return</span> <span class="p">(</span><span class="n">u64</span><span class="p">)</span><span class="n">sysctl_sched_rt_runtime</span> <span class="o">*</span> <span class="n">NSEC_PER_USEC</span><span class="p">;</span>
<span class="p">}</span>



<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">task_current</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rq</span><span class="o">-&gt;</span><span class="n">curr</span> <span class="o">==</span> <span class="n">p</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">task_running</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="k">return</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">on_cpu</span><span class="p">;</span>
<span class="cp">#else</span>
	<span class="k">return</span> <span class="n">task_current</span><span class="p">(</span><span class="n">rq</span><span class="p">,</span> <span class="n">p</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="p">}</span>


<span class="cp">#ifndef prepare_arch_switch</span>
<span class="cp"># define prepare_arch_switch(next)	do { } while (0)</span>
<span class="cp">#endif</span>
<span class="cp">#ifndef finish_arch_switch</span>
<span class="cp"># define finish_arch_switch(prev)	do { } while (0)</span>
<span class="cp">#endif</span>
<span class="cp">#ifndef finish_arch_post_lock_switch</span>
<span class="cp"># define finish_arch_post_lock_switch()	do { } while (0)</span>
<span class="cp">#endif</span>

<span class="cp">#ifndef __ARCH_WANT_UNLOCKED_CTXSW</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">prepare_lock_switch</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">next</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="cm">/*</span>
<span class="cm">	 * We can optimise this out completely for !SMP, because the</span>
<span class="cm">	 * SMP rebalancing from interrupt is the only thing that cares</span>
<span class="cm">	 * here.</span>
<span class="cm">	 */</span>
	<span class="n">next</span><span class="o">-&gt;</span><span class="n">on_cpu</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">finish_lock_switch</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">prev</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="cm">/*</span>
<span class="cm">	 * After -&gt;on_cpu is cleared, the task can be moved to a different CPU.</span>
<span class="cm">	 * We must ensure this doesn&#39;t happen until the switch is completely</span>
<span class="cm">	 * finished.</span>
<span class="cm">	 */</span>
	<span class="n">smp_wmb</span><span class="p">();</span>
	<span class="n">prev</span><span class="o">-&gt;</span><span class="n">on_cpu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef CONFIG_DEBUG_SPINLOCK</span>
	<span class="cm">/* this is a valid case when another task releases the spinlock */</span>
	<span class="n">rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">.</span><span class="n">owner</span> <span class="o">=</span> <span class="n">current</span><span class="p">;</span>
<span class="cp">#endif</span>
	<span class="cm">/*</span>
<span class="cm">	 * If we are tracking spinlock dependencies then we have to</span>
<span class="cm">	 * fix up the runqueue lock - which gets &#39;carried over&#39; from</span>
<span class="cm">	 * prev into current:</span>
<span class="cm">	 */</span>
	<span class="n">spin_acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">.</span><span class="n">dep_map</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">_THIS_IP_</span><span class="p">);</span>

	<span class="n">raw_spin_unlock_irq</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* __ARCH_WANT_UNLOCKED_CTXSW */</span><span class="cp"></span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">prepare_lock_switch</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">next</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="cm">/*</span>
<span class="cm">	 * We can optimise this out completely for !SMP, because the</span>
<span class="cm">	 * SMP rebalancing from interrupt is the only thing that cares</span>
<span class="cm">	 * here.</span>
<span class="cm">	 */</span>
	<span class="n">next</span><span class="o">-&gt;</span><span class="n">on_cpu</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW</span>
	<span class="n">raw_spin_unlock_irq</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
<span class="cp">#else</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">finish_lock_switch</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">prev</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_SMP</span>
	<span class="cm">/*</span>
<span class="cm">	 * After -&gt;on_cpu is cleared, the task can be moved to a different CPU.</span>
<span class="cm">	 * We must ensure this doesn&#39;t happen until the switch is completely</span>
<span class="cm">	 * finished.</span>
<span class="cm">	 */</span>
	<span class="n">smp_wmb</span><span class="p">();</span>
	<span class="n">prev</span><span class="o">-&gt;</span><span class="n">on_cpu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="cp">#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW</span>
	<span class="n">local_irq_enable</span><span class="p">();</span>
<span class="cp">#endif</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* __ARCH_WANT_UNLOCKED_CTXSW */</span><span class="cp"></span>


<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">update_load_add</span><span class="p">(</span><span class="k">struct</span> <span class="n">load_weight</span> <span class="o">*</span><span class="n">lw</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">inc</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">lw</span><span class="o">-&gt;</span><span class="n">weight</span> <span class="o">+=</span> <span class="n">inc</span><span class="p">;</span>
	<span class="n">lw</span><span class="o">-&gt;</span><span class="n">inv_weight</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">update_load_sub</span><span class="p">(</span><span class="k">struct</span> <span class="n">load_weight</span> <span class="o">*</span><span class="n">lw</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">dec</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">lw</span><span class="o">-&gt;</span><span class="n">weight</span> <span class="o">-=</span> <span class="n">dec</span><span class="p">;</span>
	<span class="n">lw</span><span class="o">-&gt;</span><span class="n">inv_weight</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">update_load_set</span><span class="p">(</span><span class="k">struct</span> <span class="n">load_weight</span> <span class="o">*</span><span class="n">lw</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">w</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">lw</span><span class="o">-&gt;</span><span class="n">weight</span> <span class="o">=</span> <span class="n">w</span><span class="p">;</span>
	<span class="n">lw</span><span class="o">-&gt;</span><span class="n">inv_weight</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * To aid in avoiding the subversion of &quot;niceness&quot; due to uneven distribution</span>
<span class="cm"> * of tasks with abnormal &quot;nice&quot; values across CPUs the contribution that</span>
<span class="cm"> * each task makes to its run queue&#39;s load is weighted according to its</span>
<span class="cm"> * scheduling class and &quot;nice&quot; value. For SCHED_NORMAL tasks this is just a</span>
<span class="cm"> * scaled version of the new time slice allocation that they receive on time</span>
<span class="cm"> * slice expiry etc.</span>
<span class="cm"> */</span>

<span class="cp">#define WEIGHT_IDLEPRIO                3</span>
<span class="cp">#define WMULT_IDLEPRIO         1431655765</span>

<span class="cm">/*</span>
<span class="cm"> * Nice levels are multiplicative, with a gentle 10% change for every</span>
<span class="cm"> * nice level changed. I.e. when a CPU-bound task goes from nice 0 to</span>
<span class="cm"> * nice 1, it will get ~10% less CPU time than another CPU-bound task</span>
<span class="cm"> * that remained on nice 0.</span>
<span class="cm"> *</span>
<span class="cm"> * The &quot;10% effect&quot; is relative and cumulative: from _any_ nice level,</span>
<span class="cm"> * if you go up 1 level, it&#39;s -10% CPU usage, if you go down 1 level</span>
<span class="cm"> * it&#39;s +10% CPU usage. (to achieve that we use a multiplier of 1.25.</span>
<span class="cm"> * If a task goes up by ~10% and another task goes down by ~10% then</span>
<span class="cm"> * the relative distance between them is ~25%.)</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">prio_to_weight</span><span class="p">[</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
 <span class="cm">/* -20 */</span>     <span class="mi">88761</span><span class="p">,</span>     <span class="mi">71755</span><span class="p">,</span>     <span class="mi">56483</span><span class="p">,</span>     <span class="mi">46273</span><span class="p">,</span>     <span class="mi">36291</span><span class="p">,</span>
 <span class="cm">/* -15 */</span>     <span class="mi">29154</span><span class="p">,</span>     <span class="mi">23254</span><span class="p">,</span>     <span class="mi">18705</span><span class="p">,</span>     <span class="mi">14949</span><span class="p">,</span>     <span class="mi">11916</span><span class="p">,</span>
 <span class="cm">/* -10 */</span>      <span class="mi">9548</span><span class="p">,</span>      <span class="mi">7620</span><span class="p">,</span>      <span class="mi">6100</span><span class="p">,</span>      <span class="mi">4904</span><span class="p">,</span>      <span class="mi">3906</span><span class="p">,</span>
 <span class="cm">/*  -5 */</span>      <span class="mi">3121</span><span class="p">,</span>      <span class="mi">2501</span><span class="p">,</span>      <span class="mi">1991</span><span class="p">,</span>      <span class="mi">1586</span><span class="p">,</span>      <span class="mi">1277</span><span class="p">,</span>
 <span class="cm">/*   0 */</span>      <span class="mi">1024</span><span class="p">,</span>       <span class="mi">820</span><span class="p">,</span>       <span class="mi">655</span><span class="p">,</span>       <span class="mi">526</span><span class="p">,</span>       <span class="mi">423</span><span class="p">,</span>
 <span class="cm">/*   5 */</span>       <span class="mi">335</span><span class="p">,</span>       <span class="mi">272</span><span class="p">,</span>       <span class="mi">215</span><span class="p">,</span>       <span class="mi">172</span><span class="p">,</span>       <span class="mi">137</span><span class="p">,</span>
 <span class="cm">/*  10 */</span>       <span class="mi">110</span><span class="p">,</span>        <span class="mi">87</span><span class="p">,</span>        <span class="mi">70</span><span class="p">,</span>        <span class="mi">56</span><span class="p">,</span>        <span class="mi">45</span><span class="p">,</span>
 <span class="cm">/*  15 */</span>        <span class="mi">36</span><span class="p">,</span>        <span class="mi">29</span><span class="p">,</span>        <span class="mi">23</span><span class="p">,</span>        <span class="mi">18</span><span class="p">,</span>        <span class="mi">15</span><span class="p">,</span>
<span class="p">};</span>

<span class="cm">/*</span>
<span class="cm"> * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.</span>
<span class="cm"> *</span>
<span class="cm"> * In cases where the weight does not change often, we can use the</span>
<span class="cm"> * precalculated inverse to speed up arithmetics by turning divisions</span>
<span class="cm"> * into multiplications:</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="k">const</span> <span class="n">u32</span> <span class="n">prio_to_wmult</span><span class="p">[</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
 <span class="cm">/* -20 */</span>     <span class="mi">48388</span><span class="p">,</span>     <span class="mi">59856</span><span class="p">,</span>     <span class="mi">76040</span><span class="p">,</span>     <span class="mi">92818</span><span class="p">,</span>    <span class="mi">118348</span><span class="p">,</span>
 <span class="cm">/* -15 */</span>    <span class="mi">147320</span><span class="p">,</span>    <span class="mi">184698</span><span class="p">,</span>    <span class="mi">229616</span><span class="p">,</span>    <span class="mi">287308</span><span class="p">,</span>    <span class="mi">360437</span><span class="p">,</span>
 <span class="cm">/* -10 */</span>    <span class="mi">449829</span><span class="p">,</span>    <span class="mi">563644</span><span class="p">,</span>    <span class="mi">704093</span><span class="p">,</span>    <span class="mi">875809</span><span class="p">,</span>   <span class="mi">1099582</span><span class="p">,</span>
 <span class="cm">/*  -5 */</span>   <span class="mi">1376151</span><span class="p">,</span>   <span class="mi">1717300</span><span class="p">,</span>   <span class="mi">2157191</span><span class="p">,</span>   <span class="mi">2708050</span><span class="p">,</span>   <span class="mi">3363326</span><span class="p">,</span>
 <span class="cm">/*   0 */</span>   <span class="mi">4194304</span><span class="p">,</span>   <span class="mi">5237765</span><span class="p">,</span>   <span class="mi">6557202</span><span class="p">,</span>   <span class="mi">8165337</span><span class="p">,</span>  <span class="mi">10153587</span><span class="p">,</span>
 <span class="cm">/*   5 */</span>  <span class="mi">12820798</span><span class="p">,</span>  <span class="mi">15790321</span><span class="p">,</span>  <span class="mi">19976592</span><span class="p">,</span>  <span class="mi">24970740</span><span class="p">,</span>  <span class="mi">31350126</span><span class="p">,</span>
 <span class="cm">/*  10 */</span>  <span class="mi">39045157</span><span class="p">,</span>  <span class="mi">49367440</span><span class="p">,</span>  <span class="mi">61356676</span><span class="p">,</span>  <span class="mi">76695844</span><span class="p">,</span>  <span class="mi">95443717</span><span class="p">,</span>
 <span class="cm">/*  15 */</span> <span class="mi">119304647</span><span class="p">,</span> <span class="mi">148102320</span><span class="p">,</span> <span class="mi">186737708</span><span class="p">,</span> <span class="mi">238609294</span><span class="p">,</span> <span class="mi">286331153</span><span class="p">,</span>
<span class="p">};</span>

<span class="cm">/* Time spent by the tasks of the cpu accounting group executing in ... */</span>
<span class="k">enum</span> <span class="n">cpuacct_stat_index</span> <span class="p">{</span>
	<span class="n">CPUACCT_STAT_USER</span><span class="p">,</span>	<span class="cm">/* ... user mode */</span>
	<span class="n">CPUACCT_STAT_SYSTEM</span><span class="p">,</span>	<span class="cm">/* ... kernel mode */</span>

	<span class="n">CPUACCT_STAT_NSTATS</span><span class="p">,</span>
<span class="p">};</span>


<span class="cp">#define sched_class_highest (&amp;stop_sched_class)</span>
<span class="cp">#define for_each_class(class) \</span>
<span class="cp">   for (class = sched_class_highest; class; class = class-&gt;next)</span>

<span class="k">extern</span> <span class="k">const</span> <span class="k">struct</span> <span class="n">sched_class</span> <span class="n">stop_sched_class</span><span class="p">;</span>
<span class="k">extern</span> <span class="k">const</span> <span class="k">struct</span> <span class="n">sched_class</span> <span class="n">rt_sched_class</span><span class="p">;</span>
<span class="k">extern</span> <span class="k">const</span> <span class="k">struct</span> <span class="n">sched_class</span> <span class="n">fair_sched_class</span><span class="p">;</span>
<span class="k">extern</span> <span class="k">const</span> <span class="k">struct</span> <span class="n">sched_class</span> <span class="n">idle_sched_class</span><span class="p">;</span>


<span class="cp">#ifdef CONFIG_SMP</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">trigger_load_balance</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">idle_balance</span><span class="p">(</span><span class="kt">int</span> <span class="n">this_cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">);</span>

<span class="cp">#else	</span><span class="cm">/* CONFIG_SMP */</span><span class="cp"></span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">idle_balance</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">sysrq_sched_debug_show</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">sched_init_granularity</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">update_max_interval</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">update_group_power</span><span class="p">(</span><span class="k">struct</span> <span class="n">sched_domain</span> <span class="o">*</span><span class="n">sd</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">int</span> <span class="n">update_runtime</span><span class="p">(</span><span class="k">struct</span> <span class="n">notifier_block</span> <span class="o">*</span><span class="n">nfb</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">action</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">hcpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_sched_rt_class</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_sched_fair_class</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">resched_task</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">resched_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">rt_bandwidth</span> <span class="n">def_rt_bandwidth</span><span class="p">;</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_rt_bandwidth</span><span class="p">(</span><span class="k">struct</span> <span class="n">rt_bandwidth</span> <span class="o">*</span><span class="n">rt_b</span><span class="p">,</span> <span class="n">u64</span> <span class="n">period</span><span class="p">,</span> <span class="n">u64</span> <span class="n">runtime</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">update_idle_cpu_load</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_CGROUP_CPUACCT</span>
<span class="cp">#include &lt;linux/cgroup.h&gt;</span>
<span class="cm">/* track cpu usage of a group of tasks and its child groups */</span>
<span class="k">struct</span> <span class="n">cpuacct</span> <span class="p">{</span>
	<span class="k">struct</span> <span class="n">cgroup_subsys_state</span> <span class="n">css</span><span class="p">;</span>
	<span class="cm">/* cpuusage holds pointer to a u64-type object on every cpu */</span>
	<span class="n">u64</span> <span class="n">__percpu</span> <span class="o">*</span><span class="n">cpuusage</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">kernel_cpustat</span> <span class="n">__percpu</span> <span class="o">*</span><span class="n">cpustat</span><span class="p">;</span>
<span class="p">};</span>

<span class="cm">/* return cpu accounting group corresponding to this container */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">cpuacct</span> <span class="o">*</span><span class="nf">cgroup_ca</span><span class="p">(</span><span class="k">struct</span> <span class="n">cgroup</span> <span class="o">*</span><span class="n">cgrp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">container_of</span><span class="p">(</span><span class="n">cgroup_subsys_state</span><span class="p">(</span><span class="n">cgrp</span><span class="p">,</span> <span class="n">cpuacct_subsys_id</span><span class="p">),</span>
			    <span class="k">struct</span> <span class="n">cpuacct</span><span class="p">,</span> <span class="n">css</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/* return cpu accounting group to which this task belongs */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">cpuacct</span> <span class="o">*</span><span class="nf">task_ca</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">tsk</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">container_of</span><span class="p">(</span><span class="n">task_subsys_state</span><span class="p">(</span><span class="n">tsk</span><span class="p">,</span> <span class="n">cpuacct_subsys_id</span><span class="p">),</span>
			    <span class="k">struct</span> <span class="n">cpuacct</span><span class="p">,</span> <span class="n">css</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">cpuacct</span> <span class="o">*</span><span class="nf">parent_ca</span><span class="p">(</span><span class="k">struct</span> <span class="n">cpuacct</span> <span class="o">*</span><span class="n">ca</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ca</span> <span class="o">||</span> <span class="o">!</span><span class="n">ca</span><span class="o">-&gt;</span><span class="n">css</span><span class="p">.</span><span class="n">cgroup</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">)</span>
		<span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">cgroup_ca</span><span class="p">(</span><span class="n">ca</span><span class="o">-&gt;</span><span class="n">css</span><span class="p">.</span><span class="n">cgroup</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">cpuacct_charge</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">tsk</span><span class="p">,</span> <span class="n">u64</span> <span class="n">cputime</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">cpuacct_charge</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">tsk</span><span class="p">,</span> <span class="n">u64</span> <span class="n">cputime</span><span class="p">)</span> <span class="p">{}</span>
<span class="cp">#endif</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">inc_nr_running</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rq</span><span class="o">-&gt;</span><span class="n">nr_running</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">dec_nr_running</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rq</span><span class="o">-&gt;</span><span class="n">nr_running</span><span class="o">--</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">update_rq_clock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">activate_task</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">int</span> <span class="n">flags</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">deactivate_task</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">int</span> <span class="n">flags</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">check_preempt_curr</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="kt">int</span> <span class="n">flags</span><span class="p">);</span>

<span class="k">extern</span> <span class="n">const_debug</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sysctl_sched_time_avg</span><span class="p">;</span>
<span class="k">extern</span> <span class="n">const_debug</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sysctl_sched_nr_migrate</span><span class="p">;</span>
<span class="k">extern</span> <span class="n">const_debug</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sysctl_sched_migration_cost</span><span class="p">;</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="n">u64</span> <span class="nf">sched_avg_period</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">u64</span><span class="p">)</span><span class="n">sysctl_sched_time_avg</span> <span class="o">*</span> <span class="n">NSEC_PER_MSEC</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">calc_load_account_idle</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_SCHED_HRTICK</span>

<span class="cm">/*</span>
<span class="cm"> * Use hrtick when:</span>
<span class="cm"> *  - enabled by features</span>
<span class="cm"> *  - hrtimer is actually high res</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">hrtick_enabled</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">sched_feat</span><span class="p">(</span><span class="n">HRTICK</span><span class="p">))</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">cpu_active</span><span class="p">(</span><span class="n">cpu_of</span><span class="p">(</span><span class="n">rq</span><span class="p">)))</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">hrtimer_is_hres_active</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq</span><span class="o">-&gt;</span><span class="n">hrtick_timer</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">hrtick_start</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="n">u64</span> <span class="n">delay</span><span class="p">);</span>

<span class="cp">#else</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">hrtick_enabled</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* CONFIG_SCHED_HRTICK */</span><span class="cp"></span>

<span class="cp">#ifdef CONFIG_SMP</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">sched_avg_update</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">);</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">sched_rt_avg_update</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="n">u64</span> <span class="n">rt_delta</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rq</span><span class="o">-&gt;</span><span class="n">rt_avg</span> <span class="o">+=</span> <span class="n">rt_delta</span><span class="p">;</span>
	<span class="n">sched_avg_update</span><span class="p">(</span><span class="n">rq</span><span class="p">);</span>
<span class="p">}</span>
<span class="cp">#else</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">sched_rt_avg_update</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">,</span> <span class="n">u64</span> <span class="n">rt_delta</span><span class="p">)</span> <span class="p">{</span> <span class="p">}</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">sched_avg_update</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">)</span> <span class="p">{</span> <span class="p">}</span>
<span class="cp">#endif</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">start_bandwidth_timer</span><span class="p">(</span><span class="k">struct</span> <span class="n">hrtimer</span> <span class="o">*</span><span class="n">period_timer</span><span class="p">,</span> <span class="n">ktime_t</span> <span class="n">period</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_SMP</span>
<span class="cp">#ifdef CONFIG_PREEMPT</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="n">double_rq_lock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq1</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq2</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * fair double_lock_balance: Safely acquires both rq-&gt;locks in a fair</span>
<span class="cm"> * way at the expense of forcing extra atomic operations in all</span>
<span class="cm"> * invocations.  This assures that the double_lock is acquired using the</span>
<span class="cm"> * same underlying policy as the spinlock_t on this architecture, which</span>
<span class="cm"> * reduces latency compared to the unfair variant below.  However, it</span>
<span class="cm"> * also adds more overhead and therefore may reduce throughput.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">_double_lock_balance</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">busiest</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
	<span class="n">double_rq_lock</span><span class="p">(</span><span class="n">this_rq</span><span class="p">,</span> <span class="n">busiest</span><span class="p">);</span>

	<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#else</span>
<span class="cm">/*</span>
<span class="cm"> * Unfair double_lock_balance: Optimizes throughput at the expense of</span>
<span class="cm"> * latency by eliminating extra atomic operations when the locks are</span>
<span class="cm"> * already in proper order on entry.  This favors lower cpu-ids and will</span>
<span class="cm"> * grant the double lock to lower cpus over higher ids under contention,</span>
<span class="cm"> * regardless of entry order into the function.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">_double_lock_balance</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">busiest</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="o">!</span><span class="n">raw_spin_trylock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)))</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">busiest</span> <span class="o">&lt;</span> <span class="n">this_rq</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
			<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
			<span class="n">raw_spin_lock_nested</span><span class="p">(</span><span class="o">&amp;</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span>
					      <span class="n">SINGLE_DEPTH_NESTING</span><span class="p">);</span>
			<span class="n">ret</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
		<span class="p">}</span> <span class="k">else</span>
			<span class="n">raw_spin_lock_nested</span><span class="p">(</span><span class="o">&amp;</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span>
					      <span class="n">SINGLE_DEPTH_NESTING</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* CONFIG_PREEMPT */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * double_lock_balance - lock the busiest runqueue, this_rq is locked already.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">double_lock_balance</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">busiest</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="o">!</span><span class="n">irqs_disabled</span><span class="p">()))</span> <span class="p">{</span>
		<span class="cm">/* printk() doesn&#39;t work good under rq-&gt;lock */</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
		<span class="n">BUG_ON</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
	<span class="p">}</span>

	<span class="k">return</span> <span class="n">_double_lock_balance</span><span class="p">(</span><span class="n">this_rq</span><span class="p">,</span> <span class="n">busiest</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">double_unlock_balance</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">this_rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">busiest</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">busiest</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
	<span class="n">lock_set_subclass</span><span class="p">(</span><span class="o">&amp;</span><span class="n">this_rq</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">.</span><span class="n">dep_map</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">_RET_IP_</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * double_rq_lock - safely lock two runqueues</span>
<span class="cm"> *</span>
<span class="cm"> * Note this does not disable interrupts like task_rq_lock,</span>
<span class="cm"> * you need to do so manually before calling.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">double_rq_lock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq1</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq2</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">BUG_ON</span><span class="p">(</span><span class="o">!</span><span class="n">irqs_disabled</span><span class="p">());</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rq1</span> <span class="o">==</span> <span class="n">rq2</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
		<span class="n">__acquire</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* Fake it out ;) */</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rq1</span> <span class="o">&lt;</span> <span class="n">rq2</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
			<span class="n">raw_spin_lock_nested</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">SINGLE_DEPTH_NESTING</span><span class="p">);</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
			<span class="n">raw_spin_lock_nested</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">SINGLE_DEPTH_NESTING</span><span class="p">);</span>
		<span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * double_rq_unlock - safely unlock two runqueues</span>
<span class="cm"> *</span>
<span class="cm"> * Note this does not restore interrupts like task_rq_unlock,</span>
<span class="cm"> * you need to do so manually after calling.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">double_rq_unlock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq1</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq2</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rq1</span> <span class="o">!=</span> <span class="n">rq2</span><span class="p">)</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
	<span class="k">else</span>
		<span class="n">__release</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* CONFIG_SMP */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * double_rq_lock - safely lock two runqueues</span>
<span class="cm"> *</span>
<span class="cm"> * Note this does not disable interrupts like task_rq_lock,</span>
<span class="cm"> * you need to do so manually before calling.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">double_rq_lock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq1</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq2</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__acquires</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">BUG_ON</span><span class="p">(</span><span class="o">!</span><span class="n">irqs_disabled</span><span class="p">());</span>
	<span class="n">BUG_ON</span><span class="p">(</span><span class="n">rq1</span> <span class="o">!=</span> <span class="n">rq2</span><span class="p">);</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
	<span class="n">__acquire</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* Fake it out ;) */</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * double_rq_unlock - safely unlock two runqueues</span>
<span class="cm"> *</span>
<span class="cm"> * Note this does not restore interrupts like task_rq_unlock,</span>
<span class="cm"> * you need to do so manually after calling.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">double_rq_unlock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq1</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq2</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">BUG_ON</span><span class="p">(</span><span class="n">rq1</span> <span class="o">!=</span> <span class="n">rq2</span><span class="p">);</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rq1</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
	<span class="n">__release</span><span class="p">(</span><span class="n">rq2</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#endif</span>

<span class="k">extern</span> <span class="k">struct</span> <span class="n">sched_entity</span> <span class="o">*</span><span class="n">__pick_first_entity</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_rq</span> <span class="o">*</span><span class="n">cfs_rq</span><span class="p">);</span>
<span class="k">extern</span> <span class="k">struct</span> <span class="n">sched_entity</span> <span class="o">*</span><span class="n">__pick_last_entity</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_rq</span> <span class="o">*</span><span class="n">cfs_rq</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">print_cfs_stats</span><span class="p">(</span><span class="k">struct</span> <span class="n">seq_file</span> <span class="o">*</span><span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">print_rt_stats</span><span class="p">(</span><span class="k">struct</span> <span class="n">seq_file</span> <span class="o">*</span><span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_cfs_rq</span><span class="p">(</span><span class="k">struct</span> <span class="n">cfs_rq</span> <span class="o">*</span><span class="n">cfs_rq</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">init_rt_rq</span><span class="p">(</span><span class="k">struct</span> <span class="n">rt_rq</span> <span class="o">*</span><span class="n">rt_rq</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">);</span>
<span class="k">extern</span> <span class="kt">void</span> <span class="n">unthrottle_offline_cfs_rqs</span><span class="p">(</span><span class="k">struct</span> <span class="n">rq</span> <span class="o">*</span><span class="n">rq</span><span class="p">);</span>

<span class="k">extern</span> <span class="kt">void</span> <span class="n">account_cfs_bandwidth_used</span><span class="p">(</span><span class="kt">int</span> <span class="n">enabled</span><span class="p">,</span> <span class="kt">int</span> <span class="n">was_enabled</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_NO_HZ</span>
<span class="k">enum</span> <span class="n">rq_nohz_flag_bits</span> <span class="p">{</span>
	<span class="n">NOHZ_TICK_STOPPED</span><span class="p">,</span>
	<span class="n">NOHZ_BALANCE_KICK</span><span class="p">,</span>
	<span class="n">NOHZ_IDLE</span><span class="p">,</span>
<span class="p">};</span>

<span class="cp">#define nohz_flags(cpu)	(&amp;cpu_rq(cpu)-&gt;nohz_flags)</span>
<span class="cp">#endif</span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:2}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../../javascript/docco.min.js"></script>
</html>
