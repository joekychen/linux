<!DOCTYPE html>
<html><head><title>joekychen/linux » kernel › rcutree.c

</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docco">
<link rel="stylesheet" media="all" href="../stylesheets/docco.min.css" />


</head>
<body>
<div id="container">
<div id="background"></div>
<table cellpadding="0" cellspacing="0">
<thead><tr><th class="docs"><a id="home" href="../index.html"></a><h1>rcutree.c</h1></th><th class="code"></th></tr></thead>
<tbody>


<tr id="section-1"><td class="docs"><div class="pilwrap"><a class="pilcrow" href="#section-1">&#182;</a></div></td><td class="code"><div class="highlight"><pre><span class="cm">/*</span>
<span class="cm"> * Read-Copy Update mechanism for mutual exclusion</span>
<span class="cm"> *</span>
<span class="cm"> * This program is free software; you can redistribute it and/or modify</span>
<span class="cm"> * it under the terms of the GNU General Public License as published by</span>
<span class="cm"> * the Free Software Foundation; either version 2 of the License, or</span>
<span class="cm"> * (at your option) any later version.</span>
<span class="cm"> *</span>
<span class="cm"> * This program is distributed in the hope that it will be useful,</span>
<span class="cm"> * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="cm"> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="cm"> * GNU General Public License for more details.</span>
<span class="cm"> *</span>
<span class="cm"> * You should have received a copy of the GNU General Public License</span>
<span class="cm"> * along with this program; if not, write to the Free Software</span>
<span class="cm"> * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.</span>
<span class="cm"> *</span>
<span class="cm"> * Copyright IBM Corporation, 2008</span>
<span class="cm"> *</span>
<span class="cm"> * Authors: Dipankar Sarma &lt;dipankar@in.ibm.com&gt;</span>
<span class="cm"> *	    Manfred Spraul &lt;manfred@colorfullife.com&gt;</span>
<span class="cm"> *	    Paul E. McKenney &lt;paulmck@linux.vnet.ibm.com&gt; Hierarchical version</span>
<span class="cm"> *</span>
<span class="cm"> * Based on the original work by Paul McKenney &lt;paulmck@us.ibm.com&gt;</span>
<span class="cm"> * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.</span>
<span class="cm"> *</span>
<span class="cm"> * For detailed explanation of Read-Copy Update mechanism see -</span>
<span class="cm"> *	Documentation/RCU</span>
<span class="cm"> */</span>
<span class="cp">#include &lt;linux/types.h&gt;</span>
<span class="cp">#include &lt;linux/kernel.h&gt;</span>
<span class="cp">#include &lt;linux/init.h&gt;</span>
<span class="cp">#include &lt;linux/spinlock.h&gt;</span>
<span class="cp">#include &lt;linux/smp.h&gt;</span>
<span class="cp">#include &lt;linux/rcupdate.h&gt;</span>
<span class="cp">#include &lt;linux/interrupt.h&gt;</span>
<span class="cp">#include &lt;linux/sched.h&gt;</span>
<span class="cp">#include &lt;linux/nmi.h&gt;</span>
<span class="cp">#include &lt;linux/atomic.h&gt;</span>
<span class="cp">#include &lt;linux/bitops.h&gt;</span>
<span class="cp">#include &lt;linux/export.h&gt;</span>
<span class="cp">#include &lt;linux/completion.h&gt;</span>
<span class="cp">#include &lt;linux/moduleparam.h&gt;</span>
<span class="cp">#include &lt;linux/percpu.h&gt;</span>
<span class="cp">#include &lt;linux/notifier.h&gt;</span>
<span class="cp">#include &lt;linux/cpu.h&gt;</span>
<span class="cp">#include &lt;linux/mutex.h&gt;</span>
<span class="cp">#include &lt;linux/time.h&gt;</span>
<span class="cp">#include &lt;linux/kernel_stat.h&gt;</span>
<span class="cp">#include &lt;linux/wait.h&gt;</span>
<span class="cp">#include &lt;linux/kthread.h&gt;</span>
<span class="cp">#include &lt;linux/prefetch.h&gt;</span>
<span class="cp">#include &lt;linux/delay.h&gt;</span>
<span class="cp">#include &lt;linux/stop_machine.h&gt;</span>

<span class="cp">#include &quot;rcutree.h&quot;</span>
<span class="cp">#include &lt;trace/events/rcu.h&gt;</span>

<span class="cp">#include &quot;rcu.h&quot;</span>

<span class="cm">/* Data structures. */</span>

<span class="k">static</span> <span class="k">struct</span> <span class="n">lock_class_key</span> <span class="n">rcu_node_class</span><span class="p">[</span><span class="n">NUM_RCU_LVLS</span><span class="p">];</span>

<span class="cp">#define RCU_STATE_INITIALIZER(structname) { \</span>
<span class="cp">	.level = { &amp;structname##_state.node[0] }, \</span>
<span class="cp">	.levelcnt = { \</span>
<span class="cp">		NUM_RCU_LVL_0,  </span><span class="cm">/* root of hierarchy. */</span><span class="cp"> \</span>
<span class="cp">		NUM_RCU_LVL_1, \</span>
<span class="cp">		NUM_RCU_LVL_2, \</span>
<span class="cp">		NUM_RCU_LVL_3, \</span>
<span class="cp">		NUM_RCU_LVL_4, </span><span class="cm">/* == MAX_RCU_LVLS */</span><span class="cp"> \</span>
<span class="cp">	}, \</span>
<span class="cp">	.fqs_state = RCU_GP_IDLE, \</span>
<span class="cp">	.gpnum = -300, \</span>
<span class="cp">	.completed = -300, \</span>
<span class="cp">	.onofflock = __RAW_SPIN_LOCK_UNLOCKED(&amp;structname##_state.onofflock), \</span>
<span class="cp">	.orphan_nxttail = &amp;structname##_state.orphan_nxtlist, \</span>
<span class="cp">	.orphan_donetail = &amp;structname##_state.orphan_donelist, \</span>
<span class="cp">	.fqslock = __RAW_SPIN_LOCK_UNLOCKED(&amp;structname##_state.fqslock), \</span>
<span class="cp">	.n_force_qs = 0, \</span>
<span class="cp">	.n_force_qs_ngp = 0, \</span>
<span class="cp">	.name = #structname, \</span>
<span class="cp">}</span>

<span class="k">struct</span> <span class="n">rcu_state</span> <span class="n">rcu_sched_state</span> <span class="o">=</span> <span class="n">RCU_STATE_INITIALIZER</span><span class="p">(</span><span class="n">rcu_sched</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span><span class="p">,</span> <span class="n">rcu_sched_data</span><span class="p">);</span>

<span class="k">struct</span> <span class="n">rcu_state</span> <span class="n">rcu_bh_state</span> <span class="o">=</span> <span class="n">RCU_STATE_INITIALIZER</span><span class="p">(</span><span class="n">rcu_bh</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span><span class="p">,</span> <span class="n">rcu_bh_data</span><span class="p">);</span>

<span class="k">static</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rcu_state</span><span class="p">;</span>

<span class="cm">/*</span>
<span class="cm"> * The rcu_scheduler_active variable transitions from zero to one just</span>
<span class="cm"> * before the first task is spawned.  So when this variable is zero, RCU</span>
<span class="cm"> * can assume that there is but one task, allowing RCU to (for example)</span>
<span class="cm"> * optimized synchronize_sched() to a simple barrier().  When this variable</span>
<span class="cm"> * is one, RCU must actually do all the hard work required to detect real</span>
<span class="cm"> * grace periods.  This variable is also used to suppress boot-time false</span>
<span class="cm"> * positives from lockdep-RCU error checking.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="n">rcu_scheduler_active</span> <span class="n">__read_mostly</span><span class="p">;</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_scheduler_active</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * The rcu_scheduler_fully_active variable transitions from zero to one</span>
<span class="cm"> * during the early_initcall() processing, which is after the scheduler</span>
<span class="cm"> * is capable of creating new tasks.  So RCU processing (for example,</span>
<span class="cm"> * creating tasks for RCU priority boosting) must be delayed until after</span>
<span class="cm"> * rcu_scheduler_fully_active transitions from zero to one.  We also</span>
<span class="cm"> * currently delay invocation of any RCU callbacks until after this point.</span>
<span class="cm"> *</span>
<span class="cm"> * It might later prove better for people registering RCU callbacks during</span>
<span class="cm"> * early boot to take responsibility for these callbacks, but one step at</span>
<span class="cm"> * a time.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">rcu_scheduler_fully_active</span> <span class="n">__read_mostly</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_RCU_BOOST</span>

<span class="cm">/*</span>
<span class="cm"> * Control variables for per-CPU and per-rcu_node kthreads.  These</span>
<span class="cm"> * handle all flavors of RCU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="p">,</span> <span class="n">rcu_cpu_kthread_task</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="p">,</span> <span class="n">rcu_cpu_kthread_status</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span> <span class="n">rcu_cpu_kthread_cpu</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="p">,</span> <span class="n">rcu_cpu_kthread_loops</span><span class="p">);</span>
<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="kt">char</span><span class="p">,</span> <span class="n">rcu_cpu_has_work</span><span class="p">);</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_RCU_BOOST */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">rcu_node_kthread_setaffinity</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">outgoingcpu</span><span class="p">);</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">invoke_rcu_core</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">invoke_rcu_callbacks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Track the rcutorture test sequence number and the update version</span>
<span class="cm"> * number within a given test.  The rcutorture_testseq is incremented</span>
<span class="cm"> * on every rcutorture module load and unload, so has an odd value</span>
<span class="cm"> * when a test is running.  The rcutorture_vernum is set to zero</span>
<span class="cm"> * when rcutorture starts and is incremented on each rcutorture update.</span>
<span class="cm"> * These variables enable correlating rcutorture output with the</span>
<span class="cm"> * RCU tracing information.</span>
<span class="cm"> */</span>
<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">rcutorture_testseq</span><span class="p">;</span>
<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">rcutorture_vernum</span><span class="p">;</span>

<span class="cm">/* State information for rcu_barrier() and friends. */</span>

<span class="k">static</span> <span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span><span class="p">,</span> <span class="n">rcu_barrier_head</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span><span class="nb">NULL</span><span class="p">};</span>
<span class="k">static</span> <span class="n">atomic_t</span> <span class="n">rcu_barrier_cpu_count</span><span class="p">;</span>
<span class="k">static</span> <span class="n">DEFINE_MUTEX</span><span class="p">(</span><span class="n">rcu_barrier_mutex</span><span class="p">);</span>
<span class="k">static</span> <span class="k">struct</span> <span class="n">completion</span> <span class="n">rcu_barrier_completion</span><span class="p">;</span>

<span class="cm">/*</span>
<span class="cm"> * Return true if an RCU grace period is in progress.  The ACCESS_ONCE()s</span>
<span class="cm"> * permit this function to be invoked without holding the root rcu_node</span>
<span class="cm"> * structure&#39;s -&gt;lock, but of course results can be subject to change.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_gp_in_progress</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Note a quiescent state.  Because we do not need to know</span>
<span class="cm"> * how many quiescent states passed, just if there was at least</span>
<span class="cm"> * one since the start of the grace period, this just sets a flag.</span>
<span class="cm"> * The caller must have disabled preemption.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_sched_qs</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce_gpnum</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
	<span class="n">barrier</span><span class="p">();</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="s">&quot;rcu_sched&quot;</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;cpuqs&quot;</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">rcu_bh_qs</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>

	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce_gpnum</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
	<span class="n">barrier</span><span class="p">();</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="s">&quot;rcu_bh&quot;</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;cpuqs&quot;</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Note a context switch.  This is a quiescent state for RCU-sched,</span>
<span class="cm"> * and requires special handling for preemptible RCU.</span>
<span class="cm"> * The caller must have disabled preemption.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_note_context_switch</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start context switch&quot;</span><span class="p">);</span>
	<span class="n">rcu_sched_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End context switch&quot;</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_note_context_switch</span><span class="p">);</span>

<span class="n">DEFINE_PER_CPU</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">rcu_dynticks</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span>
	<span class="p">.</span><span class="n">dynticks_nesting</span> <span class="o">=</span> <span class="n">DYNTICK_TASK_EXIT_IDLE</span><span class="p">,</span>
	<span class="p">.</span><span class="n">dynticks</span> <span class="o">=</span> <span class="n">ATOMIC_INIT</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span class="p">};</span>

<span class="k">static</span> <span class="kt">int</span> <span class="n">blimit</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>		<span class="cm">/* Maximum callbacks per rcu_do_batch. */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">qhimark</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">;</span>	<span class="cm">/* If this many pending, ignore blimit. */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">qlowmark</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>	<span class="cm">/* Once only this many pending, use blimit. */</span>

<span class="n">module_param</span><span class="p">(</span><span class="n">blimit</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="n">module_param</span><span class="p">(</span><span class="n">qhimark</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="n">module_param</span><span class="p">(</span><span class="n">qlowmark</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="kt">int</span> <span class="n">rcu_cpu_stall_suppress</span> <span class="n">__read_mostly</span><span class="p">;</span> <span class="cm">/* 1 = suppress stall warnings. */</span>
<span class="kt">int</span> <span class="n">rcu_cpu_stall_timeout</span> <span class="n">__read_mostly</span> <span class="o">=</span> <span class="n">CONFIG_RCU_CPU_STALL_TIMEOUT</span><span class="p">;</span>

<span class="n">module_param</span><span class="p">(</span><span class="n">rcu_cpu_stall_suppress</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span> <span class="mo">0644</span><span class="p">);</span>
<span class="n">module_param</span><span class="p">(</span><span class="n">rcu_cpu_stall_timeout</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span> <span class="mo">0644</span><span class="p">);</span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">force_quiescent_state</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">relaxed</span><span class="p">);</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">rcu_pending</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Return the number of RCU-sched batches processed thus far for debug &amp; stats.</span>
<span class="cm"> */</span>
<span class="kt">long</span> <span class="nf">rcu_batches_completed_sched</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rcu_sched_state</span><span class="p">.</span><span class="n">completed</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_batches_completed_sched</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Return the number of RCU BH batches processed thus far for debug &amp; stats.</span>
<span class="cm"> */</span>
<span class="kt">long</span> <span class="nf">rcu_batches_completed_bh</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">rcu_bh_state</span><span class="p">.</span><span class="n">completed</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_batches_completed_bh</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Force a quiescent state for RCU BH.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_bh_force_quiescent_state</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">force_quiescent_state</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_bh_force_quiescent_state</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Record the number of times rcutorture tests have been initiated and</span>
<span class="cm"> * terminated.  This information allows the debugfs tracing stats to be</span>
<span class="cm"> * correlated to the rcutorture messages, even when the rcutorture module</span>
<span class="cm"> * is being repeatedly loaded and unloaded.  In other words, we cannot</span>
<span class="cm"> * store this state in rcutorture itself.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcutorture_record_test_transition</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcutorture_testseq</span><span class="o">++</span><span class="p">;</span>
	<span class="n">rcutorture_vernum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcutorture_record_test_transition</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Record the number of writer passes through the current rcutorture test.</span>
<span class="cm"> * This is also used to correlate debugfs tracing stats with the rcutorture</span>
<span class="cm"> * messages.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcutorture_record_progress</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">vernum</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcutorture_vernum</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcutorture_record_progress</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Force a quiescent state for RCU-sched.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_sched_force_quiescent_state</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">force_quiescent_state</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_sched_force_quiescent_state</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Does the CPU have callbacks ready to be invoked?</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span>
<span class="nf">cpu_has_callbacks_ready_to_invoke</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">!=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">];</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Does the current CPU require a yet-as-unscheduled grace period?</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span>
<span class="nf">cpu_needs_another_gp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Return the root node of the specified rcu_state structure.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="nf">rcu_get_root</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * If the specified CPU is offline, tell the caller that it is in</span>
<span class="cm"> * a quiescent state.  Otherwise, whack it with a reschedule IPI.</span>
<span class="cm"> * Grace periods can end up waiting on an offline CPU when that</span>
<span class="cm"> * CPU is in the process of coming online -- it will be added to the</span>
<span class="cm"> * rcu_node bitmasks before it actually makes it online.  The same thing</span>
<span class="cm"> * can happen while a CPU is in the process of coming online.  Because this</span>
<span class="cm"> * race is quite rare, we check for it after detecting that the grace</span>
<span class="cm"> * period has been delayed rather than checking each and every CPU</span>
<span class="cm"> * each and every time we start a new grace period.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_implicit_offline_qs</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="cm">/*</span>
<span class="cm">	 * If the CPU is offline for more than a jiffy, it is in a quiescent</span>
<span class="cm">	 * state.  We can trust its state not to change because interrupts</span>
<span class="cm">	 * are disabled.  The reason for the jiffy&#39;s worth of slack is to</span>
<span class="cm">	 * handle CPUs initializing on the way up and finding their way</span>
<span class="cm">	 * to the idle loop on the way down.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_is_offline</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">cpu</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
	    <span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_start</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">jiffies</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">trace_rcu_fqs</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">cpu</span><span class="p">,</span> <span class="s">&quot;ofl&quot;</span><span class="p">);</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">offline_fqs</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * rcu_idle_enter_common - inform RCU that current CPU is moving towards idle</span>
<span class="cm"> *</span>
<span class="cm"> * If the new value of the -&gt;dynticks_nesting counter now is zero,</span>
<span class="cm"> * we really have entered idle, and must do the appropriate accounting.</span>
<span class="cm"> * The caller must have disabled interrupts.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_idle_enter_common</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span><span class="p">,</span> <span class="kt">long</span> <span class="kt">long</span> <span class="n">oldval</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">trace_rcu_dyntick</span><span class="p">(</span><span class="s">&quot;Start&quot;</span><span class="p">,</span> <span class="n">oldval</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_idle_task</span><span class="p">(</span><span class="n">current</span><span class="p">))</span> <span class="p">{</span>
		<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">idle</span> <span class="o">=</span> <span class="n">idle_task</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">());</span>

		<span class="n">trace_rcu_dyntick</span><span class="p">(</span><span class="s">&quot;Error on entry: not idle task&quot;</span><span class="p">,</span> <span class="n">oldval</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">ftrace_dump</span><span class="p">(</span><span class="n">DUMP_ALL</span><span class="p">);</span>
		<span class="n">WARN_ONCE</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s">&quot;Current pid: %d comm: %s / Idle pid: %d comm: %s&quot;</span><span class="p">,</span>
			  <span class="n">current</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">,</span> <span class="n">current</span><span class="o">-&gt;</span><span class="n">comm</span><span class="p">,</span>
			  <span class="n">idle</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">,</span> <span class="n">idle</span><span class="o">-&gt;</span><span class="n">comm</span><span class="p">);</span> <span class="cm">/* must be idle task! */</span>
	<span class="p">}</span>
	<span class="n">rcu_prepare_for_idle</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">());</span>
	<span class="cm">/* CPUs seeing atomic_inc() must see prior RCU read-side crit sects */</span>
	<span class="n">smp_mb__before_atomic_inc</span><span class="p">();</span>  <span class="cm">/* See above. */</span>
	<span class="n">atomic_inc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">);</span>
	<span class="n">smp_mb__after_atomic_inc</span><span class="p">();</span>  <span class="cm">/* Force ordering with next sojourn. */</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * The idle task is not permitted to enter the idle loop while</span>
<span class="cm">	 * in an RCU read-side critical section.</span>
<span class="cm">	 */</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal idle entry in RCU read-side critical section.&quot;</span><span class="p">);</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal idle entry in RCU-bh read-side critical section.&quot;</span><span class="p">);</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal idle entry in RCU-sched read-side critical section.&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_idle_enter - inform RCU that current CPU is entering idle</span>
<span class="cm"> *</span>
<span class="cm"> * Enter idle mode, in other words, -leave- the mode in which RCU</span>
<span class="cm"> * read-side critical sections can occur.  (Though RCU read-side</span>
<span class="cm"> * critical sections can occur in irq handlers in idle, a possibility</span>
<span class="cm"> * handled by irq_enter() and irq_exit().)</span>
<span class="cm"> *</span>
<span class="cm"> * We crowbar the -&gt;dynticks_nesting field to zero to allow for</span>
<span class="cm"> * the possibility of usermode upcalls having messed up our count</span>
<span class="cm"> * of interrupt nesting level during the prior busy period.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_idle_enter</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">long</span> <span class="kt">long</span> <span class="n">oldval</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">);</span>
	<span class="n">oldval</span> <span class="o">=</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">;</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">((</span><span class="n">oldval</span> <span class="o">&amp;</span> <span class="n">DYNTICK_TASK_NEST_MASK</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">((</span><span class="n">oldval</span> <span class="o">&amp;</span> <span class="n">DYNTICK_TASK_NEST_MASK</span><span class="p">)</span> <span class="o">==</span> <span class="n">DYNTICK_TASK_NEST_VALUE</span><span class="p">)</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">else</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">-=</span> <span class="n">DYNTICK_TASK_NEST_VALUE</span><span class="p">;</span>
	<span class="n">rcu_idle_enter_common</span><span class="p">(</span><span class="n">rdtp</span><span class="p">,</span> <span class="n">oldval</span><span class="p">);</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_idle_enter</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_irq_exit - inform RCU that current CPU is exiting irq towards idle</span>
<span class="cm"> *</span>
<span class="cm"> * Exit from an interrupt handler, which might possibly result in entering</span>
<span class="cm"> * idle mode, in other words, leaving the mode in which read-side critical</span>
<span class="cm"> * sections can occur.</span>
<span class="cm"> *</span>
<span class="cm"> * This code assumes that the idle loop never does anything that might</span>
<span class="cm"> * result in unbalanced calls to irq_enter() and irq_exit().  If your</span>
<span class="cm"> * architecture violates this assumption, RCU will give you what you</span>
<span class="cm"> * deserve, good and hard.  But very infrequently and irreproducibly.</span>
<span class="cm"> *</span>
<span class="cm"> * Use things like work queues to work around this limitation.</span>
<span class="cm"> *</span>
<span class="cm"> * You have been warned.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_irq_exit</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">long</span> <span class="kt">long</span> <span class="n">oldval</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">);</span>
	<span class="n">oldval</span> <span class="o">=</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">;</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="o">--</span><span class="p">;</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">)</span>
		<span class="n">trace_rcu_dyntick</span><span class="p">(</span><span class="s">&quot;--=&quot;</span><span class="p">,</span> <span class="n">oldval</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">);</span>
	<span class="k">else</span>
		<span class="n">rcu_idle_enter_common</span><span class="p">(</span><span class="n">rdtp</span><span class="p">,</span> <span class="n">oldval</span><span class="p">);</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * rcu_idle_exit_common - inform RCU that current CPU is moving away from idle</span>
<span class="cm"> *</span>
<span class="cm"> * If the new value of the -&gt;dynticks_nesting counter was previously zero,</span>
<span class="cm"> * we really have exited idle, and must do the appropriate accounting.</span>
<span class="cm"> * The caller must have disabled interrupts.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_idle_exit_common</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span><span class="p">,</span> <span class="kt">long</span> <span class="kt">long</span> <span class="n">oldval</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">smp_mb__before_atomic_inc</span><span class="p">();</span>  <span class="cm">/* Force ordering w/previous sojourn. */</span>
	<span class="n">atomic_inc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">);</span>
	<span class="cm">/* CPUs seeing atomic_inc() must see later RCU read-side crit sects */</span>
	<span class="n">smp_mb__after_atomic_inc</span><span class="p">();</span>  <span class="cm">/* See above. */</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">));</span>
	<span class="n">rcu_cleanup_after_idle</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">());</span>
	<span class="n">trace_rcu_dyntick</span><span class="p">(</span><span class="s">&quot;End&quot;</span><span class="p">,</span> <span class="n">oldval</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_idle_task</span><span class="p">(</span><span class="n">current</span><span class="p">))</span> <span class="p">{</span>
		<span class="k">struct</span> <span class="n">task_struct</span> <span class="o">*</span><span class="n">idle</span> <span class="o">=</span> <span class="n">idle_task</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">());</span>

		<span class="n">trace_rcu_dyntick</span><span class="p">(</span><span class="s">&quot;Error on exit: not idle task&quot;</span><span class="p">,</span>
				  <span class="n">oldval</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">);</span>
		<span class="n">ftrace_dump</span><span class="p">(</span><span class="n">DUMP_ALL</span><span class="p">);</span>
		<span class="n">WARN_ONCE</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s">&quot;Current pid: %d comm: %s / Idle pid: %d comm: %s&quot;</span><span class="p">,</span>
			  <span class="n">current</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">,</span> <span class="n">current</span><span class="o">-&gt;</span><span class="n">comm</span><span class="p">,</span>
			  <span class="n">idle</span><span class="o">-&gt;</span><span class="n">pid</span><span class="p">,</span> <span class="n">idle</span><span class="o">-&gt;</span><span class="n">comm</span><span class="p">);</span> <span class="cm">/* must be idle task! */</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_idle_exit - inform RCU that current CPU is leaving idle</span>
<span class="cm"> *</span>
<span class="cm"> * Exit idle mode, in other words, -enter- the mode in which RCU</span>
<span class="cm"> * read-side critical sections can occur.</span>
<span class="cm"> *</span>
<span class="cm"> * We crowbar the -&gt;dynticks_nesting field to DYNTICK_TASK_NEST to</span>
<span class="cm"> * allow for the possibility of usermode upcalls messing up our count</span>
<span class="cm"> * of interrupt nesting level during the busy period that is just</span>
<span class="cm"> * now starting.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_idle_exit</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span><span class="p">;</span>
	<span class="kt">long</span> <span class="kt">long</span> <span class="n">oldval</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">);</span>
	<span class="n">oldval</span> <span class="o">=</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">;</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">oldval</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">oldval</span> <span class="o">&amp;</span> <span class="n">DYNTICK_TASK_NEST_MASK</span><span class="p">)</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">+=</span> <span class="n">DYNTICK_TASK_NEST_VALUE</span><span class="p">;</span>
	<span class="k">else</span>
		<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">=</span> <span class="n">DYNTICK_TASK_EXIT_IDLE</span><span class="p">;</span>
	<span class="n">rcu_idle_exit_common</span><span class="p">(</span><span class="n">rdtp</span><span class="p">,</span> <span class="n">oldval</span><span class="p">);</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_idle_exit</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_irq_enter - inform RCU that current CPU is entering irq away from idle</span>
<span class="cm"> *</span>
<span class="cm"> * Enter an interrupt handler, which might possibly result in exiting</span>
<span class="cm"> * idle mode, in other words, entering the mode in which read-side critical</span>
<span class="cm"> * sections can occur.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that the Linux kernel is fully capable of entering an interrupt</span>
<span class="cm"> * handler that it never exits, for example when doing upcalls to</span>
<span class="cm"> * user mode!  This code assumes that the idle loop never does upcalls to</span>
<span class="cm"> * user mode.  If your architecture does do upcalls from the idle loop (or</span>
<span class="cm"> * does anything else that results in unbalanced calls to the irq_enter()</span>
<span class="cm"> * and irq_exit() functions), RCU will give you what you deserve, good</span>
<span class="cm"> * and hard.  But very infrequently and irreproducibly.</span>
<span class="cm"> *</span>
<span class="cm"> * Use things like work queues to work around this limitation.</span>
<span class="cm"> *</span>
<span class="cm"> * You have been warned.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_irq_enter</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span><span class="p">;</span>
	<span class="kt">long</span> <span class="kt">long</span> <span class="n">oldval</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">);</span>
	<span class="n">oldval</span> <span class="o">=</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">;</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="o">++</span><span class="p">;</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">oldval</span><span class="p">)</span>
		<span class="n">trace_rcu_dyntick</span><span class="p">(</span><span class="s">&quot;++=&quot;</span><span class="p">,</span> <span class="n">oldval</span><span class="p">,</span> <span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span><span class="p">);</span>
	<span class="k">else</span>
		<span class="n">rcu_idle_exit_common</span><span class="p">(</span><span class="n">rdtp</span><span class="p">,</span> <span class="n">oldval</span><span class="p">);</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_nmi_enter - inform RCU of entry to NMI context</span>
<span class="cm"> *</span>
<span class="cm"> * If the CPU was idle with dynamic ticks active, and there is no</span>
<span class="cm"> * irq handler running, this updates rdtp-&gt;dynticks_nmi to let the</span>
<span class="cm"> * RCU grace-period handling know that the CPU is active.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_nmi_enter</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nmi_nesting</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
	    <span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">))</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nmi_nesting</span><span class="o">++</span><span class="p">;</span>
	<span class="n">smp_mb__before_atomic_inc</span><span class="p">();</span>  <span class="cm">/* Force delay from prior write. */</span>
	<span class="n">atomic_inc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">);</span>
	<span class="cm">/* CPUs seeing atomic_inc() must see later RCU read-side crit sects */</span>
	<span class="n">smp_mb__after_atomic_inc</span><span class="p">();</span>  <span class="cm">/* See above. */</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">));</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_nmi_exit - inform RCU of exit from NMI context</span>
<span class="cm"> *</span>
<span class="cm"> * If the CPU was idle with dynamic ticks active, and there is no</span>
<span class="cm"> * irq handler running, this updates rdtp-&gt;dynticks_nmi to let the</span>
<span class="cm"> * RCU grace-period handling know that the CPU is no longer active.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_nmi_exit</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_dynticks</span> <span class="o">*</span><span class="n">rdtp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nmi_nesting</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">||</span>
	    <span class="o">--</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks_nmi_nesting</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="cm">/* CPUs seeing atomic_inc() must see prior RCU read-side crit sects */</span>
	<span class="n">smp_mb__before_atomic_inc</span><span class="p">();</span>  <span class="cm">/* See above. */</span>
	<span class="n">atomic_inc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">);</span>
	<span class="n">smp_mb__after_atomic_inc</span><span class="p">();</span>  <span class="cm">/* Force delay to next write. */</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdtp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_PROVE_RCU</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_is_cpu_idle - see if RCU thinks that the current CPU is idle</span>
<span class="cm"> *</span>
<span class="cm"> * If the current CPU is in its idle loop and is neither in an interrupt</span>
<span class="cm"> * or NMI handler, return true.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="nf">rcu_is_cpu_idle</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">ret</span><span class="p">;</span>

	<span class="n">preempt_disable</span><span class="p">();</span>
	<span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">).</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">preempt_enable</span><span class="p">();</span>
	<span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL</span><span class="p">(</span><span class="n">rcu_is_cpu_idle</span><span class="p">);</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/*</span>
<span class="cm"> * Is the current CPU online?  Disable preemption to avoid false positives</span>
<span class="cm"> * that could otherwise happen due to the current CPU number being sampled,</span>
<span class="cm"> * this task being preempted, its old CPU being taken offline, resuming</span>
<span class="cm"> * on some other CPU, then determining that its old CPU is now offline.</span>
<span class="cm"> * It is OK to use RCU on an offline processor during initial boot, hence</span>
<span class="cm"> * the check for rcu_scheduler_fully_active.  Note also that it is OK</span>
<span class="cm"> * for a CPU coming online to use RCU for one jiffy prior to marking itself</span>
<span class="cm"> * online in the cpu_online_mask.  Similarly, it is OK for a CPU going</span>
<span class="cm"> * offline to continue to use RCU for one jiffy after marking itself</span>
<span class="cm"> * offline in the cpu_online_mask.  This leniency is necessary given the</span>
<span class="cm"> * non-atomic nature of the online and offline processing, for example,</span>
<span class="cm"> * the fact that a CPU enters the scheduler after completing the CPU_DYING</span>
<span class="cm"> * notifiers.</span>
<span class="cm"> *</span>
<span class="cm"> * This is also why RCU internally marks CPUs online during the</span>
<span class="cm"> * CPU_UP_PREPARE phase and offline during the CPU_DEAD phase.</span>
<span class="cm"> *</span>
<span class="cm"> * Disable checking if in an NMI handler because we cannot safely report</span>
<span class="cm"> * errors from NMI handlers anyway.</span>
<span class="cm"> */</span>
<span class="n">bool</span> <span class="nf">rcu_lockdep_current_cpu_online</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>
	<span class="n">bool</span> <span class="n">ret</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">in_nmi</span><span class="p">())</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">preempt_disable</span><span class="p">();</span>
	<span class="n">rdp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">);</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span> <span class="o">&amp;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span><span class="p">)</span> <span class="o">||</span>
	      <span class="o">!</span><span class="n">rcu_scheduler_fully_active</span><span class="p">;</span>
	<span class="n">preempt_enable</span><span class="p">();</span>
	<span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_lockdep_current_cpu_online</span><span class="p">);</span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="cp">#endif </span><span class="cm">/* #ifdef CONFIG_PROVE_RCU */</span><span class="cp"></span>

<span class="cm">/**</span>
<span class="cm"> * rcu_is_cpu_rrupt_from_idle - see if idle or immediately interrupted from idle</span>
<span class="cm"> *</span>
<span class="cm"> * If the current CPU is idle or running at a first-level (not nested)</span>
<span class="cm"> * interrupt from idle, return true.  The caller must have at least</span>
<span class="cm"> * disabled preemption.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="nf">rcu_is_cpu_rrupt_from_idle</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">).</span><span class="n">dynticks_nesting</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Snapshot the specified CPU&#39;s dynticks counter so that we can later</span>
<span class="cm"> * credit them with an implicit quiescent state.  Return 1 if this CPU</span>
<span class="cm"> * is in dynticks idle mode, which is an extended quiescent state.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">dyntick_save_progress_counter</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks_snap</span> <span class="o">=</span> <span class="n">atomic_add_return</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">);</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks_snap</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Return true if the specified CPU has passed through a quiescent</span>
<span class="cm"> * state by virtue of being in or having passed through an dynticks</span>
<span class="cm"> * idle state since the last call to dyntick_save_progress_counter()</span>
<span class="cm"> * for this same CPU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_implicit_dynticks_qs</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">curr</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">snap</span><span class="p">;</span>

	<span class="n">curr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="p">)</span><span class="n">atomic_add_return</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">);</span>
	<span class="n">snap</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="p">)</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks_snap</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * If the CPU passed through or entered a dynticks idle phase with</span>
<span class="cm">	 * no active irq/NMI handlers, then we can safely pretend that the CPU</span>
<span class="cm">	 * already acknowledged the request to pass through a quiescent</span>
<span class="cm">	 * state.  Either way, that CPU cannot possibly be in an RCU</span>
<span class="cm">	 * read-side critical section that started before the beginning</span>
<span class="cm">	 * of the current RCU grace period.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">((</span><span class="n">curr</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">UINT_CMP_GE</span><span class="p">(</span><span class="n">curr</span><span class="p">,</span> <span class="n">snap</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">trace_rcu_fqs</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">cpu</span><span class="p">,</span> <span class="s">&quot;dti&quot;</span><span class="p">);</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks_fqs</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Go check for the CPU being offline. */</span>
	<span class="k">return</span> <span class="n">rcu_implicit_offline_qs</span><span class="p">(</span><span class="n">rdp</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">int</span> <span class="nf">jiffies_till_stall_check</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">till_stall_check</span> <span class="o">=</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rcu_cpu_stall_timeout</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Limit check must be consistent with the Kconfig limits</span>
<span class="cm">	 * for CONFIG_RCU_CPU_STALL_TIMEOUT.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">till_stall_check</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rcu_cpu_stall_timeout</span><span class="p">)</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
		<span class="n">till_stall_check</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">till_stall_check</span> <span class="o">&gt;</span> <span class="mi">300</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rcu_cpu_stall_timeout</span><span class="p">)</span> <span class="o">=</span> <span class="mi">300</span><span class="p">;</span>
		<span class="n">till_stall_check</span> <span class="o">=</span> <span class="mi">300</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">till_stall_check</span> <span class="o">*</span> <span class="n">HZ</span> <span class="o">+</span> <span class="n">RCU_STALL_DELAY_DELTA</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">record_gp_stall_check_time</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_start</span> <span class="o">=</span> <span class="n">jiffies</span><span class="p">;</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_stall</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">jiffies_till_stall_check</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_other_cpu_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="kt">long</span> <span class="n">delta</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">ndetected</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="cm">/* Only let one CPU complain about others per time interval. */</span>

	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">delta</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">-</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_stall</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">delta</span> <span class="o">&lt;</span> <span class="n">RCU_STALL_RAT_DELAY</span> <span class="o">||</span> <span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_stall</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">jiffies_till_stall_check</span><span class="p">()</span> <span class="o">+</span> <span class="mi">3</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * OK, time to rat on our buddy...</span>
<span class="cm">	 * See Documentation/RCU/stallwarn.txt for info on how to debug</span>
<span class="cm">	 * RCU CPU stall warnings.</span>
<span class="cm">	 */</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_ERR</span> <span class="s">&quot;INFO: %s detected stalls on CPUs/tasks:&quot;</span><span class="p">,</span>
	       <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">);</span>
	<span class="n">print_cpu_stall_info_begin</span><span class="p">();</span>
	<span class="n">rcu_for_each_leaf_node</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">ndetected</span> <span class="o">+=</span> <span class="n">rcu_print_task_stall</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="k">continue</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="n">cpu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">cpu</span> <span class="o">&lt;=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span> <span class="o">-</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">;</span> <span class="n">cpu</span><span class="o">++</span><span class="p">)</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1UL</span> <span class="o">&lt;&lt;</span> <span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
				<span class="n">print_cpu_stall_info</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span> <span class="o">+</span> <span class="n">cpu</span><span class="p">);</span>
				<span class="n">ndetected</span><span class="o">++</span><span class="p">;</span>
			<span class="p">}</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Now rat on any tasks that got kicked up to the root rcu_node</span>
<span class="cm">	 * due to CPU offlining.</span>
<span class="cm">	 */</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">ndetected</span> <span class="o">=</span> <span class="n">rcu_print_task_stall</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="n">print_cpu_stall_info_end</span><span class="p">();</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot;(detected by %d, t=%ld jiffies)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
	       <span class="n">smp_processor_id</span><span class="p">(),</span> <span class="p">(</span><span class="kt">long</span><span class="p">)(</span><span class="n">jiffies</span> <span class="o">-</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_start</span><span class="p">));</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">ndetected</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">printk</span><span class="p">(</span><span class="n">KERN_ERR</span> <span class="s">&quot;INFO: Stall ended before state dump start</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">trigger_all_cpu_backtrace</span><span class="p">())</span>
		<span class="n">dump_stack</span><span class="p">();</span>

	<span class="cm">/* If so configured, complain about tasks blocking the grace period. */</span>

	<span class="n">rcu_print_detail_task_stall</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="n">force_quiescent_state</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>  <span class="cm">/* Kick them all. */</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_cpu_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * OK, time to rat on ourselves...</span>
<span class="cm">	 * See Documentation/RCU/stallwarn.txt for info on how to debug</span>
<span class="cm">	 * RCU CPU stall warnings.</span>
<span class="cm">	 */</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_ERR</span> <span class="s">&quot;INFO: %s self-detected stall on CPU&quot;</span><span class="p">,</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">);</span>
	<span class="n">print_cpu_stall_info_begin</span><span class="p">();</span>
	<span class="n">print_cpu_stall_info</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">smp_processor_id</span><span class="p">());</span>
	<span class="n">print_cpu_stall_info_end</span><span class="p">();</span>
	<span class="n">printk</span><span class="p">(</span><span class="n">KERN_CONT</span> <span class="s">&quot; (t=%lu jiffies)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">jiffies</span> <span class="o">-</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_start</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">trigger_all_cpu_backtrace</span><span class="p">())</span>
		<span class="n">dump_stack</span><span class="p">();</span>

	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">ULONG_CMP_GE</span><span class="p">(</span><span class="n">jiffies</span><span class="p">,</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_stall</span><span class="p">))</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_stall</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span>
				     <span class="mi">3</span> <span class="o">*</span> <span class="n">jiffies_till_stall_check</span><span class="p">()</span> <span class="o">+</span> <span class="mi">3</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="n">set_need_resched</span><span class="p">();</span>  <span class="cm">/* kick ourselves to get things going. */</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">check_cpu_stall</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">j</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">js</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_cpu_stall_suppress</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">j</span> <span class="o">=</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">jiffies</span><span class="p">);</span>
	<span class="n">js</span> <span class="o">=</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_stall</span><span class="p">);</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">((</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">ULONG_CMP_GE</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">js</span><span class="p">))</span> <span class="p">{</span>

		<span class="cm">/* We haven&#39;t checked in, so go dump stack. */</span>
		<span class="n">print_cpu_stall</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
		   <span class="n">ULONG_CMP_GE</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">js</span> <span class="o">+</span> <span class="n">RCU_STALL_RAT_DELAY</span><span class="p">))</span> <span class="p">{</span>

		<span class="cm">/* They had a few time units to dump stack, so complain. */</span>
		<span class="n">print_other_cpu_stall</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_panic</span><span class="p">(</span><span class="k">struct</span> <span class="n">notifier_block</span> <span class="o">*</span><span class="n">this</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">ev</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">ptr</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_cpu_stall_suppress</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">NOTIFY_DONE</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_cpu_stall_reset - prevent further stall warnings in current grace period</span>
<span class="cm"> *</span>
<span class="cm"> * Set the stall-warning timeout way off into the future, thus preventing</span>
<span class="cm"> * any RCU CPU stall-warning messages from appearing in the current set of</span>
<span class="cm"> * RCU grace periods.</span>
<span class="cm"> *</span>
<span class="cm"> * The caller must disable hard irqs.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_cpu_stall_reset</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_sched_state</span><span class="p">.</span><span class="n">jiffies_stall</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">ULONG_MAX</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
	<span class="n">rcu_bh_state</span><span class="p">.</span><span class="n">jiffies_stall</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">ULONG_MAX</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
	<span class="n">rcu_preempt_stall_reset</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="k">struct</span> <span class="n">notifier_block</span> <span class="n">rcu_panic_block</span> <span class="o">=</span> <span class="p">{</span>
	<span class="p">.</span><span class="n">notifier_call</span> <span class="o">=</span> <span class="n">rcu_panic</span><span class="p">,</span>
<span class="p">};</span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">check_cpu_stall_init</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">atomic_notifier_chain_register</span><span class="p">(</span><span class="o">&amp;</span><span class="n">panic_notifier_list</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_panic_block</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Update CPU-local rcu_data state to record the newly noticed grace period.</span>
<span class="cm"> * This is used both when we started the grace period and when we notice</span>
<span class="cm"> * that someone else started the grace period.  The caller must hold the</span>
<span class="cm"> * -&gt;lock of the leaf rcu_node structure corresponding to the current CPU,</span>
<span class="cm"> *  and must have irqs disabled.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">__note_new_gpnum</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">!=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="p">{</span>
		<span class="cm">/*</span>
<span class="cm">		 * If the current grace period is waiting for this CPU,</span>
<span class="cm">		 * set up to detect a quiescent state, otherwise don&#39;t</span>
<span class="cm">		 * go looking for one.</span>
<span class="cm">		 */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
		<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;cpustart&quot;</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="p">}</span> <span class="k">else</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">zero_cpu_stall_ticks</span><span class="p">(</span><span class="n">rdp</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">note_new_gpnum</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">==</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="o">||</span> <span class="cm">/* outside lock. */</span>
	    <span class="o">!</span><span class="n">raw_spin_trylock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">))</span> <span class="p">{</span> <span class="cm">/* irqs already off, so later. */</span>
		<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">__note_new_gpnum</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Did someone else start a new RCU grace period start since we last</span>
<span class="cm"> * checked?  Update local state appropriately if so.  Must be called</span>
<span class="cm"> * on the CPU corresponding to rdp.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span>
<span class="nf">check_for_new_grace_period</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">!=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">note_new_gpnum</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
		<span class="n">ret</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Advance this CPU&#39;s callbacks, but only if the current grace period</span>
<span class="cm"> * has ended.  This may be called only from the CPU to whom the rdp</span>
<span class="cm"> * belongs.  In addition, the corresponding leaf rcu_node structure&#39;s</span>
<span class="cm"> * -&gt;lock must be held by the caller, with irqs disabled.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">__rcu_process_gp_end</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="cm">/* Did another grace period end? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">!=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">)</span> <span class="p">{</span>

		<span class="cm">/* Advance callbacks.  No harm if list empty. */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_WAIT_TAIL</span><span class="p">];</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_WAIT_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_READY_TAIL</span><span class="p">];</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_READY_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">];</span>

		<span class="cm">/* Remember that we saw this grace-period completion. */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">;</span>
		<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;cpuend&quot;</span><span class="p">);</span>

		<span class="cm">/*</span>
<span class="cm">		 * If we were in an extended quiescent state, we may have</span>
<span class="cm">		 * missed some grace periods that others CPUs handled on</span>
<span class="cm">		 * our behalf. Catch up with this state to avoid noting</span>
<span class="cm">		 * spurious new grace periods.  If another grace period</span>
<span class="cm">		 * has started, then rnp-&gt;gpnum will have advanced, so</span>
<span class="cm">		 * we will detect this later on.</span>
<span class="cm">		 */</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">))</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">;</span>

		<span class="cm">/*</span>
<span class="cm">		 * If RCU does not need a quiescent state from this CPU,</span>
<span class="cm">		 * then make sure that this CPU doesn&#39;t go looking for one.</span>
<span class="cm">		 */</span>
		<span class="k">if</span> <span class="p">((</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Advance this CPU&#39;s callbacks, but only if the current grace period</span>
<span class="cm"> * has ended.  This may be called only from the CPU to whom the rdp</span>
<span class="cm"> * belongs.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_process_gp_end</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">==</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">)</span> <span class="o">||</span> <span class="cm">/* outside lock. */</span>
	    <span class="o">!</span><span class="n">raw_spin_trylock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">))</span> <span class="p">{</span> <span class="cm">/* irqs already off, so later. */</span>
		<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">__rcu_process_gp_end</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Do per-CPU grace-period initialization for running CPU.  The caller</span>
<span class="cm"> * must hold the lock of the leaf rcu_node structure corresponding to</span>
<span class="cm"> * this CPU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_start_gp_per_cpu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="cm">/* Prior grace period ended, so advance callbacks for current CPU. */</span>
	<span class="n">__rcu_process_gp_end</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Because this CPU just now started the new grace period, we know</span>
<span class="cm">	 * that all of its callbacks will be covered by this upcoming grace</span>
<span class="cm">	 * period, even the ones that were registered arbitrarily recently.</span>
<span class="cm">	 * Therefore, advance all outstanding callbacks to RCU_WAIT_TAIL.</span>
<span class="cm">	 *</span>
<span class="cm">	 * Other CPUs cannot be sure exactly when the grace period started.</span>
<span class="cm">	 * Therefore, their recently registered callbacks must pass through</span>
<span class="cm">	 * an additional RCU_NEXT_READY stage, so that they will be handled</span>
<span class="cm">	 * by the next RCU grace period.</span>
<span class="cm">	 */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_READY_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">];</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_WAIT_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">];</span>

	<span class="cm">/* Set state so that this CPU will detect the next quiescent state. */</span>
	<span class="n">__note_new_gpnum</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Start a new RCU grace period if warranted, re-initializing the hierarchy</span>
<span class="cm"> * in preparation for detecting the next grace period.  The caller must hold</span>
<span class="cm"> * the root node&#39;s -&gt;lock, which is released before return.  Hard irqs must</span>
<span class="cm"> * be disabled.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that it is legal for a dying CPU (which is marked as offline) to</span>
<span class="cm"> * invoke this function.  This can happen when the dying CPU reports its</span>
<span class="cm"> * quiescent state.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_start_gp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">this_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_scheduler_fully_active</span> <span class="o">||</span>
	    <span class="o">!</span><span class="n">cpu_needs_another_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">))</span> <span class="p">{</span>
		<span class="cm">/*</span>
<span class="cm">		 * Either the scheduler hasn&#39;t yet spawned the first</span>
<span class="cm">		 * non-idle task or this CPU does not need another</span>
<span class="cm">		 * grace period.  Either way, don&#39;t start a new grace</span>
<span class="cm">		 * period.</span>
<span class="cm">		 */</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_active</span><span class="p">)</span> <span class="p">{</span>
		<span class="cm">/*</span>
<span class="cm">		 * This CPU needs a grace period, but force_quiescent_state()</span>
<span class="cm">		 * is running.  Tell it to start one on this CPU&#39;s behalf.</span>
<span class="cm">		 */</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_need_gp</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Advance to a new grace period and initialize state. */</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="o">++</span><span class="p">;</span>
	<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;start&quot;</span><span class="p">);</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_state</span> <span class="o">==</span> <span class="n">RCU_GP_INIT</span><span class="p">);</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_state</span> <span class="o">=</span> <span class="n">RCU_GP_INIT</span><span class="p">;</span> <span class="cm">/* Hold off force_quiescent_state. */</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">RCU_JIFFIES_TILL_FORCE_QS</span><span class="p">;</span>
	<span class="n">record_gp_stall_check_time</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* leave irqs disabled. */</span>

	<span class="cm">/* Exclude any concurrent CPU-hotplug operations. */</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">);</span>  <span class="cm">/* irqs already disabled. */</span>

	<span class="cm">/*</span>
<span class="cm">	 * Set the quiescent-state-needed bits in all the rcu_node</span>
<span class="cm">	 * structures for all currently online CPUs in breadth-first</span>
<span class="cm">	 * order, starting from the root rcu_node structure.  This</span>
<span class="cm">	 * operation relies on the layout of the hierarchy within the</span>
<span class="cm">	 * rsp-&gt;node[] array.  Note that other CPUs will access only</span>
<span class="cm">	 * the leaves of the hierarchy, which still indicate that no</span>
<span class="cm">	 * grace period is in progress, at least until the corresponding</span>
<span class="cm">	 * leaf node has been initialized.  In addition, we have excluded</span>
<span class="cm">	 * CPU-hotplug operations.</span>
<span class="cm">	 *</span>
<span class="cm">	 * Note that the grace period cannot complete until we finish</span>
<span class="cm">	 * the initialization process, as there will be at least one</span>
<span class="cm">	 * qsmask bit set in the root node until that time, namely the</span>
<span class="cm">	 * one corresponding to this CPU, due to the fact that we have</span>
<span class="cm">	 * irqs disabled.</span>
<span class="cm">	 */</span>
	<span class="n">rcu_for_each_node_breadth_first</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* irqs already disabled. */</span>
		<span class="n">rcu_preempt_check_blocked_tasks</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span><span class="p">;</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">)</span>
			<span class="n">rcu_start_gp_per_cpu</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
		<span class="n">rcu_preempt_boost_start_gp</span><span class="p">(</span><span class="n">rnp</span><span class="p">);</span>
		<span class="n">trace_rcu_grace_period_init</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span>
					    <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">,</span>
					    <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">);</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* irqs remain disabled. */</span>
	<span class="p">}</span>

	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>		<span class="cm">/* irqs already disabled. */</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_state</span> <span class="o">=</span> <span class="n">RCU_SIGNAL_INIT</span><span class="p">;</span> <span class="cm">/* force_quiescent_state now OK. */</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>		<span class="cm">/* irqs remain disabled. */</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Report a full set of quiescent states to the specified rcu_state</span>
<span class="cm"> * data structure.  This involves cleaning up after the prior grace</span>
<span class="cm"> * period and letting rcu_start_gp() start up the next grace period</span>
<span class="cm"> * if one is needed.  Note that the caller must hold rnp-&gt;lock, as</span>
<span class="cm"> * required by rcu_start_gp(), which will release it.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_report_qs_rsp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">gp_duration</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">this_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">);</span>

	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">));</span>

	<span class="cm">/*</span>
<span class="cm">	 * Ensure that all grace-period and pre-grace-period activity</span>
<span class="cm">	 * is seen before the assignment to rsp-&gt;completed.</span>
<span class="cm">	 */</span>
	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* See above block comment. */</span>
	<span class="n">gp_duration</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">-</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_start</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">gp_duration</span> <span class="o">&gt;</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_max</span><span class="p">)</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gp_max</span> <span class="o">=</span> <span class="n">gp_duration</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * We know the grace period is complete, but to everyone else</span>
<span class="cm">	 * it appears to still be ongoing.  But it is also the case</span>
<span class="cm">	 * that to everyone else it looks like there is nothing that</span>
<span class="cm">	 * they can do to advance the grace period.  It is therefore</span>
<span class="cm">	 * safe for us to drop the lock in order to mark the grace</span>
<span class="cm">	 * period as completed in all of the rcu_node structures.</span>
<span class="cm">	 *</span>
<span class="cm">	 * But if this CPU needs another grace period, it will take</span>
<span class="cm">	 * care of this while initializing the next grace period.</span>
<span class="cm">	 * We use RCU_WAIT_TAIL instead of the usual RCU_DONE_TAIL</span>
<span class="cm">	 * because the callbacks have not yet been advanced: Those</span>
<span class="cm">	 * callbacks are waiting on the grace period that just now</span>
<span class="cm">	 * completed.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_WAIT_TAIL</span><span class="p">]</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	 <span class="cm">/* irqs remain disabled. */</span>

		<span class="cm">/*</span>
<span class="cm">		 * Propagate new -&gt;completed value to rcu_node structures</span>
<span class="cm">		 * so that other CPUs don&#39;t have to wait until the start</span>
<span class="cm">		 * of the next grace period to process their callbacks.</span>
<span class="cm">		 */</span>
		<span class="n">rcu_for_each_node_breadth_first</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled. */</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>
			<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled. */</span>
		<span class="p">}</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled. */</span>
	<span class="p">}</span>

	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">;</span>  <span class="cm">/* Declare the grace period complete. */</span>
	<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">,</span> <span class="s">&quot;end&quot;</span><span class="p">);</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_state</span> <span class="o">=</span> <span class="n">RCU_GP_IDLE</span><span class="p">;</span>
	<span class="n">rcu_start_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>  <span class="cm">/* releases root node&#39;s rnp-&gt;lock. */</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Similar to rcu_report_qs_rdp(), for which it is a helper function.</span>
<span class="cm"> * Allows quiescent states for a group of CPUs to be reported at one go</span>
<span class="cm"> * to the specified rcu_node structure, though all the CPUs in the group</span>
<span class="cm"> * must be represented by the same rcu_node structure (which need not be</span>
<span class="cm"> * a leaf rcu_node structure, though it often will be).  That structure&#39;s</span>
<span class="cm"> * lock must be held upon entry, and it is released before return.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_report_qs_rnp</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
		  <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">)</span>
	<span class="n">__releases</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp_c</span><span class="p">;</span>

	<span class="cm">/* Walk up the rcu_node hierarchy. */</span>
	<span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">))</span> <span class="p">{</span>

			<span class="cm">/* Our bit has already been cleared, so done. */</span>
			<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;=</span> <span class="o">~</span><span class="n">mask</span><span class="p">;</span>
		<span class="n">trace_rcu_quiescent_state_report</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span>
						 <span class="n">mask</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">,</span>
						 <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">,</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">,</span>
						 <span class="o">!!</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gp_tasks</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">rcu_preempt_blocked_readers_cgp</span><span class="p">(</span><span class="n">rnp</span><span class="p">))</span> <span class="p">{</span>

			<span class="cm">/* Other bits still set at this level, so done. */</span>
			<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>

			<span class="cm">/* No more levels.  Exit loop holding root lock. */</span>

			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">rnp_c</span> <span class="o">=</span> <span class="n">rnp</span><span class="p">;</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">;</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rnp_c</span><span class="o">-&gt;</span><span class="n">qsmask</span><span class="p">);</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Get here if we are the last CPU to pass through a quiescent</span>
<span class="cm">	 * state for this grace period.  Invoke rcu_report_qs_rsp()</span>
<span class="cm">	 * to clean up and start the next grace period if one is needed.</span>
<span class="cm">	 */</span>
	<span class="n">rcu_report_qs_rsp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span> <span class="cm">/* releases rnp-&gt;lock. */</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Record a quiescent state for the specified CPU to that CPU&#39;s rcu_data</span>
<span class="cm"> * structure.  This must be either called from the specified CPU, or</span>
<span class="cm"> * called when the specified CPU is known to be offline (and when it is</span>
<span class="cm"> * also known that no other CPU is concurrently trying to help the offline</span>
<span class="cm"> * CPU).  The lastcomp argument is used to make sure we are still in the</span>
<span class="cm"> * grace period of interest.  We don&#39;t want to end the current grace period</span>
<span class="cm"> * based on quiescent states detected in an earlier grace period!</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_report_qs_rdp</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">,</span> <span class="kt">long</span> <span class="n">lastgp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">lastgp</span> <span class="o">!=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">||</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">==</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="p">{</span>

		<span class="cm">/*</span>
<span class="cm">		 * The grace period in which this quiescent state was</span>
<span class="cm">		 * recorded has ended, so don&#39;t report it upwards.</span>
<span class="cm">		 * We will instead need a new quiescent state that lies</span>
<span class="cm">		 * within the current grace period.</span>
<span class="cm">		 */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>	<span class="cm">/* need qs for new gp. */</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">mask</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">((</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

		<span class="cm">/*</span>
<span class="cm">		 * This GP can&#39;t end until cpu checks in, so all of our</span>
<span class="cm">		 * callbacks can be processed during the next GP.</span>
<span class="cm">		 */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_READY_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">];</span>

		<span class="n">rcu_report_qs_rnp</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span> <span class="cm">/* rlses rnp-&gt;lock */</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if there is a new grace period of which this CPU</span>
<span class="cm"> * is not yet aware, and if so, set up local rcu_data state for it.</span>
<span class="cm"> * Otherwise, see if this CPU has just passed through its first</span>
<span class="cm"> * quiescent state for this grace period, and record that fact if so.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_check_quiescent_state</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="cm">/* If there is now a new grace period, record and return. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">check_for_new_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">))</span>
		<span class="k">return</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * Does this CPU still need to do its part for current grace period?</span>
<span class="cm">	 * If no, return and let the other CPUs do their part as well.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * Was there a quiescent state since the beginning of the grace</span>
<span class="cm">	 * period? If no, then exit and wait for the next call.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * Tell RCU we are done (but rcu_report_qs_rdp() will be the</span>
<span class="cm">	 * judge of that).</span>
<span class="cm">	 */</span>
	<span class="n">rcu_report_qs_rdp</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">cpu</span><span class="p">,</span> <span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce_gpnum</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#ifdef CONFIG_HOTPLUG_CPU</span>

<span class="cm">/*</span>
<span class="cm"> * Send the specified CPU&#39;s RCU callbacks to the orphanage.  The</span>
<span class="cm"> * specified CPU must be offline, and the caller must hold the</span>
<span class="cm"> * -&gt;onofflock.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">rcu_send_cbs_to_orphanage</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
			  <span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * Orphan the callbacks.  First adjust the counts.  This is safe</span>
<span class="cm">	 * because -&gt;onofflock excludes _rcu_barrier()&#39;s adoption of</span>
<span class="cm">	 * the callbacks, thus no memory barrier is required.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">+=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">;</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">+=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">;</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_cbs_orphaned</span> <span class="o">+=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">;</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Next, move those callbacks still needing a grace period to</span>
<span class="cm">	 * the orphanage, where some other CPU will pick them up.</span>
<span class="cm">	 * Some of the callbacks might have gone partway through a grace</span>
<span class="cm">	 * period, but that is too bad.  They get to start over because we</span>
<span class="cm">	 * cannot assume that grace periods are synchronized across CPUs.</span>
<span class="cm">	 * We don&#39;t bother updating the -&gt;nxttail[] array yet, instead</span>
<span class="cm">	 * we just reset the whole thing later on.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxttail</span> <span class="o">=</span> <span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">];</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxttail</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">];</span>
		<span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Then move the ready-to-invoke callbacks to the orphanage,</span>
<span class="cm">	 * where some other CPU will pick them up.  These will not be</span>
<span class="cm">	 * required to pass though another grace period: They are done.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donetail</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">;</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donetail</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">];</span>
	<span class="p">}</span>

	<span class="cm">/* Finally, initialize the rcu_data structure&#39;s list to empty.  */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">RCU_NEXT_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Adopt the RCU callbacks from the specified rcu_state structure&#39;s</span>
<span class="cm"> * orphanage.  The caller must hold the -&gt;onofflock.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_adopt_orphan_cbs</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">__this_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * If there is an rcu_barrier() operation in progress, then</span>
<span class="cm">	 * only the task doing that operation is permitted to adopt</span>
<span class="cm">	 * callbacks.  To do otherwise breaks rcu_barrier() and friends</span>
<span class="cm">	 * by causing them to fail to wait for the callbacks in the</span>
<span class="cm">	 * orphanage.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rcu_barrier_in_progress</span> <span class="o">&amp;&amp;</span>
	    <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rcu_barrier_in_progress</span> <span class="o">!=</span> <span class="n">current</span><span class="p">)</span>
		<span class="k">return</span><span class="p">;</span>

	<span class="cm">/* Do the accounting first. */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">+=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">+=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_cbs_adopted</span> <span class="o">+=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">!=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">)</span>
		<span class="n">rcu_idle_count_callbacks_posted</span><span class="p">();</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="cm">/*</span>
<span class="cm">	 * We do not need a memory barrier here because the only way we</span>
<span class="cm">	 * can get here if there is an rcu_barrier() in flight is if</span>
<span class="cm">	 * we are the task doing the rcu_barrier().</span>
<span class="cm">	 */</span>

	<span class="cm">/* First adopt the ready-to-invoke callbacks. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donelist</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donetail</span> <span class="o">=</span> <span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">];</span>
		<span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donelist</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">RCU_NEXT_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">RCU_DONE_TAIL</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">])</span>
				<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donetail</span><span class="p">;</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donelist</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donetail</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_donelist</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* And then adopt the callbacks that still need a grace period. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxtlist</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxtlist</span><span class="p">;</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxttail</span><span class="p">;</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxtlist</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxttail</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">orphan_nxtlist</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Trace the fact that this CPU is going offline.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cleanup_dying_cpu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">RCU_TRACE</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">);</span>
	<span class="n">RCU_TRACE</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">this_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">));</span>
	<span class="n">RCU_TRACE</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">);</span>

	<span class="n">RCU_TRACE</span><span class="p">(</span><span class="n">mask</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">);</span>
	<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span>
			       <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="o">!!</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">),</span>
			       <span class="s">&quot;cpuofl&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * The CPU has been completely removed, and some other CPU is reporting</span>
<span class="cm"> * this fact from process context.  Do the remainder of the cleanup,</span>
<span class="cm"> * including orphaning the outgoing CPU&#39;s RCU callbacks, and also</span>
<span class="cm"> * adopting them, if there is no _rcu_barrier() instance running.</span>
<span class="cm"> * There can only be one CPU hotplug operation at a time, so no other</span>
<span class="cm"> * CPU can be attempting to update rcu_cpu_kthread_task.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cleanup_dead_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">need_report</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>  <span class="cm">/* Outgoing CPU&#39;s rdp &amp; rnp. */</span>

	<span class="cm">/* Adjust any no-longer-needed kthreads. */</span>
	<span class="n">rcu_stop_cpu_kthread</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
	<span class="n">rcu_node_kthread_setaffinity</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>

	<span class="cm">/* Remove the dead CPU from the bitmasks in the rcu_node hierarchy. */</span>

	<span class="cm">/* Exclude any attempts to start a new grace period. */</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="cm">/* Orphan the dead CPU&#39;s callbacks, and adopt them if appropriate. */</span>
	<span class="n">rcu_send_cbs_to_orphanage</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
	<span class="n">rcu_adopt_orphan_cbs</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="cm">/* Remove the outgoing CPU from the masks in the rcu_node hierarchy. */</span>
	<span class="n">mask</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>	<span class="cm">/* rnp-&gt;grplo is constant. */</span>
	<span class="k">do</span> <span class="p">{</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* irqs already disabled. */</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span> <span class="o">&amp;=</span> <span class="o">~</span><span class="n">mask</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">!=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">)</span>
				<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled. */</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">)</span>
			<span class="n">need_report</span> <span class="o">=</span> <span class="n">rcu_preempt_offline_tasks</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
		<span class="k">else</span>
			<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled. */</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * We still hold the leaf rcu_node structure lock here, and</span>
<span class="cm">	 * irqs are still disabled.  The reason for this subterfuge is</span>
<span class="cm">	 * because invoking rcu_report_unblock_qs_rnp() with -&gt;onofflock</span>
<span class="cm">	 * held leads to deadlock.</span>
<span class="cm">	 */</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled. */</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">need_report</span> <span class="o">&amp;</span> <span class="n">RCU_OFL_TASKS_NORM_GP</span><span class="p">)</span>
		<span class="n">rcu_report_unblock_qs_rnp</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">else</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">need_report</span> <span class="o">&amp;</span> <span class="n">RCU_OFL_TASKS_EXP_GP</span><span class="p">)</span>
		<span class="n">rcu_report_exp_rnp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_adopt_orphan_cbs</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cleanup_dying_cpu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_cleanup_dead_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_HOTPLUG_CPU */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Invoke any RCU callbacks that have made it to the end of their grace</span>
<span class="cm"> * period.  Thottle as specified by rdp-&gt;blimit.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_do_batch</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">next</span><span class="p">,</span> <span class="o">*</span><span class="n">list</span><span class="p">,</span> <span class="o">**</span><span class="n">tail</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">bl</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">count_lazy</span><span class="p">,</span> <span class="n">i</span><span class="p">;</span>

	<span class="cm">/* If no callbacks are ready, just return.*/</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">cpu_has_callbacks_ready_to_invoke</span><span class="p">(</span><span class="n">rdp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">trace_rcu_batch_start</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">trace_rcu_batch_end</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">!!</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">),</span>
				    <span class="n">need_resched</span><span class="p">(),</span> <span class="n">is_idle_task</span><span class="p">(</span><span class="n">current</span><span class="p">),</span>
				    <span class="n">rcu_is_callbacks_kthread</span><span class="p">());</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Extract the list of ready callbacks, disabling to prevent</span>
<span class="cm">	 * races with call_rcu() from interrupt handlers.</span>
<span class="cm">	 */</span>
	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">cpu_is_offline</span><span class="p">(</span><span class="n">smp_processor_id</span><span class="p">()));</span>
	<span class="n">bl</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">blimit</span><span class="p">;</span>
	<span class="n">trace_rcu_batch_start</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">,</span> <span class="n">bl</span><span class="p">);</span>
	<span class="n">list</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">=</span> <span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">];</span>
	<span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="n">tail</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">];</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">RCU_NEXT_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">])</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">;</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>

	<span class="cm">/* Invoke callbacks. */</span>
	<span class="n">count</span> <span class="o">=</span> <span class="n">count_lazy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">while</span> <span class="p">(</span><span class="n">list</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">next</span> <span class="o">=</span> <span class="n">list</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
		<span class="n">prefetch</span><span class="p">(</span><span class="n">next</span><span class="p">);</span>
		<span class="n">debug_rcu_head_unqueue</span><span class="p">(</span><span class="n">list</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">__rcu_reclaim</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">list</span><span class="p">))</span>
			<span class="n">count_lazy</span><span class="o">++</span><span class="p">;</span>
		<span class="n">list</span> <span class="o">=</span> <span class="n">next</span><span class="p">;</span>
		<span class="cm">/* Stop only if limit reached and CPU has something to do. */</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">++</span><span class="n">count</span> <span class="o">&gt;=</span> <span class="n">bl</span> <span class="o">&amp;&amp;</span>
		    <span class="p">(</span><span class="n">need_resched</span><span class="p">()</span> <span class="o">||</span>
		     <span class="p">(</span><span class="o">!</span><span class="n">is_idle_task</span><span class="p">(</span><span class="n">current</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">rcu_is_callbacks_kthread</span><span class="p">())))</span>
			<span class="k">break</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">trace_rcu_batch_end</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="o">!!</span><span class="n">list</span><span class="p">,</span> <span class="n">need_resched</span><span class="p">(),</span>
			    <span class="n">is_idle_task</span><span class="p">(</span><span class="n">current</span><span class="p">),</span>
			    <span class="n">rcu_is_callbacks_kthread</span><span class="p">());</span>

	<span class="cm">/* Update count, and requeue any remaining callbacks. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">list</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">tail</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">;</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">=</span> <span class="n">list</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">RCU_NEXT_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
			<span class="k">if</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
				<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tail</span><span class="p">;</span>
			<span class="k">else</span>
				<span class="k">break</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* List handling before counting for rcu_barrier(). */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">-=</span> <span class="n">count_lazy</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">-=</span> <span class="n">count</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_cbs_invoked</span> <span class="o">+=</span> <span class="n">count</span><span class="p">;</span>

	<span class="cm">/* Reinstate batch limit if we have worked down the excess. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">blimit</span> <span class="o">==</span> <span class="n">LONG_MAX</span> <span class="o">&amp;&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">&lt;=</span> <span class="n">qlowmark</span><span class="p">)</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">blimit</span> <span class="o">=</span> <span class="n">blimit</span><span class="p">;</span>

	<span class="cm">/* Reset -&gt;qlen_last_fqs_check trigger if enough CBs have drained. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_force_qs_snap</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">&lt;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">-</span> <span class="n">qhimark</span><span class="p">)</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">;</span>

	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>

	<span class="cm">/* Re-invoke RCU core processing if there are callbacks remaining. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_has_callbacks_ready_to_invoke</span><span class="p">(</span><span class="n">rdp</span><span class="p">))</span>
		<span class="n">invoke_rcu_core</span><span class="p">();</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if this CPU is in a non-context-switch quiescent state</span>
<span class="cm"> * (user mode or idle loop for rcu, non-softirq execution for rcu_bh).</span>
<span class="cm"> * Also schedule RCU core processing.</span>
<span class="cm"> *</span>
<span class="cm"> * This function must be called from hardirq context.  It is normally</span>
<span class="cm"> * invoked from the scheduling-clock interrupt.  If rcu_pending returns</span>
<span class="cm"> * false, there is no point in invoking rcu_check_callbacks().</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_check_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="kt">int</span> <span class="n">user</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start scheduler-tick&quot;</span><span class="p">);</span>
	<span class="n">increment_cpu_stall_ticks</span><span class="p">();</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">user</span> <span class="o">||</span> <span class="n">rcu_is_cpu_rrupt_from_idle</span><span class="p">())</span> <span class="p">{</span>

		<span class="cm">/*</span>
<span class="cm">		 * Get here if this CPU took its interrupt from user</span>
<span class="cm">		 * mode or from the idle loop, and if this is not a</span>
<span class="cm">		 * nested interrupt.  In this case, the CPU is in</span>
<span class="cm">		 * a quiescent state, so note it.</span>
<span class="cm">		 *</span>
<span class="cm">		 * No memory barrier is required here because both</span>
<span class="cm">		 * rcu_sched_qs() and rcu_bh_qs() reference only CPU-local</span>
<span class="cm">		 * variables that other CPUs neither access nor modify,</span>
<span class="cm">		 * at least not while the corresponding CPU is online.</span>
<span class="cm">		 */</span>

		<span class="n">rcu_sched_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="n">rcu_bh_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>

	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">in_softirq</span><span class="p">())</span> <span class="p">{</span>

		<span class="cm">/*</span>
<span class="cm">		 * Get here if this CPU did not take its interrupt from</span>
<span class="cm">		 * softirq, in other words, if it is not interrupting</span>
<span class="cm">		 * a rcu_bh read-side critical section.  This is an _bh</span>
<span class="cm">		 * critical section, so note it.</span>
<span class="cm">		 */</span>

		<span class="n">rcu_bh_qs</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="n">rcu_preempt_check_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_pending</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span>
		<span class="n">invoke_rcu_core</span><span class="p">();</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End scheduler-tick&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Scan the leaf rcu_node structures, processing dyntick state for any that</span>
<span class="cm"> * have not yet encountered a quiescent state, using the function specified.</span>
<span class="cm"> * Also initiate boosting for any threads blocked on the root rcu_node.</span>
<span class="cm"> *</span>
<span class="cm"> * The caller must have suppressed start of new grace periods.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">force_qs_rnp</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">f</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="p">))</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">bit</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="n">rcu_for_each_leaf_node</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">rcu_initiate_boost</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span> <span class="cm">/* releases rnp-&gt;lock */</span>
			<span class="k">continue</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">cpu</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">;</span>
		<span class="n">bit</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(;</span> <span class="n">cpu</span> <span class="o">&lt;=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">;</span> <span class="n">cpu</span><span class="o">++</span><span class="p">,</span> <span class="n">bit</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">if</span> <span class="p">((</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">&amp;</span> <span class="n">bit</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
			    <span class="n">f</span><span class="p">(</span><span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">)))</span>
				<span class="n">mask</span> <span class="o">|=</span> <span class="n">bit</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">mask</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>

			<span class="cm">/* rcu_report_qs_rnp() releases rnp-&gt;lock. */</span>
			<span class="n">rcu_report_qs_rnp</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">rsp</span><span class="p">,</span> <span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
			<span class="k">continue</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">rcu_initiate_boost</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span> <span class="cm">/* releases rnp-&gt;lock. */</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Force quiescent states on reluctant CPUs, and also detect which</span>
<span class="cm"> * CPUs are in dyntick-idle mode.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">force_quiescent_state</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">relaxed</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start fqs&quot;</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End fqs&quot;</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>  <span class="cm">/* No grace period in progress, nothing to force. */</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">raw_spin_trylock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqslock</span><span class="p">,</span> <span class="n">flags</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs_lh</span><span class="o">++</span><span class="p">;</span> <span class="cm">/* Inexact, can lose counts.  Tough! */</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End fqs&quot;</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>	<span class="cm">/* Someone else is already on the job. */</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">relaxed</span> <span class="o">&amp;&amp;</span> <span class="n">ULONG_CMP_GE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span><span class="p">,</span> <span class="n">jiffies</span><span class="p">))</span>
		<span class="k">goto</span> <span class="n">unlock_fqs_ret</span><span class="p">;</span> <span class="cm">/* no emergency and done recently. */</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs</span><span class="o">++</span><span class="p">;</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs already disabled */</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span> <span class="o">=</span> <span class="n">jiffies</span> <span class="o">+</span> <span class="n">RCU_JIFFIES_TILL_FORCE_QS</span><span class="p">;</span>
	<span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs_ngp</span><span class="o">++</span><span class="p">;</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs remain disabled */</span>
		<span class="k">goto</span> <span class="n">unlock_fqs_ret</span><span class="p">;</span>  <span class="cm">/* no GP in progress, time updated. */</span>
	<span class="p">}</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_active</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">switch</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_state</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">case</span> <span class="n">RCU_GP_IDLE</span>:
	<span class="k">case</span> <span class="n">RCU_GP_INIT</span>:

		<span class="k">break</span><span class="p">;</span> <span class="cm">/* grace period idle or initializing, ignore. */</span>

	<span class="k">case</span> <span class="n">RCU_SAVE_DYNTICK</span>:
		<span class="k">if</span> <span class="p">(</span><span class="n">RCU_SIGNAL_INIT</span> <span class="o">!=</span> <span class="n">RCU_SAVE_DYNTICK</span><span class="p">)</span>
			<span class="k">break</span><span class="p">;</span> <span class="cm">/* So gcc recognizes the dead code. */</span>

		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs remain disabled */</span>

		<span class="cm">/* Record dyntick-idle state. */</span>
		<span class="n">force_qs_rnp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">dyntick_save_progress_counter</span><span class="p">);</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs already disabled */</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">))</span>
			<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_state</span> <span class="o">=</span> <span class="n">RCU_FORCE_QS</span><span class="p">;</span>
		<span class="k">break</span><span class="p">;</span>

	<span class="k">case</span> <span class="n">RCU_FORCE_QS</span>:

		<span class="cm">/* Check dyntick-idle state, send IPI to laggarts. */</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs remain disabled */</span>
		<span class="n">force_qs_rnp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rcu_implicit_dynticks_qs</span><span class="p">);</span>

		<span class="cm">/* Leave state in case more forcing is required. */</span>

		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs already disabled */</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_active</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_need_gp</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqslock</span><span class="p">);</span> <span class="cm">/* irqs remain disabled */</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqs_need_gp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="n">rcu_start_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span> <span class="cm">/* releases rnp-&gt;lock */</span>
		<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End fqs&quot;</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>  <span class="cm">/* irqs remain disabled */</span>
<span class="nl">unlock_fqs_ret:</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">fqslock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End fqs&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * This does the RCU core processing work for the specified rcu_state</span>
<span class="cm"> * and rcu_data structures.  This may be called only from the CPU to</span>
<span class="cm"> * whom the rdp belongs.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">__rcu_process_callbacks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>

	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">beenonline</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * If an RCU GP has gone long enough, go check for dyntick</span>
<span class="cm">	 * idle CPUs and, if needed, send resched IPIs.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span><span class="p">),</span> <span class="n">jiffies</span><span class="p">))</span>
		<span class="n">force_quiescent_state</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Advance callbacks in response to end of earlier grace</span>
<span class="cm">	 * period that some other CPU ended.</span>
<span class="cm">	 */</span>
	<span class="n">rcu_process_gp_end</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>

	<span class="cm">/* Update RCU state based on any recent quiescent states. */</span>
	<span class="n">rcu_check_quiescent_state</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>

	<span class="cm">/* Does this CPU require a not-yet-started grace period? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_needs_another_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
		<span class="n">rcu_start_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>  <span class="cm">/* releases above lock */</span>
	<span class="p">}</span>

	<span class="cm">/* If there are callbacks ready, invoke them. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_has_callbacks_ready_to_invoke</span><span class="p">(</span><span class="n">rdp</span><span class="p">))</span>
		<span class="n">invoke_rcu_callbacks</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Do RCU core processing for the current CPU.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_process_callbacks</span><span class="p">(</span><span class="k">struct</span> <span class="n">softirq_action</span> <span class="o">*</span><span class="n">unused</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start RCU core&quot;</span><span class="p">);</span>
	<span class="n">__rcu_process_callbacks</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">));</span>
	<span class="n">__rcu_process_callbacks</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">));</span>
	<span class="n">rcu_preempt_process_callbacks</span><span class="p">();</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End RCU core&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Schedule RCU callback invocation.  If the specified type of RCU</span>
<span class="cm"> * does not support RCU priority boosting, just do a direct call,</span>
<span class="cm"> * otherwise wake up the per-CPU kernel kthread.  Note that because we</span>
<span class="cm"> * are running on the current CPU with interrupts disabled, the</span>
<span class="cm"> * rcu_cpu_kthread_task cannot disappear out from under us.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">invoke_rcu_callbacks</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="o">!</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rcu_scheduler_fully_active</span><span class="p">)))</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">likely</span><span class="p">(</span><span class="o">!</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">boost</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rcu_do_batch</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">invoke_rcu_callbacks_kthread</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">invoke_rcu_core</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">raise_softirq</span><span class="p">(</span><span class="n">RCU_SOFTIRQ</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span>
<span class="nf">__call_rcu</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">),</span>
	   <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="n">bool</span> <span class="n">lazy</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">;</span>

	<span class="n">WARN_ON_ONCE</span><span class="p">((</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">head</span> <span class="o">&amp;</span> <span class="mh">0x3</span><span class="p">);</span> <span class="cm">/* Misaligned rcu_head! */</span>
	<span class="n">debug_rcu_head_queue</span><span class="p">(</span><span class="n">head</span><span class="p">);</span>
	<span class="n">head</span><span class="o">-&gt;</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="p">;</span>
	<span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>

	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* Ensure RCU update seen before callback registry. */</span>

	<span class="cm">/*</span>
<span class="cm">	 * Opportunistically note grace-period endings and beginnings.</span>
<span class="cm">	 * Note that we might see a beginning right after we see an</span>
<span class="cm">	 * end, but never vice versa, since this CPU has to pass through</span>
<span class="cm">	 * a quiescent state betweentimes.</span>
<span class="cm">	 */</span>
	<span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
	<span class="n">rdp</span> <span class="o">=</span> <span class="n">this_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">);</span>

	<span class="cm">/* Add the callback to our list. */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="o">++</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">lazy</span><span class="p">)</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="o">++</span><span class="p">;</span>
	<span class="k">else</span>
		<span class="n">rcu_idle_count_callbacks_posted</span><span class="p">();</span>
	<span class="n">smp_mb</span><span class="p">();</span>  <span class="cm">/* Count before adding callback for rcu_barrier(). */</span>
	<span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="n">head</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_NEXT_TAIL</span><span class="p">]</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">__is_kfree_rcu_offset</span><span class="p">((</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">func</span><span class="p">))</span>
		<span class="n">trace_rcu_kfree_callback</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">func</span><span class="p">,</span>
					 <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">);</span>
	<span class="k">else</span>
		<span class="n">trace_rcu_callback</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">);</span>

	<span class="cm">/* If interrupts were disabled, don&#39;t dive into RCU core. */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">irqs_disabled_flags</span><span class="p">(</span><span class="n">flags</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Force the grace period if too many callbacks or too long waiting.</span>
<span class="cm">	 * Enforce hysteresis, and don&#39;t invoke force_quiescent_state()</span>
<span class="cm">	 * if some other CPU has recently done so.  Also, don&#39;t bother</span>
<span class="cm">	 * invoking force_quiescent_state() if the newly enqueued callback</span>
<span class="cm">	 * is the only one waiting for a grace period to complete.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">&gt;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">+</span> <span class="n">qhimark</span><span class="p">))</span> <span class="p">{</span>

		<span class="cm">/* Are we ignoring a completed grace period? */</span>
		<span class="n">rcu_process_gp_end</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>
		<span class="n">check_for_new_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>

		<span class="cm">/* Start a new grace period if one not already started. */</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">))</span> <span class="p">{</span>
			<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">nestflag</span><span class="p">;</span>
			<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp_root</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

			<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp_root</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">nestflag</span><span class="p">);</span>
			<span class="n">rcu_start_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">nestflag</span><span class="p">);</span>  <span class="cm">/* rlses rnp_root-&gt;lock */</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="cm">/* Give the grace period a kick. */</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">blimit</span> <span class="o">=</span> <span class="n">LONG_MAX</span><span class="p">;</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_force_qs_snap</span> <span class="o">&amp;&amp;</span>
			    <span class="o">*</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">RCU_DONE_TAIL</span><span class="p">]</span> <span class="o">!=</span> <span class="n">head</span><span class="p">)</span>
				<span class="n">force_quiescent_state</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_force_qs_snap</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs</span><span class="p">;</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span><span class="p">),</span> <span class="n">jiffies</span><span class="p">))</span>
		<span class="n">force_quiescent_state</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
	<span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Queue an RCU-sched callback for invocation after a grace period.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">call_rcu_sched</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">))</span>
<span class="p">{</span>
	<span class="n">__call_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">call_rcu_sched</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Queue an RCU callback for invocation after a quicker grace period.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">call_rcu_bh</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">rcu</span><span class="p">))</span>
<span class="p">{</span>
	<span class="n">__call_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">call_rcu_bh</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Because a context switch is a grace period for RCU-sched and RCU-bh,</span>
<span class="cm"> * any blocking grace-period wait automatically implies a grace period</span>
<span class="cm"> * if there is only one CPU online at any point time during execution</span>
<span class="cm"> * of either synchronize_sched() or synchronize_rcu_bh().  It is OK to</span>
<span class="cm"> * occasionally incorrectly indicate that there are multiple CPUs online</span>
<span class="cm"> * when there was in fact only one the whole time, as this just adds</span>
<span class="cm"> * some overhead: RCU still operates correctly.</span>
<span class="cm"> *</span>
<span class="cm"> * Of course, sampling num_online_cpus() with preemption enabled can</span>
<span class="cm"> * give erroneous results if there are concurrent CPU-hotplug operations.</span>
<span class="cm"> * For example, given a demonic sequence of preemptions in num_online_cpus()</span>
<span class="cm"> * and CPU-hotplug operations, there could be two or more CPUs online at</span>
<span class="cm"> * all times, but num_online_cpus() might well return one (or even zero).</span>
<span class="cm"> *</span>
<span class="cm"> * However, all such demonic sequences require at least one CPU-offline</span>
<span class="cm"> * operation.  Furthermore, rcu_blocking_is_gp() giving the wrong answer</span>
<span class="cm"> * is only a problem if there is an RCU read-side critical section executing</span>
<span class="cm"> * throughout.  But RCU-sched and RCU-bh read-side critical sections</span>
<span class="cm"> * disable either preemption or bh, which prevents a CPU from going offline.</span>
<span class="cm"> * Therefore, the only way that rcu_blocking_is_gp() can incorrectly return</span>
<span class="cm"> * that there is only one CPU when in fact there was more than one throughout</span>
<span class="cm"> * is when there were no RCU readers in the system.  If there are no</span>
<span class="cm"> * RCU readers, the grace period by definition can be of zero length,</span>
<span class="cm"> * regardless of the number of online CPUs.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">rcu_blocking_is_gp</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">might_sleep</span><span class="p">();</span>  <span class="cm">/* Check for RCU read-side critical section. */</span>
	<span class="k">return</span> <span class="n">num_online_cpus</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * synchronize_sched - wait until an rcu-sched grace period has elapsed.</span>
<span class="cm"> *</span>
<span class="cm"> * Control will return to the caller some time after a full rcu-sched</span>
<span class="cm"> * grace period has elapsed, in other words after all currently executing</span>
<span class="cm"> * rcu-sched read-side critical sections have completed.   These read-side</span>
<span class="cm"> * critical sections are delimited by rcu_read_lock_sched() and</span>
<span class="cm"> * rcu_read_unlock_sched(), and may be nested.  Note that preempt_disable(),</span>
<span class="cm"> * local_irq_disable(), and so on may be used in place of</span>
<span class="cm"> * rcu_read_lock_sched().</span>
<span class="cm"> *</span>
<span class="cm"> * This means that all preempt_disable code sequences, including NMI and</span>
<span class="cm"> * hardware-interrupt handlers, in progress on entry will have completed</span>
<span class="cm"> * before this primitive returns.  However, this does not guarantee that</span>
<span class="cm"> * softirq handlers will have completed, since in some kernels, these</span>
<span class="cm"> * handlers can run in process context, and can block.</span>
<span class="cm"> *</span>
<span class="cm"> * This primitive provides the guarantees made by the (now removed)</span>
<span class="cm"> * synchronize_kernel() API.  In contrast, synchronize_rcu() only</span>
<span class="cm"> * guarantees that rcu_read_lock() sections will have completed.</span>
<span class="cm"> * In &quot;classic RCU&quot;, these two guarantees happen to be one and</span>
<span class="cm"> * the same, but can differ in realtime RCU implementations.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">synchronize_sched</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_lock_map</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
			   <span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
			   <span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal synchronize_sched() in RCU-sched read-side critical section&quot;</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_blocking_is_gp</span><span class="p">())</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">wait_rcu_gp</span><span class="p">(</span><span class="n">call_rcu_sched</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">synchronize_sched</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * synchronize_rcu_bh - wait until an rcu_bh grace period has elapsed.</span>
<span class="cm"> *</span>
<span class="cm"> * Control will return to the caller some time after a full rcu_bh grace</span>
<span class="cm"> * period has elapsed, in other words after all currently executing rcu_bh</span>
<span class="cm"> * read-side critical sections have completed.  RCU read-side critical</span>
<span class="cm"> * sections are delimited by rcu_read_lock_bh() and rcu_read_unlock_bh(),</span>
<span class="cm"> * and may be nested.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">synchronize_rcu_bh</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_lockdep_assert</span><span class="p">(</span><span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_lock_map</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
			   <span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_lock_map</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
			   <span class="o">!</span><span class="n">lock_is_held</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_lock_map</span><span class="p">),</span>
			   <span class="s">&quot;Illegal synchronize_rcu_bh() in RCU-bh read-side critical section&quot;</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_blocking_is_gp</span><span class="p">())</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="n">wait_rcu_gp</span><span class="p">(</span><span class="n">call_rcu_bh</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">synchronize_rcu_bh</span><span class="p">);</span>

<span class="k">static</span> <span class="n">atomic_t</span> <span class="n">sync_sched_expedited_started</span> <span class="o">=</span> <span class="n">ATOMIC_INIT</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="k">static</span> <span class="n">atomic_t</span> <span class="n">sync_sched_expedited_done</span> <span class="o">=</span> <span class="n">ATOMIC_INIT</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="k">static</span> <span class="kt">int</span> <span class="nf">synchronize_sched_expedited_cpu_stop</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">data</span><span class="p">)</span>
<span class="p">{</span>
	<span class="cm">/*</span>
<span class="cm">	 * There must be a full memory barrier on each affected CPU</span>
<span class="cm">	 * between the time that try_stop_cpus() is called and the</span>
<span class="cm">	 * time that it returns.</span>
<span class="cm">	 *</span>
<span class="cm">	 * In the current initial implementation of cpu_stop, the</span>
<span class="cm">	 * above condition is already met when the control reaches</span>
<span class="cm">	 * this point and the following smp_mb() is not strictly</span>
<span class="cm">	 * necessary.  Do smp_mb() anyway for documentation and</span>
<span class="cm">	 * robustness against future implementation changes.</span>
<span class="cm">	 */</span>
	<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* See above comment block. */</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * synchronize_sched_expedited - Brute-force RCU-sched grace period</span>
<span class="cm"> *</span>
<span class="cm"> * Wait for an RCU-sched grace period to elapse, but use a &quot;big hammer&quot;</span>
<span class="cm"> * approach to force the grace period to end quickly.  This consumes</span>
<span class="cm"> * significant time on all CPUs and is unfriendly to real-time workloads,</span>
<span class="cm"> * so is thus not recommended for any sort of common-case code.  In fact,</span>
<span class="cm"> * if you are using synchronize_sched_expedited() in a loop, please</span>
<span class="cm"> * restructure your code to batch your updates, and then use a single</span>
<span class="cm"> * synchronize_sched() instead.</span>
<span class="cm"> *</span>
<span class="cm"> * Note that it is illegal to call this function while holding any lock</span>
<span class="cm"> * that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal</span>
<span class="cm"> * to call this function from a CPU-hotplug notifier.  Failing to observe</span>
<span class="cm"> * these restriction will result in deadlock.</span>
<span class="cm"> *</span>
<span class="cm"> * This implementation can be thought of as an application of ticket</span>
<span class="cm"> * locking to RCU, with sync_sched_expedited_started and</span>
<span class="cm"> * sync_sched_expedited_done taking on the roles of the halves</span>
<span class="cm"> * of the ticket-lock word.  Each task atomically increments</span>
<span class="cm"> * sync_sched_expedited_started upon entry, snapshotting the old value,</span>
<span class="cm"> * then attempts to stop all the CPUs.  If this succeeds, then each</span>
<span class="cm"> * CPU will have executed a context switch, resulting in an RCU-sched</span>
<span class="cm"> * grace period.  We are then done, so we use atomic_cmpxchg() to</span>
<span class="cm"> * update sync_sched_expedited_done to match our snapshot -- but</span>
<span class="cm"> * only if someone else has not already advanced past our snapshot.</span>
<span class="cm"> *</span>
<span class="cm"> * On the other hand, if try_stop_cpus() fails, we check the value</span>
<span class="cm"> * of sync_sched_expedited_done.  If it has advanced past our</span>
<span class="cm"> * initial snapshot, then someone else must have forced a grace period</span>
<span class="cm"> * some time after we took our snapshot.  In this case, our work is</span>
<span class="cm"> * done for us, and we can simply return.  Otherwise, we try again,</span>
<span class="cm"> * but keep our initial snapshot for purposes of checking for someone</span>
<span class="cm"> * doing our work for us.</span>
<span class="cm"> *</span>
<span class="cm"> * If we fail too many times in a row, we fall back to synchronize_sched().</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">synchronize_sched_expedited</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">firstsnap</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">snap</span><span class="p">,</span> <span class="n">trycount</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="cm">/* Note that atomic_inc_return() implies full memory barrier. */</span>
	<span class="n">firstsnap</span> <span class="o">=</span> <span class="n">snap</span> <span class="o">=</span> <span class="n">atomic_inc_return</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_sched_expedited_started</span><span class="p">);</span>
	<span class="n">get_online_cpus</span><span class="p">();</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">cpu_is_offline</span><span class="p">(</span><span class="n">raw_smp_processor_id</span><span class="p">()));</span>

	<span class="cm">/*</span>
<span class="cm">	 * Each pass through the following loop attempts to force a</span>
<span class="cm">	 * context switch on each CPU.</span>
<span class="cm">	 */</span>
	<span class="k">while</span> <span class="p">(</span><span class="n">try_stop_cpus</span><span class="p">(</span><span class="n">cpu_online_mask</span><span class="p">,</span>
			     <span class="n">synchronize_sched_expedited_cpu_stop</span><span class="p">,</span>
			     <span class="nb">NULL</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="n">EAGAIN</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">put_online_cpus</span><span class="p">();</span>

		<span class="cm">/* No joy, try again later.  Or just synchronize_sched(). */</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">trycount</span><span class="o">++</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">)</span>
			<span class="n">udelay</span><span class="p">(</span><span class="n">trycount</span> <span class="o">*</span> <span class="n">num_online_cpus</span><span class="p">());</span>
		<span class="k">else</span> <span class="p">{</span>
			<span class="n">synchronize_sched</span><span class="p">();</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>

		<span class="cm">/* Check to see if someone else did our work for us. */</span>
		<span class="n">s</span> <span class="o">=</span> <span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_sched_expedited_done</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">UINT_CMP_GE</span><span class="p">((</span><span class="kt">unsigned</span><span class="p">)</span><span class="n">s</span><span class="p">,</span> <span class="p">(</span><span class="kt">unsigned</span><span class="p">)</span><span class="n">firstsnap</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* ensure test happens before caller kfree */</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>

		<span class="cm">/*</span>
<span class="cm">		 * Refetching sync_sched_expedited_started allows later</span>
<span class="cm">		 * callers to piggyback on our grace period.  We subtract</span>
<span class="cm">		 * 1 to get the same token that the last incrementer got.</span>
<span class="cm">		 * We retry after they started, so our grace period works</span>
<span class="cm">		 * for them, and they started after our first try, so their</span>
<span class="cm">		 * grace period works for us.</span>
<span class="cm">		 */</span>
		<span class="n">get_online_cpus</span><span class="p">();</span>
		<span class="n">snap</span> <span class="o">=</span> <span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_sched_expedited_started</span><span class="p">);</span>
		<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* ensure read is before try_stop_cpus(). */</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Everyone up to our most recent fetch is covered by our grace</span>
<span class="cm">	 * period.  Update the counter, but only if our work is still</span>
<span class="cm">	 * relevant -- which it won&#39;t be if someone who started later</span>
<span class="cm">	 * than we did beat us to the punch.</span>
<span class="cm">	 */</span>
	<span class="k">do</span> <span class="p">{</span>
		<span class="n">s</span> <span class="o">=</span> <span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_sched_expedited_done</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">UINT_CMP_GE</span><span class="p">((</span><span class="kt">unsigned</span><span class="p">)</span><span class="n">s</span><span class="p">,</span> <span class="p">(</span><span class="kt">unsigned</span><span class="p">)</span><span class="n">snap</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">smp_mb</span><span class="p">();</span> <span class="cm">/* ensure test happens before caller kfree */</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="n">atomic_cmpxchg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sync_sched_expedited_done</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">snap</span><span class="p">)</span> <span class="o">!=</span> <span class="n">s</span><span class="p">);</span>

	<span class="n">put_online_cpus</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">synchronize_sched_expedited</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if there is any immediate RCU-related work to be done</span>
<span class="cm"> * by the current CPU, for the specified type of RCU, returning 1 if so.</span>
<span class="cm"> * The checks are in order of increasing expense: checks that can be</span>
<span class="cm"> * carried out against CPU-local state are performed first.  However,</span>
<span class="cm"> * we must check for CPU stalls first, else we might not get a chance.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">__rcu_pending</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>

	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rcu_pending</span><span class="o">++</span><span class="p">;</span>

	<span class="cm">/* Check for CPU stalls, if enabled. */</span>
	<span class="n">check_cpu_stall</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">);</span>

	<span class="cm">/* Is the RCU core waiting for a quiescent state from this CPU? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_scheduler_fully_active</span> <span class="o">&amp;&amp;</span>
	    <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span><span class="p">)</span> <span class="p">{</span>

		<span class="cm">/*</span>
<span class="cm">		 * If force_quiescent_state() coming soon and this CPU</span>
<span class="cm">		 * needs a quiescent state, and this is either RCU-sched</span>
<span class="cm">		 * or RCU-bh, force a local reschedule.</span>
<span class="cm">		 */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_qs_pending</span><span class="o">++</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">preemptible</span> <span class="o">&amp;&amp;</span>
		    <span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
				 <span class="n">jiffies</span><span class="p">))</span>
			<span class="n">set_need_resched</span><span class="p">();</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">&amp;&amp;</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_report_qs</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Does this CPU have callbacks ready to invoke? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_has_callbacks_ready_to_invoke</span><span class="p">(</span><span class="n">rdp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_cb_ready</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Has RCU gone idle with this CPU needing another grace period? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">cpu_needs_another_gp</span><span class="p">(</span><span class="n">rsp</span><span class="p">,</span> <span class="n">rdp</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_cpu_needs_gp</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Has another RCU grace period completed?  */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">)</span> <span class="o">!=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">)</span> <span class="p">{</span> <span class="cm">/* outside lock */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_gp_completed</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Has a new RCU grace period started? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="o">!=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">)</span> <span class="p">{</span> <span class="cm">/* outside lock */</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_gp_started</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* Has an RCU GP gone long enough to send resched IPIs &amp;c? */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">rcu_gp_in_progress</span><span class="p">(</span><span class="n">rsp</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
	    <span class="n">ULONG_CMP_LT</span><span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">jiffies_force_qs</span><span class="p">),</span> <span class="n">jiffies</span><span class="p">))</span> <span class="p">{</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_need_fqs</span><span class="o">++</span><span class="p">;</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>

	<span class="cm">/* nothing to do */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_rp_need_nothing</span><span class="o">++</span><span class="p">;</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if there is any immediate RCU-related work to be done</span>
<span class="cm"> * by the current CPU, returning 1 if so.  This function is part of the</span>
<span class="cm"> * RCU implementation; it is -not- an exported member of the RCU API.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_pending</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">__rcu_pending</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">))</span> <span class="o">||</span>
	       <span class="n">__rcu_pending</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">))</span> <span class="o">||</span>
	       <span class="n">rcu_preempt_pending</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Check to see if any future RCU-related work will need to be done</span>
<span class="cm"> * by the current CPU, even if none need be done immediately, returning</span>
<span class="cm"> * 1 if so.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">rcu_cpu_has_callbacks</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="cm">/* RCU callbacks either ready or pending? */</span>
	<span class="k">return</span> <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_sched_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">nxtlist</span> <span class="o">||</span>
	       <span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_bh_data</span><span class="p">,</span> <span class="n">cpu</span><span class="p">).</span><span class="n">nxtlist</span> <span class="o">||</span>
	       <span class="n">rcu_preempt_cpu_has_callbacks</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * RCU callback function for _rcu_barrier().  If we are last, wake</span>
<span class="cm"> * up the task executing _rcu_barrier().</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_barrier_callback</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">notused</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">atomic_dec_and_test</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_cpu_count</span><span class="p">))</span>
		<span class="n">complete</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_completion</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Called with preemption disabled, and from cross-cpu IRQ context.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">rcu_barrier_func</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">type</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span> <span class="o">=</span> <span class="n">smp_processor_id</span><span class="p">();</span>
	<span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_barrier_head</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">call_rcu_func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
			      <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">));</span>

	<span class="n">atomic_inc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_cpu_count</span><span class="p">);</span>
	<span class="n">call_rcu_func</span> <span class="o">=</span> <span class="n">type</span><span class="p">;</span>
	<span class="n">call_rcu_func</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">rcu_barrier_callback</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Orchestrate the specified type of RCU barrier, waiting for all</span>
<span class="cm"> * RCU callbacks of the specified type to complete.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">_rcu_barrier</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
			 <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">call_rcu_func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span>
					       <span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="k">struct</span> <span class="n">rcu_head</span> <span class="o">*</span><span class="n">head</span><span class="p">)))</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_head</span> <span class="n">rh</span><span class="p">;</span>

	<span class="n">init_rcu_head_on_stack</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rh</span><span class="p">);</span>

	<span class="cm">/* Take mutex to serialize concurrent rcu_barrier() requests. */</span>
	<span class="n">mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_mutex</span><span class="p">);</span>

	<span class="n">smp_mb</span><span class="p">();</span>  <span class="cm">/* Prevent any prior operations from leaking in. */</span>

	<span class="cm">/*</span>
<span class="cm">	 * Initialize the count to one rather than to zero in order to</span>
<span class="cm">	 * avoid a too-soon return to zero in case of a short grace period</span>
<span class="cm">	 * (or preemption of this task).  Also flag this task as doing</span>
<span class="cm">	 * an rcu_barrier().  This will prevent anyone else from adopting</span>
<span class="cm">	 * orphaned callbacks, which could cause otherwise failure if a</span>
<span class="cm">	 * CPU went offline and quickly came back online.  To see this,</span>
<span class="cm">	 * consider the following sequence of events:</span>
<span class="cm">	 *</span>
<span class="cm">	 * 1.	We cause CPU 0 to post an rcu_barrier_callback() callback.</span>
<span class="cm">	 * 2.	CPU 1 goes offline, orphaning its callbacks.</span>
<span class="cm">	 * 3.	CPU 0 adopts CPU 1&#39;s orphaned callbacks.</span>
<span class="cm">	 * 4.	CPU 1 comes back online.</span>
<span class="cm">	 * 5.	We cause CPU 1 to post an rcu_barrier_callback() callback.</span>
<span class="cm">	 * 6.	Both rcu_barrier_callback() callbacks are invoked, awakening</span>
<span class="cm">	 *	us -- but before CPU 1&#39;s orphaned callbacks are invoked!!!</span>
<span class="cm">	 */</span>
	<span class="n">init_completion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_completion</span><span class="p">);</span>
	<span class="n">atomic_set</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_cpu_count</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rcu_barrier_in_progress</span> <span class="o">=</span> <span class="n">current</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Force every CPU with callbacks to register a new callback</span>
<span class="cm">	 * that will tell us when all the preceding callbacks have</span>
<span class="cm">	 * been invoked.  If an offline CPU has callbacks, wait for</span>
<span class="cm">	 * it to either come back online or to finish orphaning those</span>
<span class="cm">	 * callbacks.</span>
<span class="cm">	 */</span>
	<span class="n">for_each_possible_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">preempt_disable</span><span class="p">();</span>
		<span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">cpu_is_offline</span><span class="p">(</span><span class="n">cpu</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">preempt_enable</span><span class="p">();</span>
			<span class="k">while</span> <span class="p">(</span><span class="n">cpu_is_offline</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">))</span>
				<span class="n">schedule_timeout_interruptible</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">ACCESS_ONCE</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span><span class="p">))</span> <span class="p">{</span>
			<span class="n">smp_call_function_single</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">rcu_barrier_func</span><span class="p">,</span>
						 <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="n">call_rcu_func</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
			<span class="n">preempt_enable</span><span class="p">();</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="n">preempt_enable</span><span class="p">();</span>
		<span class="p">}</span>
	<span class="p">}</span>

	<span class="cm">/*</span>
<span class="cm">	 * Now that all online CPUs have rcu_barrier_callback() callbacks</span>
<span class="cm">	 * posted, we can adopt all of the orphaned callbacks and place</span>
<span class="cm">	 * an rcu_barrier_callback() callback after them.  When that is done,</span>
<span class="cm">	 * we are guaranteed to have an rcu_barrier_callback() callback</span>
<span class="cm">	 * following every callback that could possibly have been</span>
<span class="cm">	 * registered before _rcu_barrier() was called.</span>
<span class="cm">	 */</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">rcu_adopt_orphan_cbs</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rcu_barrier_in_progress</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">atomic_inc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_cpu_count</span><span class="p">);</span>
	<span class="n">smp_mb__after_atomic_inc</span><span class="p">();</span> <span class="cm">/* Ensure atomic_inc() before callback. */</span>
	<span class="n">call_rcu_func</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rh</span><span class="p">,</span> <span class="n">rcu_barrier_callback</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * Now that we have an rcu_barrier_callback() callback on each</span>
<span class="cm">	 * CPU, and thus each counted, remove the initial count.</span>
<span class="cm">	 */</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">atomic_dec_and_test</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_cpu_count</span><span class="p">))</span>
		<span class="n">complete</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_completion</span><span class="p">);</span>

	<span class="cm">/* Wait for all rcu_barrier_callback() callbacks to be invoked. */</span>
	<span class="n">wait_for_completion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_completion</span><span class="p">);</span>

	<span class="cm">/* Other rcu_barrier() invocations can now safely proceed. */</span>
	<span class="n">mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_barrier_mutex</span><span class="p">);</span>

	<span class="n">destroy_rcu_head_on_stack</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rh</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_barrier_bh - Wait until all in-flight call_rcu_bh() callbacks complete.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_barrier_bh</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">_rcu_barrier</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="n">call_rcu_bh</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_barrier_bh</span><span class="p">);</span>

<span class="cm">/**</span>
<span class="cm"> * rcu_barrier_sched - Wait for in-flight call_rcu_sched() callbacks.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_barrier_sched</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">_rcu_barrier</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="n">call_rcu_sched</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL_GPL</span><span class="p">(</span><span class="n">rcu_barrier_sched</span><span class="p">);</span>

<span class="cm">/*</span>
<span class="cm"> * Do boot-time initialization of a CPU&#39;s per-CPU RCU data.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span>
<span class="nf">rcu_boot_init_percpu_data</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="cm">/* Set up local state, ensuring consistent view of global state. */</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span> <span class="o">=</span> <span class="mi">1UL</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">cpu</span> <span class="o">-</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="o">-&gt;</span><span class="n">grplo</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">RCU_NEXT_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
		<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxttail</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">nxtlist</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_lazy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">per_cpu</span><span class="p">(</span><span class="n">rcu_dynticks</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">!=</span> <span class="n">DYNTICK_TASK_EXIT_IDLE</span><span class="p">);</span>
	<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">cpu</span> <span class="o">=</span> <span class="n">cpu</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">rsp</span> <span class="o">=</span> <span class="n">rsp</span><span class="p">;</span>
	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Initialize a CPU&#39;s per-CPU RCU data.  Note that only one online or</span>
<span class="cm"> * offline event can be happening at a given time.  Note also that we</span>
<span class="cm"> * can accept some slop in the rsp-&gt;completed access due to the fact</span>
<span class="cm"> * that this CPU cannot possibly have any RCU callbacks in flight yet.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__cpuinit</span>
<span class="nf">rcu_init_percpu_data</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">,</span> <span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span> <span class="kt">int</span> <span class="n">preemptible</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">mask</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rcu_get_root</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="cm">/* Set up local state, ensuring consistent view of global state. */</span>
	<span class="n">raw_spin_lock_irqsave</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">beenonline</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>	 <span class="cm">/* We have now been online. */</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">preemptible</span> <span class="o">=</span> <span class="n">preemptible</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qlen_last_fqs_check</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">n_force_qs_snap</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">n_force_qs</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">blimit</span> <span class="o">=</span> <span class="n">blimit</span><span class="p">;</span>
	<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks_nesting</span> <span class="o">=</span> <span class="n">DYNTICK_TASK_EXIT_IDLE</span><span class="p">;</span>
	<span class="n">atomic_set</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">,</span>
		   <span class="p">(</span><span class="n">atomic_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rdp</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="o">-&gt;</span><span class="n">dynticks</span><span class="p">)</span> <span class="o">&amp;</span> <span class="o">~</span><span class="mh">0x1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
	<span class="n">rcu_prepare_for_idle_init</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
	<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>		<span class="cm">/* irqs remain disabled. */</span>

	<span class="cm">/*</span>
<span class="cm">	 * A new grace period might start here.  If so, we won&#39;t be part</span>
<span class="cm">	 * of it, but that is OK, as we are currently in a quiescent state.</span>
<span class="cm">	 */</span>

	<span class="cm">/* Exclude any attempts to start a new GP on large systems. */</span>
	<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">);</span>		<span class="cm">/* irqs already disabled. */</span>

	<span class="cm">/* Add CPU to rcu_node bitmasks. */</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>
	<span class="n">mask</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
	<span class="k">do</span> <span class="p">{</span>
		<span class="cm">/* Exclude any attempts to start a new GP on small systems. */</span>
		<span class="n">raw_spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>	<span class="cm">/* irqs already disabled. */</span>
		<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span> <span class="o">|=</span> <span class="n">mask</span><span class="p">;</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">==</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">)</span> <span class="p">{</span>
			<span class="cm">/*</span>
<span class="cm">			 * If there is a grace period in progress, we will</span>
<span class="cm">			 * set up to wait for it next time we run the</span>
<span class="cm">			 * RCU core code.</span>
<span class="cm">			 */</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">;</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">completed</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">;</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">qs_pending</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
			<span class="n">rdp</span><span class="o">-&gt;</span><span class="n">passed_quiesce_gpnum</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
			<span class="n">trace_rcu_grace_period</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">gpnum</span><span class="p">,</span> <span class="s">&quot;cpuonl&quot;</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="n">raw_spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span> <span class="cm">/* irqs already disabled. */</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="n">rnp</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">));</span>

	<span class="n">raw_spin_unlock_irqrestore</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">onofflock</span><span class="p">,</span> <span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="n">__cpuinit</span> <span class="nf">rcu_prepare_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="n">cpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">rcu_init_percpu_data</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">rcu_init_percpu_data</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">rcu_preempt_init_percpu_data</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Handle CPU online/offline notification events.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">__cpuinit</span> <span class="nf">rcu_cpu_notify</span><span class="p">(</span><span class="k">struct</span> <span class="n">notifier_block</span> <span class="o">*</span><span class="n">self</span><span class="p">,</span>
				    <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">action</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">hcpu</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">long</span> <span class="n">cpu</span> <span class="o">=</span> <span class="p">(</span><span class="kt">long</span><span class="p">)</span><span class="n">hcpu</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_data</span> <span class="o">*</span><span class="n">rdp</span> <span class="o">=</span> <span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rcu_state</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span> <span class="o">=</span> <span class="n">rdp</span><span class="o">-&gt;</span><span class="n">mynode</span><span class="p">;</span>

	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;Start CPU hotplug&quot;</span><span class="p">);</span>
	<span class="k">switch</span> <span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">case</span> <span class="n">CPU_UP_PREPARE</span>:
	<span class="k">case</span> <span class="n">CPU_UP_PREPARE_FROZEN</span>:
		<span class="n">rcu_prepare_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="n">rcu_prepare_kthreads</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="k">case</span> <span class="n">CPU_ONLINE</span>:
	<span class="k">case</span> <span class="n">CPU_DOWN_FAILED</span>:
		<span class="n">rcu_node_kthread_setaffinity</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
		<span class="n">rcu_cpu_kthread_setrt</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="k">case</span> <span class="n">CPU_DOWN_PREPARE</span>:
		<span class="n">rcu_node_kthread_setaffinity</span><span class="p">(</span><span class="n">rnp</span><span class="p">,</span> <span class="n">cpu</span><span class="p">);</span>
		<span class="n">rcu_cpu_kthread_setrt</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="k">case</span> <span class="n">CPU_DYING</span>:
	<span class="k">case</span> <span class="n">CPU_DYING_FROZEN</span>:
		<span class="cm">/*</span>
<span class="cm">		 * The whole machine is &quot;stopped&quot; except this CPU, so we can</span>
<span class="cm">		 * touch any data without introducing corruption. We send the</span>
<span class="cm">		 * dying CPU&#39;s callbacks to an arbitrarily chosen online CPU.</span>
<span class="cm">		 */</span>
		<span class="n">rcu_cleanup_dying_cpu</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">);</span>
		<span class="n">rcu_cleanup_dying_cpu</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">);</span>
		<span class="n">rcu_preempt_cleanup_dying_cpu</span><span class="p">();</span>
		<span class="n">rcu_cleanup_after_idle</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="k">case</span> <span class="n">CPU_DEAD</span>:
	<span class="k">case</span> <span class="n">CPU_DEAD_FROZEN</span>:
	<span class="k">case</span> <span class="n">CPU_UP_CANCELED</span>:
	<span class="k">case</span> <span class="n">CPU_UP_CANCELED_FROZEN</span>:
		<span class="n">rcu_cleanup_dead_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">);</span>
		<span class="n">rcu_cleanup_dead_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">);</span>
		<span class="n">rcu_preempt_cleanup_dead_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">);</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="nl">default:</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">trace_rcu_utilization</span><span class="p">(</span><span class="s">&quot;End CPU hotplug&quot;</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">NOTIFY_OK</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * This function is invoked towards the end of the scheduler&#39;s initialization</span>
<span class="cm"> * process.  Before this is called, the idle task might contain</span>
<span class="cm"> * RCU read-side critical sections (during which time, this idle</span>
<span class="cm"> * task is booting the system).  After this function is called, the</span>
<span class="cm"> * idle tasks are prohibited from containing RCU read-side critical</span>
<span class="cm"> * sections.  This function also enables RCU lockdep checking.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">rcu_scheduler_starting</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">WARN_ON</span><span class="p">(</span><span class="n">num_online_cpus</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">);</span>
	<span class="n">WARN_ON</span><span class="p">(</span><span class="n">nr_context_switches</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">rcu_scheduler_active</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/*</span>
<span class="cm"> * Compute the per-level fanout, either using the exact fanout specified</span>
<span class="cm"> * or balancing the tree, depending on CONFIG_RCU_FANOUT_EXACT.</span>
<span class="cm"> */</span>
<span class="cp">#ifdef CONFIG_RCU_FANOUT_EXACT</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_init_levelspread</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>

	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">NUM_RCU_LVLS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelspread</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">CONFIG_RCU_FANOUT</span><span class="p">;</span>
	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelspread</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">CONFIG_RCU_FANOUT_LEAF</span><span class="p">;</span>
<span class="p">}</span>
<span class="cp">#else </span><span class="cm">/* #ifdef CONFIG_RCU_FANOUT_EXACT */</span><span class="cp"></span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_init_levelspread</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">ccur</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">cprv</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>

	<span class="n">cprv</span> <span class="o">=</span> <span class="n">NR_CPUS</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">NUM_RCU_LVLS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">ccur</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelcnt</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelspread</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">cprv</span> <span class="o">+</span> <span class="n">ccur</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">ccur</span><span class="p">;</span>
		<span class="n">cprv</span> <span class="o">=</span> <span class="n">ccur</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
<span class="cp">#endif </span><span class="cm">/* #else #ifdef CONFIG_RCU_FANOUT_EXACT */</span><span class="cp"></span>

<span class="cm">/*</span>
<span class="cm"> * Helper function for rcu_init() that initializes one rcu_state structure.</span>
<span class="cm"> */</span>
<span class="k">static</span> <span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_init_one</span><span class="p">(</span><span class="k">struct</span> <span class="n">rcu_state</span> <span class="o">*</span><span class="n">rsp</span><span class="p">,</span>
		<span class="k">struct</span> <span class="n">rcu_data</span> <span class="n">__percpu</span> <span class="o">*</span><span class="n">rda</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">static</span> <span class="kt">char</span> <span class="o">*</span><span class="n">buf</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="s">&quot;rcu_node_level_0&quot;</span><span class="p">,</span>
			       <span class="s">&quot;rcu_node_level_1&quot;</span><span class="p">,</span>
			       <span class="s">&quot;rcu_node_level_2&quot;</span><span class="p">,</span>
			       <span class="s">&quot;rcu_node_level_3&quot;</span> <span class="p">};</span>  <span class="cm">/* Match MAX_RCU_LVLS */</span>
	<span class="kt">int</span> <span class="n">cpustride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">j</span><span class="p">;</span>
	<span class="k">struct</span> <span class="n">rcu_node</span> <span class="o">*</span><span class="n">rnp</span><span class="p">;</span>

	<span class="n">BUILD_BUG_ON</span><span class="p">(</span><span class="n">MAX_RCU_LVLS</span> <span class="o">&gt;</span> <span class="n">ARRAY_SIZE</span><span class="p">(</span><span class="n">buf</span><span class="p">));</span>  <span class="cm">/* Fix buf[] init! */</span>

	<span class="cm">/* Initialize the level-tracking arrays. */</span>

	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_RCU_LVLS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
		<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelcnt</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
	<span class="n">rcu_init_levelspread</span><span class="p">(</span><span class="n">rsp</span><span class="p">);</span>

	<span class="cm">/* Initialize the elements themselves, starting from the leaves. */</span>

	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">NUM_RCU_LVLS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">cpustride</span> <span class="o">*=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelspread</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
		<span class="n">rnp</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
		<span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelcnt</span><span class="p">[</span><span class="n">i</span><span class="p">];</span> <span class="n">j</span><span class="o">++</span><span class="p">,</span> <span class="n">rnp</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">raw_spin_lock_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
			<span class="n">lockdep_set_class_and_name</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span>
						   <span class="o">&amp;</span><span class="n">rcu_node_class</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">buf</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">gpnum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmask</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">qsmaskinit</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grplo</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">cpustride</span><span class="p">;</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span> <span class="o">=</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">cpustride</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span> <span class="o">&gt;=</span> <span class="n">NR_CPUS</span><span class="p">)</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span> <span class="o">=</span> <span class="n">NR_CPUS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
			<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpnum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
			<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpnum</span> <span class="o">=</span> <span class="n">j</span> <span class="o">%</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelspread</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpmask</span> <span class="o">=</span> <span class="mi">1UL</span> <span class="o">&lt;&lt;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grpnum</span><span class="p">;</span>
				<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">parent</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
					      <span class="n">j</span> <span class="o">/</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">levelspread</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
			<span class="p">}</span>
			<span class="n">rnp</span><span class="o">-&gt;</span><span class="n">level</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
			<span class="n">INIT_LIST_HEAD</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rnp</span><span class="o">-&gt;</span><span class="n">blkd_tasks</span><span class="p">);</span>
		<span class="p">}</span>
	<span class="p">}</span>

	<span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span> <span class="o">=</span> <span class="n">rda</span><span class="p">;</span>
	<span class="n">rnp</span> <span class="o">=</span> <span class="n">rsp</span><span class="o">-&gt;</span><span class="n">level</span><span class="p">[</span><span class="n">NUM_RCU_LVLS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
	<span class="n">for_each_possible_cpu</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="n">rnp</span><span class="o">-&gt;</span><span class="n">grphi</span><span class="p">)</span>
			<span class="n">rnp</span><span class="o">++</span><span class="p">;</span>
		<span class="n">per_cpu_ptr</span><span class="p">(</span><span class="n">rsp</span><span class="o">-&gt;</span><span class="n">rda</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">mynode</span> <span class="o">=</span> <span class="n">rnp</span><span class="p">;</span>
		<span class="n">rcu_boot_init_percpu_data</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">rsp</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">__init</span> <span class="nf">rcu_init</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">cpu</span><span class="p">;</span>

	<span class="n">rcu_bootup_announce</span><span class="p">();</span>
	<span class="n">rcu_init_one</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_sched_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_sched_data</span><span class="p">);</span>
	<span class="n">rcu_init_one</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rcu_bh_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rcu_bh_data</span><span class="p">);</span>
	<span class="n">__rcu_init_preempt</span><span class="p">();</span>
	 <span class="n">open_softirq</span><span class="p">(</span><span class="n">RCU_SOFTIRQ</span><span class="p">,</span> <span class="n">rcu_process_callbacks</span><span class="p">);</span>

	<span class="cm">/*</span>
<span class="cm">	 * We don&#39;t need protection against CPU-hotplug here because</span>
<span class="cm">	 * this is called early in boot, before either interrupts</span>
<span class="cm">	 * or the scheduler are operational.</span>
<span class="cm">	 */</span>
	<span class="n">cpu_notifier</span><span class="p">(</span><span class="n">rcu_cpu_notify</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">for_each_online_cpu</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
		<span class="n">rcu_cpu_notify</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="n">CPU_UP_PREPARE</span><span class="p">,</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)(</span><span class="kt">long</span><span class="p">)</span><span class="n">cpu</span><span class="p">);</span>
	<span class="n">check_cpu_stall_init</span><span class="p">();</span>
<span class="p">}</span>

<span class="cp">#include &quot;rcutree_plugin.h&quot;</span>

</pre></div></td></tr>

</tbody>
</table>
</div>

</body>
<script>docas={repo:"joekychen/linux",depth:1}</script>
<script>document.write('<script src=' + ('__proto__' in {} ? 'http://cdnjs.cloudflare.com/ajax/libs/zepto/1.0rc1/zepto.min.js' : 'https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js')+'><\\/script>')</script>
<script src="http://baoshan.github.com/moment/min/moment.min.js"></script>
<script src="../javascript/docco.min.js"></script>
</html>
