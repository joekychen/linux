f | internal.h | s | 10K | 304 | Linus Torvalds | torvalds@linux-foundation.org | 1338779157 |  | Revert "mm: compaction: handle incorrect MIGRATE_UNMOVABLE type pageblocks"  This reverts commit 5ceb9ce6fe9462a298bb2cd5c9f1ca6cb80a0199.  That commit seems to be the cause of the mm compation list corruption issues that Dave Jones reported.  The locking (or rather, absense there-of) is dubious, as is the use of the 'page' variable once it has been found to be outside the pageblock range.  So revert it for now, we can re-visit this for 3.6.  If we even need to: as Minchan Kim says, "The patch wasn't a bug fix and even test workload was very theoretical".  Reported-and-tested-by: Dave Jones <davej@redhat.com> Acked-by: Hugh Dickins <hughd@google.com> Acked-by: KOSAKI Motohiro <kosaki.motohiro@gmail.com> Acked-by: Minchan Kim <minchan@kernel.org> Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com> Cc: Kyungmin Park <kyungmin.park@samsung.com> Cc: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mremap.c | s | 13K | 462 | Al Viro | viro@zeniv.linux.org.uk | 1338561436 |  | move security_mmap_addr() to saner place  it really should be done by get_unmapped_area(); that cuts down on the amount of callers considerably and it's the right place for that stuff anyway.  Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
f | Makefile | g | 1.8K |  | Linus Torvalds | torvalds@linux-foundation.org | 1338838125 |  | 
f | memory-failure.c | s | 41K | 1421 | Borislav Petkov | borislav.petkov@amd.com | 1338333738 |  | mm/memory_failure: let the compiler add the function name  These things tend to get out of sync with time so let the compiler automatically enter the current function name using __func__.  No functional change.  Signed-off-by: Borislav Petkov <borislav.petkov@amd.com> Acked-by: Andi Kleen <andi@firstfloor.org> Cc: David Rientjes <rientjes@google.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | rmap.c | s | 52K | 1630 | Rik van Riel | riel@redhat.com | 1338333739 |  | mm: remove swap token code  The swap token code no longer fits in with the current VM model.  It does not play well with cgroups or the better NUMA placement code in development, since we have only one swap token globally.  It also has the potential to mess with scalability of the system, by increasing the number of non-reclaimable pages on the active and inactive anon LRU lists.  Last but not least, the swap token code has been broken for a year without complaints, as reported by Konstantin Khlebnikov.  This suggests we no longer have much use for it.  The days of sub-1G memory systems with heavy use of swap are over.  If we ever need thrashing reducing code in the future, we will have to implement something that does scale.  Signed-off-by: Rik van Riel <riel@redhat.com> Cc: Konstantin Khlebnikov <khlebnikov@openvz.org> Acked-by: Johannes Weiner <hannes@cmpxchg.org> Cc: Mel Gorman <mel@csn.ul.ie> Cc: Hugh Dickins <hughd@google.com> Acked-by: Bob Picco <bpicco@meloft.net> Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | percpu-vm.c | s | 12K | 398 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1340228376 |  | mm: fix kernel-doc warnings  Fix kernel-doc warnings such as    Warning(../mm/page_cgroup.c:432): No description found for parameter 'id'   Warning(../mm/page_cgroup.c:432): Excess function parameter 'mem' description in 'swap_cgroup_record'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Randy Dunlap <randy.dunlap@oracle.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | percpu-km.c | s | 2.8K | 89 | Tejun Heo | tj@kernel.org | 1284108984 |  | percpu: clear memory allocated with the km allocator  Percpu allocator should clear memory before returning it but the km allocator forgot to do it.  Fix it.  Signed-off-by: Tejun Heo <tj@kernel.org> Reported-by: Peter Zijlstra <peterz@infradead.org> Acked-by: Peter Zijlstra <peterz@infradead.org>
f | kmemleak-test.c | s | 3.3K | 96 | Jesper Juhl | jj@chaosbits.net | 1296153111 |  | kmemleak: remove memset by using kzalloc  We don't need to memset if we just use kzalloc() rather than kmalloc() in kmemleak_test_init().  Signed-off-by: Jesper Juhl <jj@chaosbits.net> Reviewed-by: Minchan Kim <minchan.kim@gmail.com> Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
f | frontswap.c | s | 9.0K | 290 | Konrad Rzeszutek Wilk | konrad.wilk@oracle.com | 1337096048 |  | frontswap: s/put_page/store/g s/get_page/load  Sounds so much more natural.  Suggested-by: Andrea Arcangeli <aarcange@redhat.com> Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
f | migrate.c | s | 33K | 1220 | Hugh Dickins | hughd@google.com | 1338779147 |  | mm: fix warning in __set_page_dirty_nobuffers  New tmpfs use of !PageUptodate pages for fallocate() is triggering the WARNING: at mm/page-writeback.c:1990 when __set_page_dirty_nobuffers() is called from migrate_page_copy() for compaction.  It is anomalous that migration should use __set_page_dirty_nobuffers() on an address_space that does not participate in dirty and writeback accounting; and this has also been observed to insert surprising dirty tags into a tmpfs radix_tree, despite tmpfs not using tags at all.  We should probably give migrate_page_copy() a better way to preserve the tag and migrate accounting info, when mapping_cap_account_dirty().  But that needs some more work: so in the interim, avoid the warning by using a simple SetPageDirty on PageSwapBacked pages.  Reported-and-tested-by: Dave Jones <davej@redhat.com> Signed-off-by: Hugh Dickins <hughd@google.com> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | Kconfig.debug | g | 1015B |  | Stanislaw Gruszka | sgruszka@redhat.com | 1326241842 |  | mm: more intensive memory corruption debugging  With CONFIG_DEBUG_PAGEALLOC configured, the CPU will generate an exception on access (read,write) to an unallocated page, which permits us to catch code which corrupts memory.  However the kernel is trying to maximise memory usage, hence there are usually few free pages in the system and buggy code usually corrupts some crucial data.  This patch changes the buddy allocator to keep more free/protected pages and to interlace free/protected and allocated pages to increase the probability of catching corruption.  When the kernel is compiled with CONFIG_DEBUG_PAGEALLOC, debug_guardpage_minorder defines the minimum order used by the page allocator to grant a request.  The requested size will be returned with the remaining pages used as guard pages.  The default value of debug_guardpage_minorder is zero: no change from current behaviour.  [akpm@linux-foundation.org: tweak documentation, s/flg/flag/] Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com> Cc: Mel Gorman <mgorman@suse.de> Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: "Rafael J. Wysocki" <rjw@sisk.pl> Cc: Christoph Lameter <cl@linux-foundation.org> Cc: Pekka Enberg <penberg@cs.helsinki.fi> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | page_isolation.c | s | 3.7K | 132 | Michal Nazarewicz | mina86@mina86.com | 1337605773 |  | mm: page_isolation: MIGRATE_CMA isolation functions added  This commit changes various functions that change pages and pageblocks migrate type between MIGRATE_ISOLATE and MIGRATE_MOVABLE in such a way as to allow to work with MIGRATE_CMA migrate type.  Signed-off-by: Michal Nazarewicz <mina86@mina86.com> Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com> Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> Tested-by: Rob Clark <rob.clark@linaro.org> Tested-by: Ohad Ben-Cohen <ohad@wizery.com> Tested-by: Benjamin Gaignard <benjamin.gaignard@linaro.org> Tested-by: Robert Nelson <robertcnelson@gmail.com> Tested-by: Barry Song <Baohua.Song@csr.com>
f | mprotect.c | s | 7.9K | 291 | Linus Torvalds | torvalds@linux-foundation.org | 1332432288 |  | Merge branch 'akpm' (Andrew's patch-bomb)  Merge first batch of patches from Andrew Morton:  "A few misc things and all the MM queue"  * emailed from Andrew Morton <akpm@linux-foundation.org>: (92 commits)   memcg: avoid THP split in task migration   thp: add HPAGE_PMD_* definitions for !CONFIG_TRANSPARENT_HUGEPAGE   memcg: clean up existing move charge code   mm/memcontrol.c: remove unnecessary 'break' in mem_cgroup_read()   mm/memcontrol.c: remove redundant BUG_ON() in mem_cgroup_usage_unregister_event()   mm/memcontrol.c: s/stealed/stolen/   memcg: fix performance of mem_cgroup_begin_update_page_stat()   memcg: remove PCG_FILE_MAPPED   memcg: use new logic for page stat accounting   memcg: remove PCG_MOVE_LOCK flag from page_cgroup   memcg: simplify move_account() check   memcg: remove EXPORT_SYMBOL(mem_cgroup_update_page_stat)   memcg: kill dead prev_priority stubs   memcg: remove PCG_CACHE page_cgroup flag   memcg: let css_get_next() rely upon rcu_read_lock()   cgroup: revert ss_id_lock to spinlock   idr: make idr_get_next() good for rcu_read_lock()   memcg: remove unnecessary thp check in page stat accounting   memcg: remove redundant returns   memcg: enum lru_list lru   ...
f | mlock.c | s | 15K | 550 | Linus Torvalds | torvalds@linux-foundation.org | 1331087016 |  | vm: avoid using find_vma_prev() unnecessarily  Several users of "find_vma_prev()" were not in fact interested in the previous vma if there was no primary vma to be found either.  And in those cases, we're much better off just using the regular "find_vma()", and then "prev" can be looked up by just checking vma->vm_prev.  The find_vma_prev() semantics are fairly subtle (see Mikulas' recent commit 83cd904d271b: "mm: fix find_vma_prev"), and the whole "return prev by reference" means that it generates worse code too.  Thus this "let's avoid using this inconvenient and clearly too subtle interface when we don't really have to" patch.  Cc: Mikulas Patocka <mpatocka@redhat.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | filemap_xip.c | s | 11K | 419 | Josef Bacik | josef@redhat.com | 1338566845 |  | fs: introduce inode operation ->update_time  Btrfs has to make sure we have space to allocate new blocks in order to modify the inode, so updating time can fail.  We've gotten around this by having our own file_update_time but this is kind of a pain, and Christoph has indicated he would like to make xfs do something different with atime updates.  So introduce ->update_time, where we will deal with i_version an a/m/c time updates and indicate which changes need to be made.  The normal version just does what it has always done, updates the time and marks the inode dirty, and then filesystems can choose to do something different.  I've gone through all of the users of file_update_time and made them check for errors with the exception of the fault code since it's complicated and I wasn't quite sure what to do there, also Jan is going to be pushing the file time updates into page_mkwrite for those who have it so that should satisfy btrfs and make it not a big deal to check the file_update_time() return code in the generic fault path. Thanks,  Signed-off-by: Josef Bacik <josef@redhat.com>
f | page-writeback.c | s | 67K | 2024 | Fengguang Wu | fengguang.wu@intel.com | 1336282918 |  | writeback: initialize global_dirty_limit  This prevents global_dirty_limit from remaining 0 (the initial value) for long time, since it's only updated in update_dirty_limit() when above the dirty freerun area.  It will avoid unexpected consequences when some random code use it as a convenient approximation of the global dirty threshold.  Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
f | memory_hotplug.c | s | 24K | 851 | Bjorn Helgaas | bhelgaas@google.com | 1338333741 |  | mm: print physical addresses consistently with other parts of kernel  Print physical address info in a style consistent with the %pR style used elsewhere in the kernel.  For example:      -Zone PFN ranges:     +Zone ranges:     -  DMA32    0x00000010 -> 0x00100000     +  DMA32    [mem 0x00010000-0xffffffff]     -  Normal   0x00100000 -> 0x01080000     +  Normal   [mem 0x100000000-0x107fffffff]  Signed-off-by: Bjorn Helgaas <bhelgaas@google.com> Cc: Yinghai Lu <yinghai@kernel.org> Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: "H. Peter Anvin" <hpa@zytor.com> Cc: Thomas Gleixner <tglx@linutronix.de> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | nommu.c | s | 50K | 1786 | Greg Ungerer | gerg@uclinux.org | 1338844651 |  | nommu: fix compilation of nommu.c  Compiling 3.5-rc1 for nommu targets gives:    CC      mm/nommu.o mm/nommu.c: In function ‘sys_mmap_pgoff’: mm/nommu.c:1489:2: error: ‘ret’ undeclared (first use in this function) mm/nommu.c:1489:2: note: each undeclared identifier is reported only once for each function it appears in  It is trivially fixed by replacing 'ret' with the local variable that is already defined for the return value 'retval'.  Signed-off-by: Greg Ungerer <gerg@uclinux.org> Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
f | util.c | s | 8.9K | 328 | Al Viro | viro@zeniv.linux.org.uk | 1338561438 |  | new helper: vm_mmap_pgoff()  take it to mm/util.c, convert vm_mmap() to use of that one and take it to mm/util.c as well, convert both sys_mmap_pgoff() to use of vm_mmap_pgoff()  Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
f | mm_init.c | s | 3.7K | 133 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067212 |  | mm: Map most files to use export.h instead of module.h  The files changed within are only using the EXPORT_SYMBOL macro variants.  They are not using core modular infrastructure and hence don't need module.h but only the export.h header.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | vmalloc.c | s | 65K | 2276 | KyongHo | pullip.cho@samsung.com | 1338333744 |  | mm: fix faulty initialization in vmalloc_init()  The transfer of ->flags causes some of the static mapping virtual addresses to be prematurely freed (before the mapping is removed) because VM_LAZY_FREE gets "set" if tmp->flags has VM_IOREMAP set.  This might cause subsequent vmalloc/ioremap calls to fail because it might allocate one of the freed virtual address ranges that aren't unmapped.  va->flags has different types of flags from tmp->flags.  If a region with VM_IOREMAP set is registered with vm_area_add_early(), it will be removed by __purge_vmap_area_lazy().  Fix vmalloc_init() to correctly initialize vmap_area for the given vm_struct.  Also initialise va->vm.  If it is not set, find_vm_area() for the early vm regions will always fail.  Signed-off-by: KyongHo Cho <pullip.cho@samsung.com> Cc: "Olav Haugan" <ohaugan@codeaurora.org> Cc: <stable@vger.kernel.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mmap.c | s | 70K | 2378 | Linus Torvalds | torvalds@linux-foundation.org | 1338572075 |  | Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs  Pull vfs changes from Al Viro.  "A lot of misc stuff.  The obvious groups:    * Miklos' atomic_open series; kills the damn abuse of      ->d_revalidate() by NFS, which was the major stumbling block for      all work in that area.    * ripping security_file_mmap() and dealing with deadlocks in the      area; sanitizing the neighborhood of vm_mmap()/vm_munmap() in      general.    * ->encode_fh() switched to saner API; insane fake dentry in      mm/cleancache.c gone.    * assorted annotations in fs (endianness, __user)    * parts of Artem's ->s_dirty work (jff2 and reiserfs parts)    * ->update_time() work from Josef.    * other bits and pieces all over the place.    Normally it would've been in two or three pull requests, but   signal.git stuff had eaten a lot of time during this cycle ;-/"  Fix up trivial conflicts in Documentation/filesystems/vfs.txt (the 'truncate_range' inode method was removed by the VM changes, the VFS update adds an 'update_time()' method), and in fs/btrfs/ulist.[ch] (due to sparse fix added twice, with other changes nearby).  * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (95 commits)   nfs: don't open in ->d_revalidate   vfs: retry last component if opening stale dentry   vfs: nameidata_to_filp(): don't throw away file on error   vfs: nameidata_to_filp(): inline __dentry_open()   vfs: do_dentry_open(): don't put filp   vfs: split __dentry_open()   vfs: do_last() common post lookup   vfs: do_last(): add audit_inode before open   vfs: do_last(): only return EISDIR for O_CREAT   vfs: do_last(): check LOOKUP_DIRECTORY   vfs: do_last(): make ENOENT exit RCU safe   vfs: make follow_link check RCU safe   vfs: do_last(): use inode variable   vfs: do_last(): inline walk_component()   vfs: do_last(): make exit RCU safe   vfs: split do_lookup()   Btrfs: move over to use ->update_time   fs: introduce inode operation ->update_time   reiserfs: get rid of resierfs_sync_super   reiserfs: mark the superblock as dirty a bit later   ...
f | page_cgroup.c | s | 11K | 443 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1340228376 |  | mm: fix kernel-doc warnings  Fix kernel-doc warnings such as    Warning(../mm/page_cgroup.c:432): No description found for parameter 'id'   Warning(../mm/page_cgroup.c:432): Excess function parameter 'mem' description in 'swap_cgroup_record'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Randy Dunlap <randy.dunlap@oracle.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | sparse-vmemmap.c | s | 5.9K | 198 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067211 |  | mm: delete various needless include <linux/module.h>  There is nothing modular in these files, and no reason to drag in all the 357 headers that module.h brings with it, since it just slows down compiles.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | oom_kill.c | s | 22K | 689 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1340228376 |  | mm: fix kernel-doc warnings  Fix kernel-doc warnings such as    Warning(../mm/page_cgroup.c:432): No description found for parameter 'id'   Warning(../mm/page_cgroup.c:432): Excess function parameter 'mem' description in 'swap_cgroup_record'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Randy Dunlap <randy.dunlap@oracle.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | percpu.c | s | 57K | 1699 | Catalin Marinas | catalin.marinas@arm.com | 1336583609 |  | kmemleak: Fix the kmemleak tracking of the percpu areas with !SMP  Kmemleak tracks the percpu allocations via a specific API and the originally allocated areas must be removed from kmemleak (via kmemleak_free). The code was already doing this for SMP systems.  Reported-by: Sami Liedes <sami.liedes@iki.fi> Cc: Tejun Heo <tj@kernel.org> Cc: Christoph Lameter <cl@linux-foundation.org> Signed-off-by: Catalin Marinas <catalin.marinas@arm.com> Signed-off-by: Tejun Heo <tj@kernel.org>
f | shmem.c | s | 78K | 2741 | Linus Torvalds | torvalds@linux-foundation.org | 1341355510 |  | Merge branch 'for-linus' of git://git.kernel.dk/linux-block  Pull block bits from Jens Axboe:  "As vacation is coming up, thought I'd better get rid of my pending   changes in my for-linus branch for this iteration.  It contains:     - Two patches for mtip32xx.  Killing a non-compliant sysfs interface      and moving it to debugfs, where it belongs.     - A few patches from Asias.  Two legit bug fixes, and one killing an      interface that is no longer in use.     - A patch from Jan, making the annoying partition ioctl warning a bit      less annoying, by restricting it to !CAP_SYS_RAWIO only.     - Three bug fixes for drbd from Lars Ellenberg.     - A fix for an old regression for umem, it hasn't really worked since      the plugging scheme was changed in 3.0.     - A few fixes from Tejun.     - A splice fix from Eric Dumazet, fixing an issue with pipe      resizing."  * 'for-linus' of git://git.kernel.dk/linux-block:   scsi: Silence unnecessary warnings about ioctl to partition   block: Drop dead function blk_abort_queue()   block: Mitigate lock unbalance caused by lock switching   block: Avoid missed wakeup in request waitqueue   umem: fix up unplugging   splice: fix racy pipe->buffers uses   drbd: fix null pointer dereference with on-congestion policy when diskless   drbd: fix list corruption by failing but already aborted reads   drbd: fix access of unallocated pages and kernel panic   xen/blkfront: Add WARN to deal with misbehaving backends.   blkcg: drop local variable @q from blkg_destroy()   mtip32xx: Create debugfs entries for troubleshooting   mtip32xx: Remove 'registers' and 'flags' from sysfs   blkcg: fix blkg_alloc() failure path   block: blkcg_policy_cfq shouldn't be used if !CONFIG_CFQ_GROUP_IOSCHED   block: fix return value on cfq_init() failure   mtip32xx: Remove version.h header file inclusion   xen/blkback: Copy id field when doing BLKIF_DISCARD.
f | debug-pagealloc.c | s | 2.1K | 82 | Stanislaw Gruszka | sgruszka@redhat.com | 1323159847 |  | mm, x86: Remove debug_pagealloc_enabled  When (no)bootmem finish operation, it pass pages to buddy allocator. Since debug_pagealloc_enabled is not set, we will do not protect pages, what is not what we want with CONFIG_DEBUG_PAGEALLOC=y.  To fix remove debug_pagealloc_enabled. That variable was introduced by commit 12d6f21e "x86: do not PSE on CONFIG_DEBUG_PAGEALLOC=y" to get more CPA (change page attribude) code testing. But currently we have CONFIG_CPA_DEBUG, which test CPA.  Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com> Acked-by: Mel Gorman <mgorman@suse.de> Cc: linux-mm@kvack.org Link: http://lkml.kernel.org/r/1322582711-14571-1-git-send-email-sgruszka@redhat.com Signed-off-by: Ingo Molnar <mingo@elte.hu>
f | memblock.c | s | 28K | 894 | Greg Pearson | greg.pearson@hp.com | 1340228376 |  | mm/memblock: fix overlapping allocation when doubling reserved array  __alloc_memory_core_early() asks memblock for a range of memory then try to reserve it.  If the reserved region array lacks space for the new range, memblock_double_array() is called to allocate more space for the array.  If memblock is used to allocate memory for the new array it can end up using a range that overlaps with the range originally allocated in __alloc_memory_core_early(), leading to possible data corruption.  With this patch memblock_double_array() now calls memblock_find_in_range() with a narrowed candidate range (in cases where the reserved.regions array is being doubled) so any memory allocated will not overlap with the original range that was being reserved.  The range is narrowed by passing in the starting address and size of the previously allocated range.  Then the range above the ending address is searched and if a candidate is not found, the range below the starting address is searched.  Signed-off-by: Greg Pearson <greg.pearson@hp.com> Signed-off-by: Yinghai Lu <yinghai@kernel.org> Acked-by: Tejun Heo <tj@kernel.org> Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mempolicy.c | s | 65K | 2288 | David Rientjes | rientjes@google.com | 1340255442 |  | mm, mempolicy: fix mbind() to do synchronous migration  If the range passed to mbind() is not allocated on nodes set in the nodemask, it migrates the pages to respect the constraint.  The final formal of migrate_pages() is a mode of type enum migrate_mode, not a boolean.  do_mbind() is currently passing "true" which is the equivalent of MIGRATE_SYNC_LIGHT.  This should instead be MIGRATE_SYNC for synchronous page migration.  Signed-off-by: David Rientjes <rientjes@google.com> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | vmscan.c | s | 93K | 2901 | Hugh Dickins | hughd@google.com | 1338333748 |  | mm/memcg: apply add/del_page to lruvec  Take lruvec further: pass it instead of zone to add_page_to_lru_list() and del_page_from_lru_list(); and pagevec_lru_move_fn() pass lruvec down to its target functions.  This cleanup eliminates a swathe of cruft in memcontrol.c, including mem_cgroup_lru_add_list(), mem_cgroup_lru_del_list() and mem_cgroup_lru_move_lists() - which never actually touched the lists.  In their place, mem_cgroup_page_lruvec() to decide the lruvec, previously a side-effect of add, and mem_cgroup_update_lru_size() to maintain the lru_size stats.  Whilst these are simplifications in their own right, the goal is to bring the evaluation of lruvec next to the spin_locking of the lrus, in preparation for a future patch.  Signed-off-by: Hugh Dickins <hughd@google.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> Acked-by: Michal Hocko <mhocko@suse.cz> Acked-by: Konstantin Khlebnikov <khlebnikov@openvz.org> Cc: Johannes Weiner <hannes@cmpxchg.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | prio_tree.c | s | 6.3K | 189 | Linus Torvalds | torvalds@linux-foundation.org | 1305921029 |  | sanitize <linux/prefetch.h> usage  Commit e66eed651fd1 ("list: remove prefetching from regular list iterators") removed the include of prefetch.h from list.h, which uncovered several cases that had apparently relied on that rather obscure header file dependency.  So this fixes things up a bit, using     grep -L linux/prefetch.h $(git grep -l '[^a-z_]prefetchw*(' -- '*.[ch]')    grep -L 'prefetchw*(' $(git grep -l 'linux/prefetch.h' -- '*.[ch]')  to guide us in finding files that either need <linux/prefetch.h> inclusion, or have it despite not needing it.  There are more of them around (mostly network drivers), but this gets many core ones.  Reported-by: Stephen Rothwell <sfr@canb.auug.org.au> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | nobootmem.c | s | 10K | 346 | Johannes Weiner | hannes@cmpxchg.org | 1338333742 |  | mm: remove sparsemem allocation details from the bootmem allocator  alloc_bootmem_section() derives allocation area constraints from the specified sparsemem section.  This is a bit specific for a generic memory allocator like bootmem, though, so move it over to sparsemem.  As __alloc_bootmem_node_nopanic() already retries failed allocations with relaxed area constraints, the fallback code in sparsemem.c can be removed and the code becomes a bit more compact overall.  [akpm@linux-foundation.org: fix build] Signed-off-by: Johannes Weiner <hannes@cmpxchg.org> Acked-by: Tejun Heo <tj@kernel.org> Acked-by: David S. Miller <davem@davemloft.net> Cc: Yinghai Lu <yinghai@kernel.org> Cc: Gavin Shan <shangw@linux.vnet.ibm.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | memory.c | s | 108K | 3601 | Randy Dunlap | rdunlap@xenotime.net | 1340228376 |  | mm/memory.c: fix kernel-doc warnings  Fix kernel-doc warnings in mm/memory.c:    Warning(mm/memory.c:1377): No description found for parameter 'start'   Warning(mm/memory.c:1377): Excess function parameter 'address' description in 'zap_page_range'  Signed-off-by: Randy Dunlap <rdunlap@xenotime.net> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | ksm.c | s | 54K | 1785 | Bob Liu | lliubbo@gmail.com | 1332377699 |  | ksm: cleanup: introduce find_mergeable_vma()  There are multiple places which perform the same check.  Add a new find_mergeable_vma() to handle this.  Signed-off-by: Bob Liu <lliubbo@gmail.com> Acked-by: Hugh Dickins <hughd@google.com> Cc: Andrea Arcangeli <aarcange@redhat.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | hwpoison-inject.c | s | 3.3K | 117 | Tony Luck | tony.luck@intel.com | 1325621192 |  | HWPOISON: Clean up memory_failure() vs. __memory_failure()  There is only one caller of memory_failure(), all other users call __memory_failure() and pass in the flags argument explicitly. The lone user of memory_failure() will soon need to pass flags too.  Add flags argument to the callsite in mce.c. Delete the old memory_failure() function, and then rename __memory_failure() without the leading "__".  Provide clearer message when action optional memory errors are ignored.  Acked-by: Borislav Petkov <bp@amd64.org> Signed-off-by: Tony Luck <tony.luck@intel.com>
f | page_alloc.c | s | 165K | 5230 | Linus Torvalds | torvalds@linux-foundation.org | 1338779157 |  | Revert "mm: compaction: handle incorrect MIGRATE_UNMOVABLE type pageblocks"  This reverts commit 5ceb9ce6fe9462a298bb2cd5c9f1ca6cb80a0199.  That commit seems to be the cause of the mm compation list corruption issues that Dave Jones reported.  The locking (or rather, absense there-of) is dubious, as is the use of the 'page' variable once it has been found to be outside the pageblock range.  So revert it for now, we can re-visit this for 3.6.  If we even need to: as Minchan Kim says, "The patch wasn't a bug fix and even test workload was very theoretical".  Reported-and-tested-by: Dave Jones <davej@redhat.com> Acked-by: Hugh Dickins <hughd@google.com> Acked-by: KOSAKI Motohiro <kosaki.motohiro@gmail.com> Acked-by: Minchan Kim <minchan@kernel.org> Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com> Cc: Kyungmin Park <kyungmin.park@samsung.com> Cc: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | truncate.c | s | 18K | 588 | Hugh Dickins | hughd@google.com | 1338333743 |  | mm/fs: remove truncate_range  Remove vmtruncate_range(), and remove the truncate_range method from struct inode_operations: only tmpfs ever supported it, and tmpfs has now converted over to using the fallocate method of file_operations.  Update Documentation accordingly, adding (setlease and) fallocate lines. And while we're in mm.h, remove duplicate declarations of shmem_lock() and shmem_file_setup(): everyone is now using the ones in shmem_fs.h.  Based-on-patch-by: Cong Wang <amwang@redhat.com> Signed-off-by: Hugh Dickins <hughd@google.com> Cc: Christoph Hellwig <hch@infradead.org> Cc: Cong Wang <amwang@redhat.com> Cc: Al Viro <viro@zeniv.linux.org.uk> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | madvise.c | s | 11K | 397 | Hugh Dickins | hughd@google.com | 1338333742 |  | mm/fs: route MADV_REMOVE to FALLOC_FL_PUNCH_HOLE  Now tmpfs supports hole-punching via fallocate(), switch madvise_remove() to use do_fallocate() instead of vmtruncate_range(): which extends madvise(,,MADV_REMOVE) support from tmpfs to ext4, ocfs2 and xfs.  There is one more user of vmtruncate_range() in our tree, staging/android's ashmem_shrink(): convert it to use do_fallocate() too (but if its unpinned areas are already unmapped - I don't know - then it would do better to use shmem_truncate_range() directly).  Based-on-patch-by: Cong Wang <amwang@redhat.com> Signed-off-by: Hugh Dickins <hughd@google.com> Cc: Christoph Hellwig <hch@infradead.org> Cc: Al Viro <viro@zeniv.linux.org.uk> Cc: Colin Cross <ccross@android.com> Cc: John Stultz <john.stultz@linaro.org> Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org> Cc: "Theodore Ts'o" <tytso@mit.edu> Cc: Andreas Dilger <adilger@dilger.ca> Cc: Mark Fasheh <mfasheh@suse.de> Cc: Joel Becker <jlbec@evilplan.org> Cc: Dave Chinner <david@fromorbit.com> Cc: Ben Myers <bpm@sgi.com> Cc: Michael Kerrisk <mtk.manpages@gmail.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | maccess.c | s | 1.6K | 53 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067212 |  | mm: Map most files to use export.h instead of module.h  The files changed within are only using the EXPORT_SYMBOL macro variants.  They are not using core modular infrastructure and hence don't need module.h but only the export.h header.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | page_io.c | s | 3.5K | 136 | Konrad Rzeszutek Wilk | konrad.wilk@oracle.com | 1337096048 |  | frontswap: s/put_page/store/g s/get_page/load  Sounds so much more natural.  Suggested-by: Andrea Arcangeli <aarcange@redhat.com> Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
f | hugetlb.c | s | 79K | 2652 | Dave Hansen | dave@linux.vnet.ibm.com | 1338392893 |  | mm: fix vma_resv_map() NULL pointer  hugetlb_reserve_pages() can be used for either normal file-backed hugetlbfs mappings, or MAP_HUGETLB.  In the MAP_HUGETLB, semi-anonymous mode, there is not a VMA around.  The new call to resv_map_put() assumed that there was, and resulted in a NULL pointer dereference:    BUG: unable to handle kernel NULL pointer dereference at 0000000000000030   IP: vma_resv_map+0x9/0x30   PGD 141453067 PUD 1421e1067 PMD 0   Oops: 0000 [#1] PREEMPT SMP   ...   Pid: 14006, comm: trinity-child6 Not tainted 3.4.0+ #36   RIP: vma_resv_map+0x9/0x30   ...   Process trinity-child6 (pid: 14006, threadinfo ffff8801414e0000, task ffff8801414f26b0)   Call Trace:     resv_map_put+0xe/0x40     hugetlb_reserve_pages+0xa6/0x1d0     hugetlb_file_setup+0x102/0x2c0     newseg+0x115/0x360     ipcget+0x1ce/0x310     sys_shmget+0x5a/0x60     system_call_fastpath+0x16/0x1b  This was reported by Dave Jones, but was reproducible with the libhugetlbfs test cases, so shame on me for not running them in the first place.  With this, the oops is gone, and the output of libhugetlbfs's run_tests.py is identical to plain 3.4 again.  [ Marked for stable, since this was introduced by commit c50ac050811d   ("hugetlb: fix resv_map leak in error path") which was also marked for   stable ]  Reported-by: Dave Jones <davej@redhat.com> Cc: Mel Gorman <mel@csn.ul.ie> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: Christoph Lameter <cl@linux.com> Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: <stable@vger.kernel.org>        [2.6.32+] Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | kmemleak.c | s | 52K | 1641 | Catalin Marinas | catalin.marinas@arm.com | 1327078625 |  | kmemleak: Disable early logging when kmemleak is off by default  Commit b6693005 (kmemleak: When the early log buffer is exceeded, report the actual number) deferred the disabling of the early logging to kmemleak_init(). However, when CONFIG_DEBUG_KMEMLEAK_DEFAULT_OFF=y, the early logging was no longer disabled causing __init kmemleak functions to be called even after the kernel freed the init memory. This patch disables the early logging during kmemleak_init() if kmemleak is left disabled.  Reported-by: Dirk Gouders <gouders@et.bocholt.fh-gelsenkirchen.de> Tested-by: Dirk Gouders <gouders@et.bocholt.fh-gelsenkirchen.de> Tested-by: Josh Boyer <jwboyer@gmail.com> Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
f | readahead.c | s | 16K | 529 | Cong Wang | xiyou.wangcong@gmail.com | 1338333743 |  | mm: move readahead syscall to mm/readahead.c  It is better to define readahead(2) in mm/readahead.c than in mm/filemap.c.  Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com> Cc: Fengguang Wu <fengguang.wu@intel.com> Cc: Al Viro <viro@zeniv.linux.org.uk> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | memcontrol.c | s | 143K | 4886 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1340228376 |  | mm: fix kernel-doc warnings  Fix kernel-doc warnings such as    Warning(../mm/page_cgroup.c:432): No description found for parameter 'id'   Warning(../mm/page_cgroup.c:432): Excess function parameter 'mem' description in 'swap_cgroup_record'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Randy Dunlap <randy.dunlap@oracle.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | huge_memory.c | s | 63K | 2173 | Hugh Dickins | hughd@google.com | 1338333748 |  | mm/memcg: apply add/del_page to lruvec  Take lruvec further: pass it instead of zone to add_page_to_lru_list() and del_page_from_lru_list(); and pagevec_lru_move_fn() pass lruvec down to its target functions.  This cleanup eliminates a swathe of cruft in memcontrol.c, including mem_cgroup_lru_add_list(), mem_cgroup_lru_del_list() and mem_cgroup_lru_move_lists() - which never actually touched the lists.  In their place, mem_cgroup_page_lruvec() to decide the lruvec, previously a side-effect of add, and mem_cgroup_update_lru_size() to maintain the lru_size stats.  Whilst these are simplifications in their own right, the goal is to bring the evaluation of lruvec next to the spin_locking of the lrus, in preparation for a future patch.  Signed-off-by: Hugh Dickins <hughd@google.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> Acked-by: Michal Hocko <mhocko@suse.cz> Acked-by: Konstantin Khlebnikov <khlebnikov@openvz.org> Cc: Johannes Weiner <hannes@cmpxchg.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | process_vm_access.c | s | 13K | 438 | Christopher Yeoh | cyeoh@au1.ibm.com | 1338511772 |  | aio/vfs: cleanup of rw_copy_check_uvector() and compat_rw_copy_check_uvector()  A cleanup of rw_copy_check_uvector and compat_rw_copy_check_uvector after changes made to support CMA in an earlier patch.  Rather than having an additional check_access parameter to these functions, the first paramater type is overloaded to allow the caller to specify CHECK_IOVEC_ONLY which means check that the contents of the iovec are valid, but do not check the memory that they point to.  This is used by process_vm_readv/writev where we need to validate that a iovec passed to the syscall is valid but do not want to check the memory that it points to at this point because it refers to an address space in another process.  Signed-off-by: Chris Yeoh <yeohc@au1.ibm.com> Reviewed-by: Oleg Nesterov <oleg@redhat.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | fremap.c | s | 6.7K | 230 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067211 |  | mm: delete various needless include <linux/module.h>  There is nothing modular in these files, and no reason to drag in all the 357 headers that module.h brings with it, since it just slows down compiles.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | vmstat.c | s | 33K | 1177 | Sasikantha babu | sasikanth.v19@gmail.com | 1338333739 |  | mm/vmstat.c: remove debug fs entries on failure of file creation and made extfrag_debug_root dentry local  Remove debug fs files and directory on failure.  Since no one is using "extfrag_debug_root" dentry outside of extfrag_debug_init(), make it local to the function.  Signed-off-by: Sasikantha babu <sasikanth.v19@gmail.com> Acked-by: David Rientjes <rientjes@google.com> Acked-by: Mel Gorman <mel@csn.ul.ie> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mempool.c | s | 10K | 332 | Tejun Heo | tj@kernel.org | 1326241845 |  | mempool: fix first round failure behavior  mempool modifies gfp_mask so that the backing allocator doesn't try too hard or trigger warning message when there's pool to fall back on.  In addition, for the first try, it removes __GFP_WAIT and IO, so that it doesn't trigger reclaim or wait when allocation can be fulfilled from pool; however, when that allocation fails and pool is empty too, it waits for the pool to be replenished before retrying.  Allocation which could have succeeded after a bit of reclaim has to wait on the reserved items and it's not like mempool doesn't retry with __GFP_WAIT and IO.  It just does that *after* someone returns an element, pointlessly delaying things.  Fix it by retrying immediately if the first round of allocation attempts w/o __GFP_WAIT and IO fails.  [akpm@linux-foundation.org: shorten the lock hold time] Signed-off-by: Tejun Heo <tj@kernel.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | dmapool.c | s | 12K | 448 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067212 |  | mm: fix implicit stat.h usage in dmapool.c  The removal of the implicitly everywhere module.h and its child includes will reveal this implicit stat.h usage:  mm/dmapool.c:108: error: ‘S_IRUGO’ undeclared here (not in a function)  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | fadvise.c | s | 3.6K | 142 | Shawn Bohrer | sbohrer@rgmadvisors.com | 1326241843 |  | fadvise: only initiate writeback for specified range with FADV_DONTNEED  Previously POSIX_FADV_DONTNEED would start writeback for the entire file when the bdi was not write congested.  This negatively impacts performance if the file contains dirty pages outside of the requested range.  This change uses __filemap_fdatawrite_range() to only initiate writeback for the requested range.  Signed-off-by: Shawn Bohrer <sbohrer@rgmadvisors.com> Acked-by: Johannes Weiner <jweiner@redhat.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | slub.c | s | 129K | 4647 | Linus Torvalds | torvalds@linux-foundation.org | 1338594623 |  | Merge branch 'slab/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux  Pull slab updates from Pekka Enberg:  "Mainly a bunch of SLUB fixes from Joonsoo Kim"  * 'slab/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux:   slub: use __SetPageSlab function to set PG_slab flag   slub: fix a memory leak in get_partial_node()   slub: remove unused argument of init_kmem_cache_node()   slub: fix a possible memory leak   Documentations: Fix slabinfo.c directory in vm/slub.txt   slub: fix incorrect return type of get_any_partial()
f | swap_state.c | s | 10K | 349 | Hugh Dickins | hughd@google.com | 1335230362 |  | mm: fix s390 BUG by __set_page_dirty_no_writeback on swap  Mel reports a BUG_ON(slot == NULL) in radix_tree_tag_set() on s390 3.0.13: called from __set_page_dirty_nobuffers() when page_remove_rmap() tries to transfer dirty flag from s390 storage key to struct page and radix_tree.  That would be because of reclaim's shrink_page_list() calling add_to_swap() on this page at the same time: first PageSwapCache is set (causing page_mapping(page) to appear as &swapper_space), then page->private set, then tree_lock taken, then page inserted into radix_tree - so there's an interval before taking the lock when the radix_tree slot is empty.  We could fix this by moving __add_to_swap_cache()'s spin_lock_irq up before the SetPageSwapCache.  But a better fix is simply to do what's five years overdue: Ken Chen introduced __set_page_dirty_no_writeback() (if !PageDirty TestSetPageDirty) for tmpfs to skip all the radix_tree overhead, and swap is just the same - it ignores the radix_tree tag, and does not participate in dirty page accounting, so should be using __set_page_dirty_no_writeback() too.  s390 testing now confirms that this does indeed fix the problem.  Reported-by: Mel Gorman <mgorman@suse.de> Signed-off-by: Hugh Dickins <hughd@google.com> Acked-by: Mel Gorman <mgorman@suse.de> Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Martin Schwidefsky <schwidefsky@de.ibm.com> Cc: Heiko Carstens <heiko.carstens@de.ibm.com> Cc: Rik van Riel <riel@redhat.com> Cc: Ken Chen <kenchen@google.com> Cc: stable@vger.kernel.org Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | pagewalk.c | s | 5.7K | 214 | Wanpeng Li | liwp@linux.vnet.ibm.com | 1340228376 |  | mm: fix kernel-doc warnings  Fix kernel-doc warnings such as    Warning(../mm/page_cgroup.c:432): No description found for parameter 'id'   Warning(../mm/page_cgroup.c:432): Excess function parameter 'mem' description in 'swap_cgroup_record'  Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com> Cc: Randy Dunlap <randy.dunlap@oracle.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | backing-dev.c | s | 22K | 749 | Rabin Vincent | rabin@rab.in | 1328086369 |  | backing-dev: fix wakeup timer races with bdi_unregister()  While 7a401a972df8e18 ("backing-dev: ensure wakeup_timer is deleted") addressed the problem of the bdi being freed with a queued wakeup timer, there are other races that could happen if the wakeup timer expires after/during bdi_unregister(), before bdi_destroy() is called.  wakeup_timer_fn() could attempt to wakeup a task which has already has been freed, or could access a NULL bdi->dev via the wake_forker_thread tracepoint.  Cc: <stable@kernel.org> Cc: Jens Axboe <axboe@kernel.dk> Reported-by: Chanho Min <chanho.min@lge.com> Reviewed-by: Namjae Jeon <linkinjeon@gmail.com> Signed-off-by: Rabin Vincent <rabin@rab.in> Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
f | Kconfig | g | 13K |  | Linus Torvalds | torvalds@linux-foundation.org | 1338838125 |  | 
f | slab.c | s | 120K | 4109 | Linus Torvalds | torvalds@linux-foundation.org | 1332972266 |  | Merge branch 'slab/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux  Pull SLAB changes from Pekka Enberg:  "There's the new kmalloc_array() API, minor fixes and performance   improvements, but quite honestly, nothing terribly exciting."  * 'slab/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux:   mm: SLAB Out-of-memory diagnostics   slab: introduce kmalloc_array()   slub: per cpu partial statistics change   slub: include include for prefetch   slub: Do not hold slub_lock when calling sysfs_slab_add()   slub: prefetch next freelist pointer in slab_alloc()   slab, cleanup: remove unneeded return
f | highmem.c | s | 10K | 371 | Linus Torvalds | torvalds@linux-foundation.org | 1320637487 |  | Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux  * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)   Revert "tracing: Include module.h in define_trace.h"   irq: don't put module.h into irq.h for tracking irqgen modules.   bluetooth: macroize two small inlines to avoid module.h   ip_vs.h: fix implicit use of module_get/module_put from module.h   nf_conntrack.h: fix up fallout from implicit moduleparam.h presence   include: replace linux/module.h with "struct module" wherever possible   include: convert various register fcns to macros to avoid include chaining   crypto.h: remove unused crypto_tfm_alg_modname() inline   uwb.h: fix implicit use of asm/page.h for PAGE_SIZE   pm_runtime.h: explicitly requires notifier.h   linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h   miscdevice.h: fix up implicit use of lists and types   stop_machine.h: fix implicit use of smp.h for smp_processor_id   of: fix implicit use of errno.h in include/linux/of.h   of_platform.h: delete needless include <linux/module.h>   acpi: remove module.h include from platform/aclinux.h   miscdevice.h: delete unnecessary inclusion of module.h   device_cgroup.h: delete needless include <linux/module.h>   net: sch_generic remove redundant use of <linux/module.h>   net: inet_timewait_sock doesnt need <linux/module.h>   ...  Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in  - drivers/media/dvb/frontends/dibx000_common.c  - drivers/media/video/{mt9m111.c,ov6650.c}  - drivers/mfd/ab3550-core.c  - include/linux/dmaengine.h
f | mmu_notifier.c | s | 9.1K | 284 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067212 |  | mm: Map most files to use export.h instead of module.h  The files changed within are only using the EXPORT_SYMBOL macro variants.  They are not using core modular infrastructure and hence don't need module.h but only the export.h header.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | swap.c | s | 21K | 713 | Hugh Dickins | hughd@google.com | 1338333748 |  | mm/memcg: apply add/del_page to lruvec  Take lruvec further: pass it instead of zone to add_page_to_lru_list() and del_page_from_lru_list(); and pagevec_lru_move_fn() pass lruvec down to its target functions.  This cleanup eliminates a swathe of cruft in memcontrol.c, including mem_cgroup_lru_add_list(), mem_cgroup_lru_del_list() and mem_cgroup_lru_move_lists() - which never actually touched the lists.  In their place, mem_cgroup_page_lruvec() to decide the lruvec, previously a side-effect of add, and mem_cgroup_update_lru_size() to maintain the lru_size stats.  Whilst these are simplifications in their own right, the goal is to bring the evaluation of lruvec next to the spin_locking of the lrus, in preparation for a future patch.  Signed-off-by: Hugh Dickins <hughd@google.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> Acked-by: Michal Hocko <mhocko@suse.cz> Acked-by: Konstantin Khlebnikov <khlebnikov@openvz.org> Cc: Johannes Weiner <hannes@cmpxchg.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mmzone.c | s | 2.0K | 85 | Konstantin Khlebnikov | khlebnikov@openvz.org | 1338333746 |  | mm: add link from struct lruvec to struct zone  This is the first stage of struct mem_cgroup_zone removal.  Further patches replace struct mem_cgroup_zone with a pointer to struct lruvec.  If CONFIG_CGROUP_MEM_RES_CTLR=n lruvec_zone() is just container_of().  Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org> Cc: Mel Gorman <mel@csn.ul.ie> Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> Acked-by: Hugh Dickins <hughd@google.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | bounce.c | s | 6.5K | 242 | Cong Wang | amwang@redhat.com | 1332251307 |  | mm: remove the second argument of k[un]map_atomic()  Signed-off-by: Cong Wang <amwang@redhat.com>
f | msync.c | s | 2.4K | 99 | Christoph Hellwig | hch@lst.de | 1274481081 |  | sanitize vfs_fsync calling conventions  Now that the last user passing a NULL file pointer is gone we can remove the redundant dentry argument and associated hacks inside vfs_fsynmc_range.  The next step will be removig the dentry argument from ->fsync, but given the luck with the last round of method prototype changes I'd rather defer this until after the main merge window.  Signed-off-by: Christoph Hellwig <hch@lst.de> Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
f | filemap.c | s | 66K | 2288 | Linus Torvalds | torvalds@linux-foundation.org | 1338572075 |  | Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs  Pull vfs changes from Al Viro.  "A lot of misc stuff.  The obvious groups:    * Miklos' atomic_open series; kills the damn abuse of      ->d_revalidate() by NFS, which was the major stumbling block for      all work in that area.    * ripping security_file_mmap() and dealing with deadlocks in the      area; sanitizing the neighborhood of vm_mmap()/vm_munmap() in      general.    * ->encode_fh() switched to saner API; insane fake dentry in      mm/cleancache.c gone.    * assorted annotations in fs (endianness, __user)    * parts of Artem's ->s_dirty work (jff2 and reiserfs parts)    * ->update_time() work from Josef.    * other bits and pieces all over the place.    Normally it would've been in two or three pull requests, but   signal.git stuff had eaten a lot of time during this cycle ;-/"  Fix up trivial conflicts in Documentation/filesystems/vfs.txt (the 'truncate_range' inode method was removed by the VM changes, the VFS update adds an 'update_time()' method), and in fs/btrfs/ulist.[ch] (due to sparse fix added twice, with other changes nearby).  * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (95 commits)   nfs: don't open in ->d_revalidate   vfs: retry last component if opening stale dentry   vfs: nameidata_to_filp(): don't throw away file on error   vfs: nameidata_to_filp(): inline __dentry_open()   vfs: do_dentry_open(): don't put filp   vfs: split __dentry_open()   vfs: do_last() common post lookup   vfs: do_last(): add audit_inode before open   vfs: do_last(): only return EISDIR for O_CREAT   vfs: do_last(): check LOOKUP_DIRECTORY   vfs: do_last(): make ENOENT exit RCU safe   vfs: make follow_link check RCU safe   vfs: do_last(): use inode variable   vfs: do_last(): inline walk_component()   vfs: do_last(): make exit RCU safe   vfs: split do_lookup()   Btrfs: move over to use ->update_time   fs: introduce inode operation ->update_time   reiserfs: get rid of resierfs_sync_super   reiserfs: mark the superblock as dirty a bit later   ...
f | bootmem.c | s | 20K | 709 | Gavin Shan | shangw@linux.vnet.ibm.com | 1338333744 |  | mm/bootmem.c: cleanup on addition to bootmem data list  The objects of "struct bootmem_data_t" are linked together to form double-linked list sequentially based on its minimal page frame number.  The current implementation implicitly supports the following cases, which means the inserting point for current bootmem data depends on how "list_for_each" works.  That makes the code a little hard to read. Besides, "list_for_each" and "list_entry" can be replaced with "list_for_each_entry".          - The linked list is empty.         - There has no entry in the linked list, whose minimal page           frame number is bigger than current one.  Signed-off-by: Gavin Shan <shangw@linux.vnet.ibm.com> Acked-by: Johannes Weiner <hannes@cmpxchg.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | failslab.c | s | 1.3K | 47 | Al Viro | viro@zeniv.linux.org.uk | 1325649296 |  | switch debugfs to umode_t  Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
f | slob.c | s | 17K | 603 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067212 |  | mm: Map most files to use export.h instead of module.h  The files changed within are only using the EXPORT_SYMBOL macro variants.  They are not using core modular infrastructure and hence don't need module.h but only the export.h header.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | swapfile.c | s | 63K | 2231 | Hugh Dickins | hughd@google.com | 1339822094 |  | swap: fix shmem swapping when more than 8 areas  Minchan Kim reports that when a system has many swap areas, and tmpfs swaps out to the ninth or more, shmem_getpage_gfp()'s attempts to read back the page cannot locate it, and the read fails with -ENOMEM.  Whoops.  Yes, I blindly followed read_swap_header()'s pte_to_swp_entry( swp_entry_to_pte()) technique for determining maximum usable swap offset, without stopping to realize that that actually depends upon the pte swap encoding shifting swap offset to the higher bits and truncating it there.  Whereas our radix_tree swap encoding leaves offset in the lower bits: it's swap "type" (that is, index of swap area) that was truncated.  Fix it by reducing the SWP_TYPE_SHIFT() in swapops.h, and removing the broken radix_to_swp_entry(swp_to_radix_entry()) from read_swap_header().  This does not reduce the usable size of a swap area any further, it leaves it as claimed when making the original commit: no change from 3.0 on x86_64, nor on i386 without PAE; but 3.0's 512GB is reduced to 128GB per swapfile on i386 with PAE.  It's not a change I would have risked five years ago, but with x86_64 supported for ten years, I believe it's appropriate now.  Hmm, and what if some architecture implements its swap pte with offset encoded below type? That would equally break the maximum usable swap offset check.  Happily, they all follow the same tradition of encoding offset above type, but I'll prepare a check on that for next.  Reported-and-Reviewed-and-Tested-by: Minchan Kim <minchan@kernel.org> Signed-off-by: Hugh Dickins <hughd@google.com> Cc: stable@vger.kernel.org [3.1, 3.2, 3.3, 3.4] Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mincore.c | s | 7.8K | 280 | Andrea Arcangeli | aarcange@redhat.com | 1332377694 |  | mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode  In some cases it may happen that pmd_none_or_clear_bad() is called with the mmap_sem hold in read mode.  In those cases the huge page faults can allocate hugepmds under pmd_none_or_clear_bad() and that can trigger a false positive from pmd_bad() that will not like to see a pmd materializing as trans huge.  It's not khugepaged causing the problem, khugepaged holds the mmap_sem in write mode (and all those sites must hold the mmap_sem in read mode to prevent pagetables to go away from under them, during code review it seems vm86 mode on 32bit kernels requires that too unless it's restricted to 1 thread per process or UP builds).  The race is only with the huge pagefaults that can convert a pmd_none() into a pmd_trans_huge().  Effectively all these pmd_none_or_clear_bad() sites running with mmap_sem in read mode are somewhat speculative with the page faults, and the result is always undefined when they run simultaneously.  This is probably why it wasn't common to run into this.  For example if the madvise(MADV_DONTNEED) runs zap_page_range() shortly before the page fault, the hugepage will not be zapped, if the page fault runs first it will be zapped.  Altering pmd_bad() not to error out if it finds hugepmds won't be enough to fix this, because zap_pmd_range would then proceed to call zap_pte_range (which would be incorrect if the pmd become a pmd_trans_huge()).  The simplest way to fix this is to read the pmd in the local stack (regardless of what we read, no need of actual CPU barriers, only compiler barrier needed), and be sure it is not changing under the code that computes its value.  Even if the real pmd is changing under the value we hold on the stack, we don't care.  If we actually end up in zap_pte_range it means the pmd was not none already and it was not huge, and it can't become huge from under us (khugepaged locking explained above).  All we need is to enforce that there is no way anymore that in a code path like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad can run into a hugepmd.  The overhead of a barrier() is just a compiler tweak and should not be measurable (I only added it for THP builds).  I don't exclude different compiler versions may have prevented the race too by caching the value of *pmd on the stack (that hasn't been verified, but it wouldn't be impossible considering pmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines and there's no external function called in between pmd_trans_huge and pmd_none_or_clear_bad).  		if (pmd_trans_huge(*pmd)) { 			if (next-addr != HPAGE_PMD_SIZE) { 				VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem)); 				split_huge_page_pmd(vma->vm_mm, pmd); 			} else if (zap_huge_pmd(tlb, vma, pmd, addr)) 				continue; 			/* fall through */ 		} 		if (pmd_none_or_clear_bad(pmd))  Because this race condition could be exercised without special privileges this was reported in CVE-2012-1179.  The race was identified and fully explained by Ulrich who debugged it. I'm quoting his accurate explanation below, for reference.  ====== start quote =======       mapcount 0 page_mapcount 1       kernel BUG at mm/huge_memory.c:1384!      At some point prior to the panic, a "bad pmd ..." message similar to the     following is logged on the console:        mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).      The "bad pmd ..." message is logged by pmd_clear_bad() before it clears     the page's PMD table entry.          143 void pmd_clear_bad(pmd_t *pmd)         144 {     ->  145         pmd_ERROR(*pmd);         146         pmd_clear(pmd);         147 }      After the PMD table entry has been cleared, there is an inconsistency     between the actual number of PMD table entries that are mapping the page     and the page's map count (_mapcount field in struct page). When the page     is subsequently reclaimed, __split_huge_page() detects this inconsistency.         1381         if (mapcount != page_mapcount(page))        1382                 printk(KERN_ERR "mapcount %d page_mapcount %d\n",        1383                        mapcount, page_mapcount(page));     -> 1384         BUG_ON(mapcount != page_mapcount(page));      The root cause of the problem is a race of two threads in a multithreaded     process. Thread B incurs a page fault on a virtual address that has never     been accessed (PMD entry is zero) while Thread A is executing an madvise()     system call on a virtual address within the same 2 MB (huge page) range.                 virtual address space               .---------------------.               ||                     ||               ||                     ||             .-||---------------------||             || ||                     ||             || ||                     ||<-- B(fault)             || ||                     ||       2 MB  || ||/////////////////////||-.       huge <  ||/////////////////////||  > A(range)       page  || ||/////////////////////||-'             || ||                     ||             || ||                     ||             '-||---------------------||               ||                     ||               ||                     ||               '---------------------'      - Thread A is executing an madvise(..., MADV_DONTNEED) system call       on the virtual address range "A(range)" shown in the picture.      sys_madvise       // Acquire the semaphore in shared mode.       down_read(&current->mm->mmap_sem)       ...       madvise_vma         switch (behavior)         case MADV_DONTNEED:              madvise_dontneed                zap_page_range                  unmap_vmas                    unmap_page_range                      zap_pud_range                        zap_pmd_range                          //                          // Assume that this huge page has never been accessed.                          // I.e. content of the PMD entry is zero (not mapped).                          //                          if (pmd_trans_huge(*pmd)) {                              // We don't get here due to the above assumption.                          }                          //                          // Assume that Thread B incurred a page fault and              .---------> // sneaks in here as shown below.              ||           //              ||           if (pmd_none_or_clear_bad(pmd))              ||               {              ||                 if (unlikely(pmd_bad(*pmd)))              ||                     pmd_clear_bad              ||                     {              ||                       pmd_ERROR              ||                         // Log "bad pmd ..." message here.              ||                       pmd_clear              ||                         // Clear the page's PMD entry.              ||                         // Thread B incremented the map count              ||                         // in page_add_new_anon_rmap(), but              ||                         // now the page is no longer mapped              ||                         // by a PMD entry (-> inconsistency).              ||                     }              ||               }              ||              v     - Thread B is handling a page fault on virtual address "B(fault)" shown       in the picture.      ...     do_page_fault       __do_page_fault         // Acquire the semaphore in shared mode.         down_read_trylock(&mm->mmap_sem)         ...         handle_mm_fault           if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))               // We get here due to the above assumption (PMD entry is zero).               do_huge_pmd_anonymous_page                 alloc_hugepage_vma                   // Allocate a new transparent huge page here.                 ...                 __do_huge_pmd_anonymous_page                   ...                   spin_lock(&mm->page_table_lock)                   ...                   page_add_new_anon_rmap                     // Here we increment the page's map count (starts at -1).                     atomic_set(&page->_mapcount, 0)                   set_pmd_at                     // Here we set the page's PMD entry which will be cleared                     // when Thread A calls pmd_clear_bad().                   ...                   spin_unlock(&mm->page_table_lock)      The mmap_sem does not prevent the race because both threads are acquiring     it in shared mode (down_read).  Thread B holds the page_table_lock while     the page's map count and PMD table entry are updated.  However, Thread A     does not synchronize on that lock.  ====== end quote =======  [akpm@linux-foundation.org: checkpatch fixes] Reported-by: Ulrich Obergfell <uobergfe@redhat.com> Signed-off-by: Andrea Arcangeli <aarcange@redhat.com> Acked-by: Johannes Weiner <hannes@cmpxchg.org> Cc: Mel Gorman <mgorman@suse.de> Cc: Hugh Dickins <hughd@google.com> Cc: Dave Jones <davej@redhat.com> Acked-by: Larry Woodman <lwoodman@redhat.com> Acked-by: Rik van Riel <riel@redhat.com> Cc: <stable@vger.kernel.org>		[2.6.38+] Cc: Mark Salter <msalter@redhat.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | init-mm.c | s | 619B | 22 | Arun Sharma | asharma@fb.com | 1311724187 |  | atomic: use <linux/atomic.h>  This allows us to move duplicated code in <asm/atomic.h> (atomic_inc_not_zero() for now) to <linux/atomic.h>  Signed-off-by: Arun Sharma <asharma@fb.com> Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: David Miller <davem@davemloft.net> Cc: Eric Dumazet <eric.dumazet@gmail.com> Acked-by: Mike Frysinger <vapier@gentoo.org> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | quicklist.c | s | 2.4K | 84 | Paul Gortmaker | paul.gortmaker@windriver.com | 1320067211 |  | mm: delete various needless include <linux/module.h>  There is nothing modular in these files, and no reason to drag in all the 357 headers that module.h brings with it, since it just slows down compiles.  Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
f | compaction.c | s | 23K | 750 | Linus Torvalds | torvalds@linux-foundation.org | 1338779157 |  | Revert "mm: compaction: handle incorrect MIGRATE_UNMOVABLE type pageblocks"  This reverts commit 5ceb9ce6fe9462a298bb2cd5c9f1ca6cb80a0199.  That commit seems to be the cause of the mm compation list corruption issues that Dave Jones reported.  The locking (or rather, absense there-of) is dubious, as is the use of the 'page' variable once it has been found to be outside the pageblock range.  So revert it for now, we can re-visit this for 3.6.  If we even need to: as Minchan Kim says, "The patch wasn't a bug fix and even test workload was very theoretical".  Reported-and-tested-by: Dave Jones <davej@redhat.com> Acked-by: Hugh Dickins <hughd@google.com> Acked-by: KOSAKI Motohiro <kosaki.motohiro@gmail.com> Acked-by: Minchan Kim <minchan@kernel.org> Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com> Cc: Kyungmin Park <kyungmin.park@samsung.com> Cc: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | mmu_context.c | s | 1.4K | 55 | David Rientjes | rientjes@google.com | 1332377699 |  | mm, counters: remove task argument to sync_mm_rss() and __sync_task_rss_stat()  sync_mm_rss() can only be used for current to avoid race conditions in iterating and clearing its per-task counters.  Remove the task argument for it and its helper function, __sync_task_rss_stat(), to avoid thinking it can be used safely for anything other than current.  Signed-off-by: David Rientjes <rientjes@google.com> Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | sparse.c | s | 20K | 691 | Johannes Weiner | hannes@cmpxchg.org | 1338333742 |  | mm: remove sparsemem allocation details from the bootmem allocator  alloc_bootmem_section() derives allocation area constraints from the specified sparsemem section.  This is a bit specific for a generic memory allocator like bootmem, though, so move it over to sparsemem.  As __alloc_bootmem_node_nopanic() already retries failed allocations with relaxed area constraints, the fallback code in sparsemem.c can be removed and the code becomes a bit more compact overall.  [akpm@linux-foundation.org: fix build] Signed-off-by: Johannes Weiner <hannes@cmpxchg.org> Acked-by: Tejun Heo <tj@kernel.org> Acked-by: David S. Miller <davem@davemloft.net> Cc: Yinghai Lu <yinghai@kernel.org> Cc: Gavin Shan <shangw@linux.vnet.ibm.com> Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
f | kmemcheck.c | s | 2.8K | 100 | Vegard Nossum | vegard.nossum@gmail.com | 1245073713 |  | kmemcheck: add hooks for the page allocator  This adds support for tracking the initializedness of memory that was allocated with the page allocator. Highmem requests are not tracked.  Cc: Dave Hansen <dave@linux.vnet.ibm.com> Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>  [build fix for !CONFIG_KMEMCHECK] Signed-off-by: Ingo Molnar <mingo@elte.hu>  [rebased for mainline inclusion] Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
f | pgtable-generic.c | s | 3.3K | 114 | Chris Metcalf | cmetcalf@tilera.com | 1337964501 |  | arch/tile: allow building Linux with transparent huge pages enabled  The change adds some infrastructure for managing tile pmd's more generally, using pte_pmd() and pmd_pte() methods to translate pmd values to and from ptes, since on TILEPro a pmd is really just a nested structure holding a pgd (aka pte).  Several existing pmd methods are moved into this framework, and a whole raft of additional pmd accessors are defined that are used by the transparent hugepage framework.  The tile PTE now has a "client2" bit.  The bit is used to indicate a transparent huge page is in the process of being split into subpages.  This change also fixes a generic bug where the return value of the generic pmdp_splitting_flush() was incorrect.  Signed-off-by: Chris Metcalf <cmetcalf@tilera.com>
f | cleancache.c | s | 6.5K | 198 | Al Viro | viro@zeniv.linux.org.uk | 1338348513 |  | ->encode_fh() API change  pass inode + parent's inode or NULL instead of dentry + bool saying whether we want the parent or not.  NOTE: that needs ceph fix folded in.  Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
